<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Thoughts on Data Science, ML and Startups</title><link href="https://va1da2.github.io/" rel="alternate"></link><link href="https://va1da2.github.io/feeds/all.atom.xml" rel="self"></link><id>https://va1da2.github.io/</id><updated>2021-02-14T05:44:06+02:00</updated><subtitle></subtitle><entry><title>Machine Learning Design Patterns: Problem Representation Part 2</title><link href="https://va1da2.github.io/machine-learning-design-patterns-problem-representation-part-2.html" rel="alternate"></link><published>2021-02-14T05:44:06+02:00</published><updated>2021-02-14T05:44:06+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2021-02-14:/machine-learning-design-patterns-problem-representation-part-2.html</id><summary type="html">&lt;p&gt;In the &lt;a href="/machine-learning-design-patterns-problem-representation-part-1.html"&gt;first part&lt;/a&gt; of this &lt;strong&gt;Problem Representation&lt;/strong&gt; series, we saw that representing seemingly regression problem as a classification problem can increase performance. We also noticed that constructing a label in a specific way can additionality increase the performance, but results weren't great - we achieved only about 30% correlation with the correct label. To improve our predictions, we will clean our dataset from those unpopular tracks before predicting popularity. For this, we will try to use a couple of design patterns - &lt;strong&gt;Rebalancing&lt;/strong&gt; and &lt;strong&gt;Ensembles&lt;/strong&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the &lt;a href="/machine-learning-design-patterns-problem-representation-part-1.html"&gt;first part&lt;/a&gt; of this &lt;strong&gt;Problem Representation&lt;/strong&gt; series, we saw that representing seemingly regression problem as a classification problem can bring increased performance. We also saw that constructing a label in a specific way can additionality increase the performance, but results weren't great - we achieved about 30% correlation only. To increase performance, we will split our problem  - separate low popularity and medium-high popularity songs and treat them separately. Therefore this post is about separating unpopular tracks as best as we can. For this will take a look at two ML representation design patterns from the &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;book&lt;/a&gt; - &lt;strong&gt;Rebalancing&lt;/strong&gt; and &lt;strong&gt;Ensembles&lt;/strong&gt;. As before, the code to reproduce these results is on &lt;a href="https://github.com/Va1da2/blog-notebooks/blob/main/sampling-ensemble-ml-design-patterns/Rebalancing%20and%20Ensembles.ipynb"&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Task: Classifying songs &lt;code&gt;unpopular&lt;/code&gt; vs &lt;code&gt;popular&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Looking at the distribution of popularity for our dataset (tracks produced from 2011), we see that a small portion of the songs is very unpopular (have a popularity score below 20):
&lt;img alt="popularity-distribution-with-low-scores-highlighted" src="/images/popularity_distribution_train_test_highlight_low.png"&gt;&lt;/p&gt;
&lt;p&gt;Low scores are outliers - only around &lt;code&gt;4%&lt;/code&gt; of all popularity scores in our dataset fall below the popularity score of 20. However, our track popularity models developed in &lt;a href="/machine-learning-design-patterns-problem-representation-part-1.html"&gt;part 1&lt;/a&gt; of this series still tried to accommodate these low scores. Our models might perform better if we remove these "outlier" tracks before predicting the song's popularity. To do this, we need to classify songs - &lt;code&gt;unpopular&lt;/code&gt;, the ones having Popularity of 20 or less, and &lt;code&gt;popular&lt;/code&gt;, all the others. Since only 4% of our dataset tracks have such a low popularity score, this is a highly imbalanced problem. To solve such a task, we will try out a couple of ML design patterns - &lt;strong&gt;Rebalancing&lt;/strong&gt; and &lt;strong&gt;Ensembles&lt;/strong&gt;.&lt;/p&gt;
&lt;h1&gt;Rebalancing and Ensemble Design Patterns&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Rebalancing Design Pattern&lt;/strong&gt; addresses the problem of the imbalanced dataset in three ways:
* &lt;strong&gt;undersampling&lt;/strong&gt; the majority class - randomly or selectively discarding data from majority class;
* &lt;strong&gt;oversampling&lt;/strong&gt; the minority class - producing additional data points from the minority class based on some heuristic;
* &lt;strong&gt;weighted classes&lt;/strong&gt; - giving weight to errors from minority class to stair model in producing better predictions. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ensemble Design Pattern&lt;/strong&gt; is a combination of multiple machine learning algorithms trained on subsamples of data to reduce bias, variance, or both. There are three main approaches for ensembles:
* &lt;strong&gt;Bagging&lt;/strong&gt; (bootstrap aggregating) - great for reducing variance in ML models. We train a few models on random samples of our dataset and average (or take the majority vote) of their output. 
* &lt;strong&gt;Boosting&lt;/strong&gt; - used for bias reduction, as it constructs a more powerful ensemble model than any of its models. 
* &lt;strong&gt;Stacking&lt;/strong&gt; - yet another way to combine models. Training a model on top of other model outputs to find the input model outputs' best weighting. &lt;/p&gt;
&lt;p&gt;We will use &lt;em&gt;bagging&lt;/em&gt; in solving our problem since we will train those models on small data, and our goal is to reduce the variance of the combined score.&lt;/p&gt;
&lt;h1&gt;Solving the task&lt;/h1&gt;
&lt;p&gt;We will go through the experiments to see which produces the best result for our use-case. First, let's see a bit more about the data we are working with and define evaluation metrics.&lt;/p&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;You will find a bit more EDA in the notebook referenced above.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We have 19,788 tracks collected for the years 2011-2020. We split this dataset randomly to train and test sets - 15,830 and 3,958. We have 11 audio features to predict popularity. Based on the plot provided above, let us mark tracks with the popularity of 20 and less as &lt;code&gt;unpopular&lt;/code&gt; and other songs as &lt;code&gt;popular&lt;/code&gt;. Having this definition, summary statistics for the label are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Statistic&lt;/th&gt;
&lt;th&gt;Popularity /train/&lt;/th&gt;
&lt;th&gt;Popularity /test/&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Count&lt;/td&gt;
&lt;td&gt;15,830&lt;/td&gt;
&lt;td&gt;3,958&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td&gt;0.042&lt;/td&gt;
&lt;td&gt;0.041&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Std&lt;/td&gt;
&lt;td&gt;0.201&lt;/td&gt;
&lt;td&gt;0.200&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The distributions of response are close in training and test sets. &lt;/p&gt;
&lt;h3&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;Our problem is a standard classification - the only thing we care about is to classify as accurately as possible examples in the test set. Since our dataset is highly imbalanced, &lt;code&gt;accuracy&lt;/code&gt; is not an appropriate metric. We will use &lt;code&gt;precision&lt;/code&gt;, &lt;code&gt;recall&lt;/code&gt;, and &lt;code&gt;f1-score&lt;/code&gt; for evaluation instead as these metrics better capture the performance of the model in our setting.&lt;/p&gt;
&lt;p&gt;Let us start our experiments with the naive approach.&lt;/p&gt;
&lt;h2&gt;Naive classification&lt;/h2&gt;
&lt;p&gt;The naive approach is not to modify our dataset or classifier's hyper-parameters (weights for the classes). Since we evaluate our models based on precision, recall, and f1-score, we need to select a score threshold. We will do this by looking at the recall-precision-f1 graph like the one below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="naive-model-recall-precision-f1" src="/images/naive_model_precision_recall_f1_tradeoff.png"&gt;&lt;/p&gt;
&lt;p&gt;The best point we can find from the above graph is &lt;code&gt;0.402&lt;/code&gt; recall, &lt;code&gt;0.564&lt;/code&gt; precision, and &lt;code&gt;0.470&lt;/code&gt; f1-score. The metrics mentioned above correspond to the threshold of &lt;code&gt;0.205&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We have our baseline. Next, let us explore various ways to account for imbalance in our dataset and see the impact on model performance.&lt;/p&gt;
&lt;h2&gt;Weighting classes&lt;/h2&gt;
&lt;p&gt;One way to achieve rebalancing is through the algorithm parameter rather than through changes to the training data. We can penalize errors made on the minority class examples more, and in this way, bias our algorithm to learn a better representation of the minority class. &lt;/p&gt;
&lt;p&gt;After running a few experiments, we achieved the best f1-score with the minority-to-majority weight ratio of 7-to-1. The performance of the model trained with this weighing is as follows.&lt;/p&gt;
&lt;p&gt;&lt;img alt="weighing-scheme-model-threshold-selection" src="/images/examples_weights_precision_recall_f1_tradeoff.png"&gt;&lt;/p&gt;
&lt;p&gt;The above graph's optimal point is an f1-score of &lt;code&gt;0.489&lt;/code&gt; (a 4% improvement on the naive approach), the precision of &lt;code&gt;0.524&lt;/code&gt;, recall of &lt;code&gt;0.457&lt;/code&gt;. We achieve these metrics with the threshold of &lt;code&gt;0.545&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now let us see what impact does the rebalancing of the data brings.&lt;/p&gt;
&lt;h2&gt;Undersampling&lt;/h2&gt;
&lt;p&gt;Dealing with the imbalanced dataset can be a series of blog posts in itself. Here we will look into a couple of techniques for &lt;code&gt;undersampling&lt;/code&gt; and later for &lt;code&gt;oversampling&lt;/code&gt; to get a sense of how these techniques perform. Overall - throwing out data is not wise, so we expect oversampling to come out on top. Let's see what our experiments show.&lt;/p&gt;
&lt;p&gt;We will use a great package to make this process easy for us - &lt;a href="https://imbalanced-learn.org/stable/index.html"&gt;imbalanced-learn&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Random&lt;/h3&gt;
&lt;p&gt;Random undersampling is the most straightforward approach we can take to alter our dataset - randomly discarding some of the majority class records to achieve a specified "balanced" level. Running a few experiments for different values of this balance, we have found that the best performing ratio is &lt;code&gt;0.2&lt;/code&gt; - undersampling until the balance is 20% minority examples and 80% majority examples (the original dataset was 4% minority and 96% majority).&lt;/p&gt;
&lt;p&gt;&lt;img alt="random-undersampling-precision-recall-tradeoff" src="/images/random_undersampling_precision_recall_f1_tradeoff.png"&gt;&lt;/p&gt;
&lt;p&gt;The above graph's optimal point is an f1-score of &lt;code&gt;0.484&lt;/code&gt;, the precision of &lt;code&gt;0.469&lt;/code&gt;, recall of &lt;code&gt;0.500&lt;/code&gt;. We achieve these metrics with the threshold of &lt;code&gt;0.423&lt;/code&gt;. We see that this approach gives us the model with a slightly different precision-recall tradeoff - we have better recall with this model and a somewhat lower precision score. With the f1-score being almost the same, we can pick a model that fits our use-case better in terms of this tradeoff later. Next, let us investigate a bit cleverer undersampling technique.&lt;/p&gt;
&lt;h3&gt;NearMiss&lt;/h3&gt;
&lt;p&gt;NearMiss method is based on a &lt;a href="https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf"&gt;paper&lt;/a&gt; by Zhang J. and Mani I. (2003). There are three variations of the algorithm:
1.  &lt;strong&gt;NearMiss1&lt;/strong&gt; - selects majority class examples with the smallest average distance to three closets minority class examples.
2. &lt;strong&gt;NearMiss2&lt;/strong&gt; - selects majority class examples according to their distance to three farthest minority class examples.
3. &lt;strong&gt;NearMiss3&lt;/strong&gt; - selects a given number of the closest majority class examples to the minority class example. &lt;/p&gt;
&lt;p&gt;After running nine experiments for each of the above variations, we achieve the best result for &lt;code&gt;version 3&lt;/code&gt; with the equal sampling strategy - we downsampled the majority class to a 50/50 ratio. &lt;/p&gt;
&lt;p&gt;&lt;img alt="near-miss-undersampling-precision-recall-f1-tradeoff" src="/images/near_miss_precision_recall_f1_tradeoff.png"&gt;&lt;/p&gt;
&lt;p&gt;Given how much data we discard, it is no surprise that results are worse than those of other methods. We achieve the best performance with the threshold of &lt;code&gt;0.692&lt;/code&gt; - the precision of &lt;code&gt;0.483&lt;/code&gt;, recall of &lt;code&gt;0.341&lt;/code&gt;, and f1-score of &lt;code&gt;0.400&lt;/code&gt;. Let us see if oversampling can improve results.&lt;/p&gt;
&lt;h2&gt;Oversampling&lt;/h2&gt;
&lt;p&gt;It is no surprise that we lose model performance as we discard records from our dataset. Our dataset is not very big, to begin with, so next, we will try a couple of oversampling methods - &lt;strong&gt;SMOTE&lt;/strong&gt; and &lt;strong&gt;ADASYN&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;SMOTE&lt;/h3&gt;
&lt;p&gt;SMOTE was proposed in the paper by &lt;a href="https://arxiv.org/pdf/1106.1813.pdf"&gt;Chawla et al. (2002)&lt;/a&gt; as a means of improving model performance on imbalanced datasets and as an alternative to random over-sampling with replacement. Instead of oversampling actual records, this approach creates synthetic examples from the minority class. Let's see how it performs on our dataset.&lt;/p&gt;
&lt;p&gt;Running several experiments with different oversampling ratios, we find that oversampling the dataset to the 20/80 level gives us the best performing model.&lt;/p&gt;
&lt;p&gt;&lt;img alt="smote-oversampling-precision-recall-f1-tradeoff" src="/images/smote_oversampling_precision_recall_f1_tradeoff.png"&gt;&lt;/p&gt;
&lt;p&gt;The best performance is with the threshold of &lt;code&gt;0.567&lt;/code&gt; - the precision of &lt;code&gt;0.500&lt;/code&gt;, recall of &lt;code&gt;0.396&lt;/code&gt;, and f1-score of &lt;code&gt;0.442&lt;/code&gt;. 
Oversampling with SMOTE produced a better model than NearMiss undersampling. However, it still does not outperform any of the naive, weighted, or models produced by randomly undersampling records. Let us investigate another oversampling technique.&lt;/p&gt;
&lt;h3&gt;ADASYN&lt;/h3&gt;
&lt;p&gt;ADASYN is the algorithm proposed in &lt;a href="https://ieeexplore.ieee.org/abstract/document/4633969"&gt;Haibo et al. (2008)&lt;/a&gt;. The main difference from SMOTE is that ADASYN generates more synthetic examples that are harder to learn instead of treating every minority group example equally.&lt;/p&gt;
&lt;p&gt;As with SMOTE, ADASYN oversampling produces the best model with the dataset balance of 20/80.&lt;/p&gt;
&lt;p&gt;&lt;img alt="adasyn-oversampling-precision-recall-f1-tradeoff" src="/images/adasyn_oversampling_precision_recall_f1_tradeoff.png"&gt;&lt;/p&gt;
&lt;p&gt;As is evident from the graph, oversampling with ADASYN gives us a model that outperforms just one other - NearMiss model - the precision of &lt;code&gt;0.453&lt;/code&gt;, recall of &lt;code&gt;0.378&lt;/code&gt;, and overall f1-score of &lt;code&gt;0.412&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Ensemble&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Ensemble&lt;/strong&gt; design pattern is useful for many problems, not just for imbalanced datasets. Model performance will be more stable by training and aggregating multiple models that have seen different parts of the training dataset. There are different ensemble techniques to target bias or variance issues, and there are algorithms that already incorporate this technique - Random forest (bagging) and Boosted trees (boosting). However, when having an imbalanced dataset, there is a twist.&lt;/p&gt;
&lt;p&gt;We do not want to train the model on the random sample of our data; what we want to do, is to randomly downsample the majority class and trained our model on balanced data. Then we repeat this process as many times as needed for a stable performance. In our case, with song popularity prediction, aggregating 40 models produced the best result.&lt;/p&gt;
&lt;p&gt;By running several experiments, we found that the best ensemble has 45 models. That's a lot, but models are tree-based algorithms, so the inference is not snail-slow.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ensemble-precision-recall-f1-tradeoff" src="/images/ensemble_precision_recall_f1_tradeoff.png"&gt;&lt;/p&gt;
&lt;p&gt;Ensemble approach gives us the third-best model with the precision of &lt;code&gt;0.481&lt;/code&gt;, recall of &lt;code&gt;0.470&lt;/code&gt;, and f1-score of &lt;code&gt;0.475&lt;/code&gt; at a threshold of &lt;code&gt;0.733&lt;/code&gt;. &lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;Putting all our results together we get the following table:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Approach&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;F1-Score&lt;/th&gt;
&lt;th align="left"&gt;Notes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Weighting training examples&lt;/td&gt;
&lt;td&gt;0.524&lt;/td&gt;
&lt;td&gt;0.457&lt;/td&gt;
&lt;td&gt;0.489&lt;/td&gt;
&lt;td align="left"&gt;Best performing weighting at 7-to-1; threshold - 0.545&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Random undersampling&lt;/td&gt;
&lt;td&gt;0.469&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;td&gt;0.484&lt;/td&gt;
&lt;td align="left"&gt;Best performing undersampling to 20/80; threshold - 0.423&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ensemble&lt;/td&gt;
&lt;td&gt;0.481&lt;/td&gt;
&lt;td&gt;0.470&lt;/td&gt;
&lt;td&gt;0.475&lt;/td&gt;
&lt;td align="left"&gt;Best performing ensemble was with 45 models; threshold - 0.733&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Naive&lt;/td&gt;
&lt;td&gt;0.564&lt;/td&gt;
&lt;td&gt;0.402&lt;/td&gt;
&lt;td&gt;0.470&lt;/td&gt;
&lt;td align="left"&gt;Threshold - 0.205&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SMOTE oversampling&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;td&gt;0.396&lt;/td&gt;
&lt;td&gt;0.442&lt;/td&gt;
&lt;td align="left"&gt;Best performing oversampling at 20/80; threshold - 0.567&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ADASYN oversampling&lt;/td&gt;
&lt;td&gt;0.453&lt;/td&gt;
&lt;td&gt;0.378&lt;/td&gt;
&lt;td&gt;0.412&lt;/td&gt;
&lt;td align="left"&gt;Best performing oversampling at 20/80; threshold - 0.578&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NearMiss undersampling&lt;/td&gt;
&lt;td&gt;0.483&lt;/td&gt;
&lt;td&gt;0.341&lt;/td&gt;
&lt;td&gt;0.400&lt;/td&gt;
&lt;td align="left"&gt;Best performing undersampling was done with version 3 to 50/50; threshold - 0.692&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Evaluating models on the highest f1-score achieved, we have two close contenders with slightly different performance profiles. Model build by weighing training examples has higher precision but lower recall, while model build by randomly undersampling majority class in training data has a higher recall and lower precision. The next two models (the 3rd and 4th by f1 score) are close in terms of f1 score but are slightly different in the performance profile. The ensemble model is relatively balanced, with recall and precision being near, while the Naive model has the highest precision among selected models, with recall being barely above 0.4.&lt;/p&gt;
&lt;p&gt;We set out on this imbalanced classification journey to "clean" our dataset from unpopular tracks before predicting that popularity for "normal" tracks. By applying our top-performing model, we get the following popularity distribution:&lt;/p&gt;
&lt;p&gt;&lt;img alt="popularity-distribution-for-original-and-cleaned-dataset" src="/images/comparison_of_popularity_distributions.png"&gt;&lt;/p&gt;
&lt;p&gt;We remove around 60% of the unpopular songs with our model. We want to remove more, but we don't know if this will be enough to impact the overall popularity prediction performance. We will investigate this in &lt;strong&gt;Part 3&lt;/strong&gt; of this &lt;strong&gt;Problem Representation Design Patterns&lt;/strong&gt; series. We will bring together the last two posts and combine them with the &lt;strong&gt;Cascade&lt;/strong&gt; design pattern.&lt;/p&gt;
&lt;h3&gt;Other Posts in Machine Learning Design Patterns Series&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/machine-learning-design-patterns-data-representation.html"&gt;Data Representation Design Patters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/machine-learning-design-patterns-problem-representation-part-1.html"&gt;Problem Representation Design Patterns Part 1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Data Science"></category><category term="machine learning"></category><category term="ml"></category><category term="data science"></category><category term="design patterns"></category><category term="ml problem representation"></category></entry><entry><title>Machine Learning Design Patterns: Problem Representation Part 1</title><link href="https://va1da2.github.io/machine-learning-design-patterns-problem-representation-part-1.html" rel="alternate"></link><published>2021-01-16T06:01:54+02:00</published><updated>2021-01-16T06:01:54+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2021-01-16:/machine-learning-design-patterns-problem-representation-part-1.html</id><summary type="html">&lt;p&gt;In my previous &lt;a href="/machine-learning-design-patterns-data-representation.html"&gt;post&lt;/a&gt;, I have discussed data representation patterns presented in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;. In this post, I would like to talk about the next topic in the book mentioned above - &lt;strong&gt;problem representation design patterns&lt;/strong&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my previous &lt;a href="/machine-learning-design-patterns-data-representation.html"&gt;post&lt;/a&gt;, I have discussed data representation patterns presented in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;. In this post, I would like to talk about the next topic in the book mentioned above - problem representation. After taking care of our data representation, this is the next logical step (the next chapter in the book). Problem representation is probably the most critical decision to make for an ML problem - the decision to model a given problem will define how well our solution will perform. The good news is that we do not need to make the right decision from the start - as, with everything in ML, it is an iterative process. When you find yourself struggling to solve your problem with regression, try classification (always try classification if you can).&lt;/p&gt;
&lt;p&gt;I will do it differently this time - instead of just discussing patterns, I will define a task which we will solve using different design patterns. This way, we will be able to compare results and see the influence of problem representation.&lt;/p&gt;
&lt;p&gt;In this post, I will concentrate on &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt; design patterns. Next time I will cover &lt;strong&gt;Rebalancing&lt;/strong&gt; and &lt;strong&gt;Ensemble&lt;/strong&gt; design patterns. But first, let's define our task.&lt;/p&gt;
&lt;h1&gt;Task: Predict song popularity&lt;/h1&gt;
&lt;p&gt;To illustrate the design patterns mentioned above, I will try to predict track popularity on Spotify only using track (mostly audio) features such as &lt;em&gt;danceability&lt;/em&gt;, &lt;em&gt;liveness&lt;/em&gt;, and &lt;em&gt;tempo&lt;/em&gt;. You can download data from &lt;a href="https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks"&gt;Kaggle&lt;/a&gt; with the full list of features used. Also, you can find a notebook to reproduce results &lt;a href="https://github.com/Va1da2/blog-notebooks/blob/main/reframing-neutral-class-ml-design-patterns/Reframing%20and%20Neutral%20Class.ipynb"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Popularity&lt;/strong&gt; is a Spotify metric calculated for each track based on the number of plays and those plays' recency. Given the above definition, we expect that new songs will be more popular on average. So we will limit ourselves to the tracks released in the past decade. Again, for more details, see the notebook.&lt;/p&gt;
&lt;h1&gt;Reframing and Neutral Class Design Patterns&lt;/h1&gt;
&lt;p&gt;Like I have already mentioned, in this post, I would like to discuss and illustrate two problem representation design patterns - &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt;. The first should be bread and butter for any data scientist, but the second I haven't seen anywhere else so far.&lt;/p&gt;
&lt;h1&gt;Solving the task&lt;/h1&gt;
&lt;p&gt;The task is simple - predict which song is popular. Popularity rating is an integer from 1 to 100. It's not a real-valued target, as a house price would be, but regression is a reasonable approach here. However, if the only thing we want to predict is popularity (not the rating itself), we can make this task a classification problem by thresholding the popularity index.&lt;/p&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;You can find more on data in the notebook referenced above. &lt;/p&gt;
&lt;p&gt;We have 19,788 tracks collected for the years 2011-2020 and 11 audio features that we will use to predict the song's popularity. We split this dataset randomly to train, and test sets - 15,830 and 3,958 tracks fall in each, respectively.&lt;/p&gt;
&lt;p&gt;The popularity has the following distributions for train and test splits
&lt;img alt="popularity-distribution" src="/images/popularity_distribution_train_test_better.png"&gt;&lt;/p&gt;
&lt;p&gt;Not ideal, but close. Statistics are close too:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Statistic&lt;/th&gt;
&lt;th&gt;Popularity /train/&lt;/th&gt;
&lt;th&gt;Popularity /test/&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Count&lt;/td&gt;
&lt;td&gt;15,830&lt;/td&gt;
&lt;td&gt;3,958&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td&gt;58.89&lt;/td&gt;
&lt;td&gt;58.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Std&lt;/td&gt;
&lt;td&gt;15.30&lt;/td&gt;
&lt;td&gt;15.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Min&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25th&lt;/td&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50th&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;75th&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max&lt;/td&gt;
&lt;td&gt;99&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;For these types of problems, I use correlation (rank correlation) as an evaluation metric. Since I want to know which tracks are/will be popular, I am interested to know does my predicted score indicates higher actual popularity - and that's a correlation. I will use &lt;strong&gt;Spearman's Rho&lt;/strong&gt; and &lt;strong&gt;Kendall's Tau&lt;/strong&gt; with corresponding p-values for evaluation.&lt;/p&gt;
&lt;h2&gt;Regression&lt;/h2&gt;
&lt;p&gt;As mentioned, regression is a reasonable approach to model popularity. Using sklearn's &lt;em&gt;GradientBoostingRegressor&lt;/em&gt; to model popularity gives us the following results.&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-regression" src="/images/predicted_vs_actual_regression_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.270&lt;/td&gt;
&lt;td&gt;5.52e-67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.184&lt;/td&gt;
&lt;td&gt;6.47e-66&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There is a statistically significant correlation between predicted and actual popularity. But let's see if we can do better.&lt;/p&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;p&gt;We can reframe our original regression problem to classification by thresholding the popularity index to create classes for our model to predict. Classification usually works better than regression because we simplify our task a bit. If the only thing we need is a relative ordering of the songs - scores from the classification model are enough.&lt;/p&gt;
&lt;h3&gt;Split at the median&lt;/h3&gt;
&lt;p&gt;The most straightforward approach for binary class creation - split the scores at the median value. With this approach, we will have nicely distributed training data. It might not work if your dataset values are very skewed towards one or the other end of the scale. In that case, you will have to experiment if having the imbalanced dataset works out OK, or you should employ some rebalancing design patterns (the subject of my next post!).&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-classification-simple" src="/images/predicted_vs_actual_classification1_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.307&lt;/td&gt;
&lt;td&gt;5.62e-87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.210&lt;/td&gt;
&lt;td&gt;2.68e-85&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is an 11% and 14% improvement in Spearman's and Kendall's correlations, respectively. Not bad for simple thresholding. Let's see if we can improve upon this result.&lt;/p&gt;
&lt;h3&gt;Middle values removed&lt;/h3&gt;
&lt;p&gt;Another trick that I always experiment with is simplifying a problem for the algorithm by removing intermediate values. Depending on your question, this might help the model distinguish between &lt;code&gt;good&lt;/code&gt; and &lt;code&gt;bad&lt;/code&gt; examples. However, this means that we are discarding some of our data. And therefore, this tradeoff will improve results when an amount of discarded data is less costly than the removed ambiguity.&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-classification-middle-removed" src="/images/predicted_vs_actual_classification2_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.303&lt;/td&gt;
&lt;td&gt;7.06e-85&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.207&lt;/td&gt;
&lt;td&gt;1.22e-82&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In our example, I took top and bottom 40 percent of the dataset, and as we can see, this ended up hurting performance. I also varied the amount of data I remove, and this approach always has a detrimental effect on model performance in this case.&lt;/p&gt;
&lt;h2&gt;Classification with Neutral Class&lt;/h2&gt;
&lt;p&gt;The above example tried to remove ambiguity introduced by splitting continuous response at a threshold but hurt model performance because we removed part of the data. The &lt;strong&gt;Neutral Class&lt;/strong&gt; design pattern takes care of that - instead of eliminating some data, we give it a class and use it for prediction. Then, at inference time, we only look at the &lt;code&gt;high&lt;/code&gt; class probability.&lt;/p&gt;
&lt;p&gt;The neutral class design pattern is also useful when we train models on human labelers output, like medical imaging applications. When human labelers do not agree, we can represent that uncertainty to our model via neutral class.&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-classification-neutral-class" src="/images/predicted_vs_actual_classification3_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.308&lt;/td&gt;
&lt;td&gt;7.61e-88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.211&lt;/td&gt;
&lt;td&gt;6.66e-86&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, an additional 0.5% increase in Spearman's and Kendall's metrics compared to our simple classification approach. It won't make-or-break your ML project, but the size of the effect will depend on the dataset. And with this simple change, any positive impact is welcome.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Looking at the scatterplots (or even looking at the label distribution), we see that we could employ more tricks to increase model performance. We could train a model to distinguish &lt;code&gt;0&lt;/code&gt; (or very low) scores from the higher ones and then introduce another model to predict higher scores on the first's output. It is called the &lt;strong&gt;Cascade&lt;/strong&gt; design pattern, and I will write about it in the future. &lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;This post explored two ML design patterns - &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt;. We have shown that these can work in tandem by reframing a problem at hand from regression to classification and then adding a neutral class to help our model distinguish better high and low values. These two steps add a nice performance boost if we can live with having an estimate of a probability of the &lt;code&gt;high class&lt;/code&gt; rather than an estimate of the metric in question.&lt;/p&gt;
&lt;p&gt;Next, we will look at &lt;strong&gt;Rebalancing&lt;/strong&gt; and &lt;strong&gt;Ensembles&lt;/strong&gt; design patterns. I think they are useful daily since most of the exciting problems are imbalanced by nature (predicting high-value customers, churners, etc.).&lt;/p&gt;
&lt;h3&gt;Other Posts in Machine Learning Design Patterns Series&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/machine-learning-design-patterns-data-representation.html"&gt;Data Representation Design Patters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/machine-learning-design-patterns-problem-representation-part-2.html"&gt;Problem Representation Design Patterns Part 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Data Science"></category><category term="machine learning"></category><category term="ml"></category><category term="data science"></category><category term="design patterns"></category><category term="ml problem representation"></category></entry><entry><title>Machine Learning Design Patterns: Data Representation</title><link href="https://va1da2.github.io/machine-learning-design-patterns-data-representation.html" rel="alternate"></link><published>2021-01-09T12:26:48+02:00</published><updated>2021-01-09T12:26:48+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2021-01-09:/machine-learning-design-patterns-data-representation.html</id><summary type="html">&lt;p&gt;Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can benefit immensely by following such idioms. In this and following posts, I will discuss ML patterns outlined in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;. Let us start with &lt;strong&gt;Data Representation Patterns&lt;/strong&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can benefit immensely by following such idioms. In this and following posts, I will discuss ML patterns outlined in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Data Representation Design Patterns&lt;/h1&gt;
&lt;p&gt;Let us start with &lt;strong&gt;Data Representation Patterns&lt;/strong&gt;. These patterns focus on the feature engineering part of the ML workflow. It would be a stretch to call some simple and commonly used techniques a &lt;code&gt;design pattern&lt;/code&gt;:
1. Linear transformations: &lt;strong&gt;min-max scaling&lt;/strong&gt;, &lt;strong&gt;clipping&lt;/strong&gt;, and &lt;strong&gt;z-score normalization&lt;/strong&gt;; 
2. Non-linear transformations: &lt;strong&gt;logarithms&lt;/strong&gt;, &lt;strong&gt;taking a root of a value&lt;/strong&gt;, &lt;strong&gt;histogram equalization&lt;/strong&gt;, and &lt;strong&gt;box-cox transform&lt;/strong&gt;; 
3. Categorical feature handling: &lt;strong&gt;one-hot-encoding&lt;/strong&gt; (this might be a pattern, but it's just too common these days, and there are quite a few better approaches);
4. Handling array of categorical features: &lt;strong&gt;array statistics&lt;/strong&gt;;&lt;/p&gt;
&lt;p&gt;Patterns that authors describe in the book, and we will discuss here are: &lt;strong&gt;Embeddings&lt;/strong&gt;, &lt;strong&gt;Feature Cross&lt;/strong&gt;, &lt;strong&gt;Multimodal Input&lt;/strong&gt;, and &lt;strong&gt;Hashed Feature&lt;/strong&gt;, &lt;/p&gt;
&lt;h2&gt;Embeddings&lt;/h2&gt;
&lt;p&gt;An embedding is a learnable representation of a high cardinality feature into a lower-dimensional space while preserving information. I would even say that embeddings can enhance categorical features by encoding them to make ML tasks easier for a learning algorithm. These days (DL boom), it seems that everybody knows to use an image embedding in some ML task if an image is a contributing factor. However, this pattern is not only about this type of embedding. Let us see what problems this pattern address.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;The problem with categorical features is that simple conversion techniques do not capture relationships between classes and rely on the algorithm to distill those relationships. Whatever categorical feature you have:
A day of the week - if day number starts from Sunday, Saturday and Sunday will be far apart in terms of numerical value, while they are most likely close to each other in meaning;
A book or a food category - we can one-hot encode them, but we cannot encode relationships among the classes this way;
Very high cardinality features, such as a fine-grained catalog of an e-shop, can be impractical one-hot encode.
All categorical features would benefit from embedding. The cardinality may also play a role here - if we have a vast vocabulary for categorical feature (fine-grained catalog in an e-shop can be prohibitively large to one-hot encode)&lt;/p&gt;
&lt;p&gt;There is also a problem with unstructured data incorporation into our algorithm. Text, image, and audio are rich sources of information but are not easy to incorporate in algorithms that are not specifically built for those types of input.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;Embeddings are a great way to encode categorical or unstructured data so that other algorithms can use a better representation of the feature.&lt;/p&gt;
&lt;p&gt;Embedding categorical features is probably the best and easiest way to improve model performance. By learning an embedding for any categorical feature, we extract information on how categories relate to each other for the task we are trying to learn. Embedding boosts the algorithm's performance and has a nice side effect - we can use learned embeddings in different learning tasks.&lt;/p&gt;
&lt;p&gt;Embedding unstructured data is almost the only way to include that data in learning models. Suppose we have a problem that would benefit from including unstructured information. In that case, we could use one of the pre-trained models for images (one of many trained on ImageNet) or text (e.g., Glove/Word2Vec word vectors) and include the provided information to any other model together with structured data.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;The embedding pattern is unique - there is no real alternative to it. We can encode categories by integers or one-hot encode them, but it is not even close in terms of model performance. If possible, we should always use &lt;strong&gt;Embedding pattern&lt;/strong&gt;; the only consideration is about what type of embedding to include.&lt;/p&gt;
&lt;h2&gt;Feature Cross&lt;/h2&gt;
&lt;p&gt;Combining feature values and making every combination a different feature helps our algorithm learn relationships between characteristics faster.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;When our features relate in non-linear ways, we can improve our model by providing those non-linear features by "crossing" them.&lt;/p&gt;
&lt;p&gt;One typical example is a time of day and the day of the week for some event. For instance, we want to predict a demand for bikes in a specific city bike-sharing spot. Having just time of day and day of the week might not be enough, and an explicit &lt;code&gt;AND&lt;/code&gt; relationship could improve our model.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;Feature cross is a simple combination of two or more categorical features (or bucketed numerical ones). For example, if we have the day of a week (Monday, Tuesday, etc.) and the time of day (1 PM, 2 PM, etc.) features, we could get the time of the day of a week feature (Monday 1 PM, Monday 2 PM, Tuesday 1 PM, etc.). Crossing features increase cardinality considerably; therefore, using this pattern with the embedding pattern could yield even better results.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;Feature crosses are a simple way to introduce non-linearity in our models and help to learn relationships faster. This pattern is even better when using it with the embeddings pattern discussed above. However, we should consider features that we want to cross carefully. Since this pattern increases the model's complexity quite a bit, we should not go and cross all our features.&lt;/p&gt;
&lt;h2&gt;Multimodal Input&lt;/h2&gt;
&lt;p&gt;When we have multiple representations of the phenomenon that we want to model, we should include all those representations in our algorithm.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;Many algorithms that are available online are designed for a specific type of input - image models (ResNets), text models (all the transformers out there), audio models (I haven't used any myself so far). However, there is a significant class of problems where we would like to use several types of inputs — a combination of structured and unstructured data. For example, modeling social media campaign results include all the above input types.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;By employing the embedding pattern, we can join different types of input into a single model. We can either use any of the pre-trained models for image/text input and extract the last layer features to concatenate them with numerical or categorical features. For example, if we classify a complicated situation from a picture, having metadata about that situation (date and time, weather, geographical location, etc.) can significantly increase our model's accuracy.
Additionally, this pattern is useful with the same data but different representations - bucketing is the simplest example. For example, if we have a distance feature in a dataset as a continuous feature, bucketing could help us learn non-linear relationships. Maybe very short and very long distances correlate with our outcome.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;As data scientists/machine learning engineers, we should always seek new features to add to our models. By seeking out features from different modalities of the phenomenon, we can build better models. Similarly, we can increase our model's performance by presenting the same information from a different angle.&lt;/p&gt;
&lt;h2&gt;Hashed Feature&lt;/h2&gt;
&lt;p&gt;The hashed feature design pattern is an interesting approach meant to address cold start, incomplete vocabulary, and model size problems.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;Having high cardinality categorical features poses three main challenges when building an ML model:
* Not all categories might exist in the training dataset;
* The number of categories might be prohibitively big;
* Cold-start problem;&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;The proposed solution is a deterministic hash function (authors of the book proposes &lt;a href="https://github.com/google/farmhash"&gt;&lt;code&gt;FarmHash&lt;/code&gt;&lt;/a&gt;). We would hash a given categorical feature in a pre-defined number of buckets and would use that as a feature instead of the original value. With this approach, we tick all the above boxes:
* All categories would get a bucket, even those that were not present in the training dataset;
* We control how many buckets we hash our values into;
* Even new values of the feature that wasn't available during training time would get handled (our model would not error out.)&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;This pattern is the least appealing of all the data representation design patterns. There might be situations where this is necessary, but it should be a necessity. By randomly assigning buckets to our feature values, we might group very different values, and therefore our model would suffer. So I would go with anything other than hashing, if possible, describing high cardinality features with values metadata or descriptive statistics. Maybe grouping those values based on those statistics and then using that as an input for future values to handle the cold start problem.&lt;/p&gt;
&lt;p&gt;On the other hand, this is the only new design pattern, so I am delighted that the authors decided to include this.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This post is the first in the series about ML design patterns. The book contains several more chapters I intend to write about, including &lt;strong&gt;Problem Representation Design Pattern&lt;/strong&gt;, &lt;strong&gt;Model Training Patterns&lt;/strong&gt;, &lt;strong&gt;Model Serving Design Patterns&lt;/strong&gt;, &lt;strong&gt;Reproducibility Design Patterns&lt;/strong&gt;, and &lt;strong&gt;Responsible AI Design Patterns&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Other Posts in Machine Learning Design Patterns Series&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/machine-learning-design-patterns-problem-representation-part-1.html"&gt;Problem Representation Design Patterns Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/machine-learning-design-patterns-problem-representation-part-2.html"&gt;Problem Representation Design Patterns Part 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Data Science"></category><category term="machine learning"></category><category term="ml"></category><category term="data science"></category><category term="design patterns"></category></entry><entry><title>2020 in Books</title><link href="https://va1da2.github.io/2020-in-books.html" rel="alternate"></link><published>2020-12-23T13:11:46+02:00</published><updated>2020-12-23T13:11:46+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2020-12-23:/2020-in-books.html</id><summary type="html">&lt;p&gt;2020 was not easy by any measure. COVID-19, adapting to parenthood, rollercoaster of working at an early stage startup - it was a memorable year (though I might not remember a lot because of sleep deprivation). However, I still managed to read a few books and there were several that I would like to reflect upon. So here is an overview and I will share more details on a few books in later posts.&lt;/p&gt;</summary><content type="html">&lt;p&gt;2020 was not easy by any measure. COVID-19, adapting to parenthood, rollercoaster of working at an early stage startup - it was a memorable year (though I might not remember a lot because of sleep deprivation). However, I still managed to read a few books and there were several that I would like to reflect upon. So here is an overview and I will share more details on a few books in later posts.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I have read 27 books this year (a bit less than the past two years), but I will aim to read even less (maybe around 20 or so) going forward and spent a little bit more time on the ones that I do read. I can roughly categorize my books into five categories: Work-related reading (Software engineering, Machine Learning/Data Science, Product Management, and People Management), Fiction, Real-life events/people, Parenthood (Familly psychology), and Miscellaneous. One observation I can see right at the start - I do not read fiction enough. Just one this year. I will try to fix this next year. Another issue is that I haven't read enough Machine Learning / Data Science books. I have read a lot of blog posts or articles, but that is a separate matter. I will try to address this next year too.&lt;/p&gt;
&lt;h1&gt;Fiction (1)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;1984 by George Orwell&lt;/strong&gt; - This is one of the classics that I haven't read before and I started it with no expectations. I have also read &lt;code&gt;The Animal Farm&lt;/code&gt; previously and was not impressed (maybe because I did live a bit under the soviet regime and it still echos to this day in Lithuania). However, what I found was a very relevant novel in times of &lt;code&gt;fake news&lt;/code&gt; and &lt;code&gt;alternative facts&lt;/code&gt;. This novel didn't feel like a stretch of the imagination. It was a very chilling realization, especially considering that there are groups of people in the 21st century that live by very different facts than scientifically accepted ones. But the most appealing idea for me in this book was that of a &lt;code&gt;human condition&lt;/code&gt;. No matter how severe the punishment, no matter how negligible the odds, humans are drawn to act out their inner beliefs and disregard the dangers. It is tragic and maybe even stupid for any one single individual, but very very beneficial for humanity - no matter how small the odds, some will succeed, and that will move the entire species forward.&lt;/p&gt;
&lt;h1&gt;Parenthood (3)&lt;/h1&gt;
&lt;p&gt;I have started reading parenthood books last year as I was expecting my son's birth. What I didn't expect, was to find that it was mostly learning about myself. It provided me with insight into my childhood behavior. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hold On To Your Kids by Gordon Neufeld &amp;amp; Gabor Mate&lt;/strong&gt; - I have sought this book out after watching the author's &lt;a href="https://www.youtube.com/watch?v=gX4EFwv76Vg"&gt;interview&lt;/a&gt;. I highly recommend watching it and the book. Although the book is a bit repetitive at times and is a bit dated (it was first published in 2004) the message it carries is timeless and couldn't be more relevant today - &lt;code&gt;attachment&lt;/code&gt; with our children is crucial for them to develop and for us to be able to perform our parental duties, while &lt;code&gt;peer orientation&lt;/code&gt; robs our children of the secure connections they need to develop and parents of "power" to guide our children towards maturation.&lt;/p&gt;
&lt;p&gt;Another book about parenting that I have read this year was  &lt;strong&gt;Brain Rules For Baby by John Medina&lt;/strong&gt;. In this book, John Medina reviews research in child development and makes the connection between &lt;code&gt;smart&lt;/code&gt; and &lt;code&gt;happy&lt;/code&gt; kids - surprise surprise, the same behavior and methods that help children develop make them happy. Or to put it the other way around - kids need to be happy and safe for them to develop. Not an earth-shattering revelation, but just a reminder where to concentrate the effort.&lt;/p&gt;
&lt;p&gt;On a related topic, I have also read a very good book on the relationship in a family where both sides want to excel not only in the relationship, but at work too - &lt;strong&gt;Couples that Work by Jennifer Petriglieri&lt;/strong&gt;. Based on the authors' research, this book examines what transitions couples face in their life together, and give advice on how to go through these transitions as a couple. I've read this because having a child impacted my relationship with my wife in ways I didn't expect. And according to the author, having a child is the first of three transitions that couples go through life and to manage this successfully requires quite a big shift in mindset. It helped me understand the dynamics of our family a bit better and even shifted my perspective a little bit. It is a work in progress, but I am glad I've read this book and recommend it to anyone going through tough times together - especially if you are a new family.&lt;/p&gt;
&lt;h1&gt;Work-related (5)&lt;/h1&gt;
&lt;p&gt;This year I haven't read enough on ML/DS topics (book-wise) and will remedy that during the upcoming year. The one technical book I have read I enjoyed very much - &lt;strong&gt;Building Machine Learning Powered Applications by Emmanuel Ameisen&lt;/strong&gt;. This is a very good overview of what it takes to deliver ML products. Data management and machine learning matters, but looking from the product perspective, how we define a problem matters a lot more. It also went into the iteration process of improving models and suggested very interesting ideas on how to improve ML models with the help of other ML models - train a model that predicts which inputs confuses our main model the most and gather/label more of these is genius.&lt;/p&gt;
&lt;p&gt;Having read little on technical aspects of ML/DS I have read a bit more on the managerial aspects of my work - I am taking care of a few data scientists at work. I wanted to be a bit more prepared, so I invested time in this area. I cannot distinguish one book, since I have gained something from them all.
&lt;strong&gt;Radical Candor by Kim Scott&lt;/strong&gt; highlights the benefits and even the necessity of being candid with your peers. Giving feedback the right way can transform you, your colleagues, and the company. Kim Scott also plots the trajectory that could be followed to introduce radical candor in their own company or department. I struggled following her advice - I was not persistent enough to extract candid feedback from my peers and by the same token was not able to give candid feedback back (as I did not want to offend them). I am aware of my failure and will try to improve upon this next year.
Another book that I have read with similar ideas was &lt;strong&gt;No Rules Rules by Reed Hastings &amp;amp; Erin Meyer&lt;/strong&gt;. The ideas behind high performance and innovation culture @ Netflix is very counter-intuitive, but with the right people, it works as is continuously demonstrated by the company. Authors walk readers through the origins of the philosophy and what it takes to maintain it. I have to admit - I am a bit intimidated by this approach. Especially now, when I have to devote a significant part of my time to family, I probably would be constantly stressed about my performance. But I would also gladly take on the challenge if presented with the opportunity. I actually even went to their hiring site to see if they have any remote positions. 
Another great book, and sort of a classic, on management, is &lt;strong&gt;High Output Management by Andrew S. Grove&lt;/strong&gt;. The main concept I took from it is &lt;code&gt;task-relevant maturity&lt;/code&gt;. This is crucial to consider, especially in the early-stage startup environment. If you hire people that are not yet capable to perform without much supervision velocity will suffer. I do not have a good answer to this question, but it seems that the two-to-one ratio of senior to junior employees is a good start.
While ML/DS is not software engineering, there is still a lot to be learned from the discipline. Therefore, &lt;strong&gt;The Nature of Software Development by Ron Jeffries&lt;/strong&gt; was an easy overview of how software is delivered in an agile environment.&lt;/p&gt;
&lt;h1&gt;Real-life events/people (10)&lt;/h1&gt;
&lt;p&gt;This category can be further broken down to biographies about prominent people and descriptions of events. I have started reading more about prominent people and events last year and found it to be a great inspiration - a lot of the time events happen at random and we, humans, tend to overfit a lot to the patterns we perceive. It is very refreshing to see how much luck plays a role and how much perseverance helps to achieve great things.&lt;/p&gt;
&lt;h2&gt;Biographies&lt;/h2&gt;
&lt;p&gt;I have read or listened to a few books on prominent people and the theme that emerged was &lt;code&gt;perseverance&lt;/code&gt;. In the &lt;strong&gt;The Snowball: Warren Buffett and the Business of Life by Alice Schroeder&lt;/strong&gt; the author goes through the history of Warren Buffett in fine detail - detailing his entrepreneurial mindset from childhood to the present day. From paperboy to the prominent businessman he became. From collecting and selling lost golf balls from the golf club ponds to the board rooms of the most prominent companies in the world. What emerges from this book is that there are no shortcuts - if you work hard enough for long enough (with a bit of good luck) you will achieve great things. It may not be Warren Buffett's level of fame, you need to be gifted for such hights, I believe, but it will be great things.
After watching &lt;code&gt;The First Man&lt;/code&gt; movie a couple of times, I've read the book &lt;strong&gt;First Man by James R. Hansen&lt;/strong&gt; too. Apollo missions are the greatest achievement in my book, so I wanted to read about the man that became the name for it. NASA couldn't have picked a better man to be the first on another world. I also thoroughly enjoyed &lt;strong&gt;Skunk Works by Ben R. Rich &amp;amp; Leo Janos&lt;/strong&gt;. Reading about amazing engineering that made Lockheed A-12 possible (among other Skunk Works innovations) highlights what is possible when people put their minds to it. 
&lt;strong&gt;Facts and Fears by James R. Clapper&lt;/strong&gt; is from a very different field, that I have little interest in and therefore knowledge of - Intelligence. And because of that, it was very illuminating. It was very interesting to listen to Clappers' perspectives and insights on the processes and abilities and shortcomings of intelligence. What it can and cannot deliver and how much uncertainty there is always present in intelligence assessment.&lt;/p&gt;
&lt;p&gt;The one book that maybe should not entirely be in this category is &lt;strong&gt;A Carlin Home Companion by Kelly Carlin&lt;/strong&gt;. I wanted to read this book to get an insight into George and what kind of family man he was. I did get that, but I also got a lot of Kelly's teenage experiences, which wasn't my point - but it is her's book. &lt;/p&gt;
&lt;h2&gt;Events/History&lt;/h2&gt;
&lt;p&gt;Similar to biographies, I like reading about significant events and experiences or achievements. All these books are amazing and I would recommend reading them all as they highlight what we, humans, can achieve or endure to achieve our goals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Making of the Atomic Bomb by Richard Rhodes&lt;/strong&gt; is probably a bit different from others, as in this book Rhodes goes all the events that enable and later made the atomic bomb. Chemistry and physics discoveries are discussed and notable scientists are described. What I did get from this book that something truly significant can only be achieved via cooperation between people. It highlighted the importance of diversity (of opinion, point of view, you name it) in achieved making something so unbelievable at the time. It also showed that not all great men saw this the same way - there were quite a few that doubted the possibility of making it. However, the description of the impact of the bomb on two Japanese cities left the strongest impact on me. After reading around the Internet and watching &lt;code&gt;The Pacific&lt;/code&gt; I was of the impression that bombs were the only way to end that war. But there had to be a better way. The horrors of the atomic bomb impact described in the book was something I can scarcely imagine.&lt;/p&gt;
&lt;p&gt;Another very different development, which also had and still has a huge effect on us all is the Internet, and &lt;strong&gt;How the Internet Happened by Brian McCullough&lt;/strong&gt; describes beautifully how this phenomenon happened. From the birth of the World Wide Web in the CERN, through the dot come bonanza, to the present day. It was very interesting to hear about the early days, the first companies (e.g. Netscape) and how things rapidly evolved.&lt;/p&gt;
&lt;p&gt;Other books in this category described one particular event and super-human efforts that were required to come out the other side of them. &lt;strong&gt;Touching the Void by Joe Simpson&lt;/strong&gt; describes one trip to the Andes by the author, where he survived after he fell (was cut-of from the rope, to be precise) into the crevasse. I have seen the movie by the same name several times before, but wanted to read the book too. The journey Joe had to make is unbelievable and it only highlights the strength of the human spirit - we are not the ones who would wait still for our demise. &lt;strong&gt;Into the Thin Air by Jon Krakauer&lt;/strong&gt; described a bit different journey but also already widely known because of several adaptations to the big screen. The book describes events during a guided expedition to Mt. Everest in 1996 (also known as &lt;a href="https://en.wikipedia.org/wiki/1996_Mount_Everest_disaster"&gt;1996 Mount Everest disaster&lt;/a&gt;). It is a chilling reminder of what wait us when we venture a bit too far out of our comfort zone and do not pay attention. The fact that two of the most experienced mountaineers (Rob Hall and Scott Fisher) perished during this disaster highlights the fact of how fallible we all are. Mountaineers that should have known better fell to hubris, that they were hired to curb. 
I have entertained the thought of &lt;a href="https://www.adventureconsultants.com/expeditions/antarctica/south-pole-all-the-way/#:~:text=The%20route%20from%20Hercules%20Inlet,adventure%20under%20their%20own%20power."&gt;going to the South Pole&lt;/a&gt;. So to understand better what it takes and what hardships prominent explorers faced, I wanted to read something on the topic. &lt;strong&gt;Endurance by Alfred Lansing&lt;/strong&gt; came up in my search and even if this is not the kind of journey I would ultimately embark upon, I wanted to see how the early explorers faired and what challenges they faced. Ernest Shackleton's journey is fascinating. They embarked on this journey (departed South Georgia) on December 1914 and were locked in the ice quite quickly. However, the real journey for them started once &lt;code&gt;Endurance&lt;/code&gt;, their ship, sank to the bottom of the Weddell Sea - most of the crew managed to save themselves through cold and wet and hunger. Once again, while reading these books you cannot escape the feeling that everything is possible.&lt;/p&gt;
&lt;h1&gt;Miscelanious (8)&lt;/h1&gt;
&lt;p&gt;The last category is a "catch-all" for all the books that I do not know better categories yet. Topics are varied too. From &lt;strong&gt;Kabinetas 339 by Dovydas Pancerovas &amp;amp; Birute Dovydaite&lt;/strong&gt;, a Lithuanian book on the 17th government dealings and relationships with journalists. Maybe I am spoiled by the quality of the Western books, but it is poorly written. However, I am glad I bought it. We need to support journalists and authors to produce more, that's the only way they will improve. 
In the &lt;strong&gt;The Art of Statistics by David Spiegelhalter&lt;/strong&gt;, Spiegelhalter goes through data stories explaining how the correct statistical analysis can uncover hidden patterns. This is a book on statistics but aimed at the general audience and I enjoyed reading it.
Other books described entrepreneurial efforts, examined their impact on subjects and the world at large. &lt;strong&gt;Chaos Monkeys by Antonio G. Martinez&lt;/strong&gt; is the author's take on entrepreneurship and Silicon Valley culture. It vividly describes a journey from Goldman Sachs trading desk, where Martinez worked as a quant after flunking from Physics PhD program) to Silicon Valley startup scene to founding and selling his own company to working at Facebook and leaving this all to sailing and writing for a living. I didn't like the tone of the book. It was too "Goldman Sachs'ish" and the author tried to show how well-read he is by inserting needless metaphors everywhere he could fit them in, but I appreciated his take on the industry and the overall evaluation of the culture.
&lt;strong&gt;The Everything Store by Brad Stone&lt;/strong&gt; details the rise of Amazon. You have to appreciate the foresight of Jeff Bezos on this. I liked the description of his childhood and college years, as I didn't know much about him beforehand. It also detailed failed experiments along the way. It only further solidified my belief that making the right call is not the goal. Making the call and then working hard to implement it is. Being flexible to let it go, once it does not work out is part of success.
&lt;strong&gt;The Next Great Migration by Sonia Shah&lt;/strong&gt; investigated the cultural bias against migrants that is prevalent currently. Shah contrasts these views with conservationists, who believe that certain animals should be banned from areas where they have not been found recently because they are not "native". The emerging consensus among scientists is that there is no such thing as "native" animal species. Animals (humans included) and plants migrate all the time. However, the issue of refugees is not that simple. Animals who migrate to other areas need to fight with the ones that are already inhabiting them for space, as humans, we think we are better than that, but we have to have rules on migration then. It's not black and white - migration is always a force for good. However, what I think is clear (and there is a lot of evidence to support this), that diversity enriches our lives. Diversity of opinions, experiences, and of cultural backgrounds.
&lt;strong&gt;Human Errors by Nathan H. Lents&lt;/strong&gt; was interesting to listen, because it showcases through a series of "flaws", that evolution truly is random. And it does not get everything right. It's just random mutations that some help our cause of survival and reproduction and some don't. And that timescale at which this force works is too big for us humans to comprehend.
The &lt;strong&gt;Pale Blue Dot by Carl Sagan&lt;/strong&gt; failed to impress me. I don't know exactly why - maybe it is because this is a really old book, maybe because I am already a firm supporter of the scientific approach and exploration of the unknown. It was interesting to hear the information on the old experiments (all the drones sent to other planets of the Sun), but other than that, this book left nothing for me. However, I do have &lt;a href="https://upload.wikimedia.org/wikipedia/commons/7/73/Pale_Blue_Dot.png"&gt;the picture&lt;/a&gt; to remind me daily about the scale of our life.
One last book that I managed to squeeze in during the festive period was &lt;strong&gt;Man's Search For Meaning by  Viktor E. Frankl&lt;/strong&gt;. A psychiatrist's account of the horrors he had to endure in the Nazie concentration camps (Auschwitz and others). It's a short read and it's more of a collection of observations than a detailed account of the time author spent in these camps. It is written with the purpose to illustrate a authors' ideas and a school of psychotherapy he founded - &lt;a href="https://en.wikipedia.org/wiki/Logotherapy"&gt;Logotherapy&lt;/a&gt;. A few quotes from the book would illustrate it best:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Those who have a 'why' to live, can bear with almost any 'how'.&lt;/p&gt;
&lt;p&gt;Nietzsche&lt;/p&gt;
&lt;p&gt;In some ways suffering ceases to be suffering at the moment it finds a meaning, such as the meaning of a sacrifice.&lt;/p&gt;
&lt;p&gt;Viktor E. Frankl&lt;/p&gt;
&lt;p&gt;Everything can be taken from a man but one thing: the last of the human freedoms—to choose one’s attitude in any given set of circumstances, to choose one’s own way.&lt;/p&gt;
&lt;p&gt;Viktor E. Frankl&lt;/p&gt;
&lt;/blockquote&gt;</content><category term="Books"></category><category term="2020 Review"></category><category term="Year Review"></category></entry><entry><title>A Case For Agile Data Science</title><link href="https://va1da2.github.io/a-case-for-agile-data-science.html" rel="alternate"></link><published>2020-11-23T05:50:58+02:00</published><updated>2020-11-23T05:50:58+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2020-11-23:/a-case-for-agile-data-science.html</id><summary type="html">&lt;p&gt;I have encountered a lot of resistance in the data science community against agile methodology and specifically the scrum framework. I don’t see it this way and claim that most disciplines would improve by adopting an agile mindset. We will go through a typical scrum sprint to highlight the compatibility of the data science process and the agile development process. Finally, we discuss when a scrum is not an appropriate process to follow. If you are a consultant working on many projects at a time or your work requires deep concentration on a single and narrow issue (narrow, so that you alone can solve it).&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This article was first published in &lt;a href="https://towardsdatascience.com/agile-data-science-data-science-can-and-should-be-agile-c719a511b868"&gt;TowardsDataScience&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;tl;dr;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;I have encountered a lot of resistance in the data science community against agile methodology and specifically scrum framework. &lt;/li&gt;
&lt;li&gt;I don’t see it this way and claim that most disciplines would improve by adopting an agile mindset. &lt;/li&gt;
&lt;li&gt;We will go through a typical scrum sprint to highlight the compatibility of the data science process and the agile development process. &lt;/li&gt;
&lt;li&gt;Finally, we discuss when a scrum is not an appropriate process to follow. If you are a consultant working on many projects at a time or your work requires deep concentration on a single and narrow issue (narrow, so that you alone can solve it).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;I have found a medium post recently, which claims that &lt;a href="https://towardsdatascience.com/why-scrum-is-awful-for-data-science-db3e5c1bb3b4"&gt;Scrum is awful for data science&lt;/a&gt;. I’m afraid I have to disagree and would like to make a case for Agile Data Science.&lt;/p&gt;
&lt;p&gt;Ideas for this post are significantly influenced by the &lt;a href="https://www.amazon.co.uk/Agile-Data-Science-Russell-Jurney/dp/1491960116"&gt;Agile Data Science 2.0&lt;/a&gt; book (which I highly recommend) and personal experience. I am eager to know other experiences, so please share them in the comments.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;First, we need to agree on what data science is and how it solves business problems so we can investigate the process of data science and how agile (and specifically Scrum) can improve it.&lt;/p&gt;
&lt;h2&gt;What is Data Science?&lt;/h2&gt;
&lt;p&gt;There are countless definitions online. For example, &lt;a href="https://en.wikipedia.org/wiki/Data_science"&gt;Wikipedia&lt;/a&gt; gives such a description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In my opinion, it is quite an accurate definition of what data science tries to accomplish. But I would simplify this definition further.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Science solves business problems by combining business understanding, data and algorithms.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Compared to the definition in Wikipedia, I would like to stress that data scientists should aim to &lt;strong&gt;solve&lt;/strong&gt; business problems rather than &lt;strong&gt;“extract knowledge and insights.”&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;How Data Science Solves business problems?&lt;/h2&gt;
&lt;p&gt;So data science is here to solve business problems. We need to accomplish a few things along the way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Understand the business problem;&lt;/li&gt;
&lt;li&gt;Identify and acquire available data;&lt;/li&gt;
&lt;li&gt;Clean / transform / prepare data;&lt;/li&gt;
&lt;li&gt;Select and fit an appropriate “model” for a given data;&lt;/li&gt;
&lt;li&gt;Deploy model to “production” — this is our attempt to solving a given problem;&lt;/li&gt;
&lt;li&gt;Monitoring performance;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As with everything, there are countless ways to go about implementing those steps, but I will try to persuade you that the agile (incremental and iterative) approach brings the most value to the company and the most joy to data scientists.&lt;/p&gt;
&lt;h2&gt;Agile Data Science Manifesto&lt;/h2&gt;
&lt;p&gt;I took this from page 6 in the &lt;a href="https://www.amazon.co.uk/Agile-Data-Science-Russell-Jurney/dp/1491960116"&gt;Agile Data Science 2.0&lt;/a&gt; book, so you are encouraged to read the original, but here it is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Iterate, iterate, iterate — tables, charts, reports, predictions.&lt;/li&gt;
&lt;li&gt;Ship intermediate output. Even failed experiments have output.&lt;/li&gt;
&lt;li&gt;Prototype experiments over implementing tasks.&lt;/li&gt;
&lt;li&gt;Integrate the tyrannical opinion of data in product management.&lt;/li&gt;
&lt;li&gt;Climb up and down the data-value pyramid as you work.&lt;/li&gt;
&lt;li&gt;Discover and pursue the critical path to a killer product.&lt;/li&gt;
&lt;li&gt;Get meta. Describe the process, not just the end state.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not all the steps are self-explanatory, and I encourage you to go and read what Russel Jurney had to say, but I hope that the main idea is clear — we share and intermediate output, and we iterate to achieve value.&lt;/p&gt;
&lt;p&gt;Given the above preliminaries, let us go over a standard week for a scrum team. And we will assume a one week sprint.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Scrum Team Sprint&lt;/h2&gt;
&lt;h3&gt;Day 1&lt;/h3&gt;
&lt;p&gt;There are many sprint structure variations, but I will assume that planning is done on Monday morning. The team will decide which user stories from the product backlog will be transferred to the Sprint backlog. The most pressing issue for our business, as evident from the backlog ranking, is customer fraud — fraudulent transactions are causing our valuable customers out of our platform. During the previous backlog refinement session, the team already discussed this task, and the product owner got additional information from the Fraud Investigation team. So during the meeting, the team decides to start with a simple experiment (and already is thinking of interesting iterations further down the road) — an initial model based on simple features of the transaction and participating users. Work is split so that the data scientist can go and have a look at the data team identified for this problem. The data engineer will set up the pipeline for model output integration to DWH systems, and the full-stack engineer starts to set up a page for transaction review and alert system for the Fraud Investigation team.&lt;/p&gt;
&lt;h3&gt;Day 2&lt;/h3&gt;
&lt;p&gt;At the start of Tuesday, all team gathers and shares progress. Data scientist shows a few graphs which indicate that even with limited features, we will have a decent model. At the same time, the data engineer is already halfway through setting up the system to score incoming transactions with the new model. The full-stack engineer is also progressing nicely, and just after a few minutes, everyone is back at their desk working on the agreed tasks.&lt;/p&gt;
&lt;h3&gt;Day 3&lt;/h3&gt;
&lt;p&gt;As with Tuesday, the team starts Wednesday with a standup meeting to share their progress. There is already a simple model build and some accuracy and error rate numbers. The data engineer shows the infrastructure for the transaction scoring, and the team discusses how the features arrive at the system and what needs to be done for them to be ready for the algorithm. The full-stack engineer shows the admin panel with metadata on transactions is displayed and the triggering mechanism. Another discussion follows on the threshold value for the model output to trigger a message for a fraud analyst. The team agrees that we need to be able to adjust this value since different models might have different distributions, and also, depending on other variables, we might want to increase and decrease the number of approved transactions.&lt;/p&gt;
&lt;h3&gt;Day 4&lt;/h3&gt;
&lt;p&gt;On Thursday, the team already has all the pieces, and during the standup, discuss how to integrate those pieces. Team also outlines how to best monitor models in production, so that model performance could be evaluated and also degradation could be detected before it causes any real damage. They agree that a simple dashboard for monitoring accuracy and error rates will suffice for now.&lt;/p&gt;
&lt;h3&gt;Day 5&lt;/h3&gt;
&lt;p&gt;Friday is a demo day. During standup, the team discusses the last issues remaining with the first iteration of the transaction fraud detection. Team members prepare for the meeting with the fraud analysts that will be using this solution.&lt;/p&gt;
&lt;p&gt;During the demo, the team shows what they have built for the fraud analysts. The team presents performance metrics and their implications for the fraud analysts. All feedback is converted to tasks for future sprints.
Another vital part of the Sprint is a retrospective — meeting where the team discusses three things:
1. What went well in the Sprint;
2. What could be improved;
3. What will we commit to improving in the next Sprint;&lt;/p&gt;
&lt;h3&gt;Further down the road&lt;/h3&gt;
&lt;p&gt;During the next Sprint, the team is working on another most important item from the product backlog. It might be feedback from the fraud analysts, or it might be something else that the product owner thinks will improve the overall business the most. However, the team closely monitors the performance of the initial version of the solution. It will continue to do so because ML solutions are sensitive to changes in underlying assumptions that the model made about data distribution.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Above is a relatively “clean” exposition of the scrum process for data science solutions. Real-world rarely is that way, but I wanted to convey a few points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data Science cannot stand on its own. If we’re going to impact the real world we have to collaborate in a cross-functional team, it should be a part of a wider team;&lt;/li&gt;
&lt;li&gt;Iteration is critical in data science, and we should expose artifacts of those iterations to our stakeholders to receive feedback as fast as possible;&lt;/li&gt;
&lt;li&gt;Scrum is a framework that is designed for iterative progress. Therefore it is a perfect fit for data science work;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, it is &lt;em&gt;not&lt;/em&gt; a framework for any endeavor. If your job requires you to think deeply for days, then Scrum and agile would probably be very disruptive and counterproductive. Also, if your work requires you to handle a lot of different and small data science-related tasks, following Scrum would be inappropriate, and maybe Kanban should be considered. However, typical product data science work is not like that. Iteration is king, and getting feedback fast is key to providing the right solutions to business problems.&lt;/p&gt;
&lt;h3&gt;In summary&lt;/h3&gt;
&lt;p&gt;Data Science is a perfect fit for the Scrum with a single modification — we do not expect to ship finished models. Instead, we ship artifacts of our work and solicit feedback from our stakeholders so we can make progress faster. Project managers might not like data science for the unpredictability of the progress, but iteration is not at fault, it is the only way forward.&lt;/p&gt;</content><category term="Data Science"></category><category term="data science"></category><category term="agile"></category><category term="iterative development"></category></entry></feed>