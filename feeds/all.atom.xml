<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Thoughts on Data Science, ML and Startups</title><link href="https://va1da2.github.io/" rel="alternate"></link><link href="https://va1da2.github.io/feeds/all.atom.xml" rel="self"></link><id>https://va1da2.github.io/</id><updated>2021-01-16T06:01:54+02:00</updated><entry><title>Machine Learning Design Patterns: Problem Representation Part 1</title><link href="https://va1da2.github.io/machine-learning-design-patterns-problem-representation-part-1.html" rel="alternate"></link><published>2021-01-16T06:01:54+02:00</published><updated>2021-01-16T06:01:54+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2021-01-16:/machine-learning-design-patterns-problem-representation-part-1.html</id><summary type="html">&lt;p&gt;In my previous &lt;a href="/machine-learning-design-patterns-data-representation.html"&gt;post&lt;/a&gt; I have discussed data representation patterns presented in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;. In this post I would like to talk about the next topic in the above-mentioned book - &lt;strong&gt;problem representation design patterns&lt;/strong&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my previous &lt;a href="/machine-learning-design-patterns-data-representation.html"&gt;post&lt;/a&gt; I have discussed data representation patterns presented in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;. In this post I would like to talk about the next topic in the above-mentioned book - problem representation. After taking care of our data representation, this is the next logical step (and therefore the next chapter in the book). This is also probably the most important decision to make for an ML problem - the decision how to model a given problem will define how well our solution will perform. The good bit is that we do not need to make this decision correct from the start - as with everything in ML, it is an iterative process, and when you find that your problem cannot be solved by regression, try classification (always try classification if you can).&lt;/p&gt;
&lt;p&gt;I will do it differently this time - instead of just discussing patterns, I will define a task which we will solve using different design patterns. This way we will be able to compare results and see the influence of problem representation.&lt;/p&gt;
&lt;p&gt;In this post, I will concentrate on &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt; design patterns. Next time I will cover &lt;strong&gt;Rebalancing&lt;/strong&gt; and &lt;strong&gt;Ensemble&lt;/strong&gt; design patterns. But first, let's define our task.&lt;/p&gt;
&lt;h1&gt;Task: Predict song popularity&lt;/h1&gt;
&lt;p&gt;To illustrate the above-mentioned design patterns I will try to predict track popularity on Spotify only using track (mostly audio) features such as &lt;em&gt;danceability&lt;/em&gt;, &lt;em&gt;liveness&lt;/em&gt; and &lt;em&gt;tempo&lt;/em&gt;. Data can be downloaded from &lt;a href="https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks"&gt;Kaggle&lt;/a&gt; and the full list of features used and I will provide the accompanying notebook shortly (needs a bit of editing).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Popularity&lt;/strong&gt; is a Spotify metric calculated for each track mostly based on the number of plays and the recency of those plays. Given the above definition, we expect that new songs will be more popular on average. So we will limit ourselves 
to the tracks that were released in the past decade. Again, for more details, see the notebook.&lt;/p&gt;
&lt;h1&gt;Reframing and Neutral Class Design Patterns&lt;/h1&gt;
&lt;p&gt;Like I have already mentioned, in this post I would like to discuss and illustrate two problem representation design patterns - &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt;. The first should be bread and butter for any data scientist, but the second I haven't seen anywhere else so far.&lt;/p&gt;
&lt;h1&gt;Solving the task&lt;/h1&gt;
&lt;p&gt;The task is simple - predict which song is popular. Popularity rating is an integer from 1 to 100. It's not a real-valued target, as for example price of a house would be, but regression is a reasonable approach here. However, if the only thing we want to predict is popularity (not the rating itself), we can make this task a classification problem by thresholding the popularity index.&lt;/p&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;More on data can be found in the notebook referenced above. &lt;/p&gt;
&lt;p&gt;We have 19,788 tracks collected for the years 2011-2020. We split this dataset randomly to train and test sets - 15,830 and 3,958. We have 11 audio features to predict popularity from.&lt;/p&gt;
&lt;p&gt;The popularity has the following distributions for train and test splits
&lt;img alt="popularity-distribution" src="/images/popularity_distribution_train_test_better.png"&gt;&lt;/p&gt;
&lt;p&gt;Not ideal, but close. Statistics are close too:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Statistic&lt;/th&gt;
&lt;th&gt;Popularity /train/&lt;/th&gt;
&lt;th&gt;Popularity /test/&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Count&lt;/td&gt;
&lt;td&gt;15,830&lt;/td&gt;
&lt;td&gt;3,958&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td&gt;58.89&lt;/td&gt;
&lt;td&gt;58.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Std&lt;/td&gt;
&lt;td&gt;15.30&lt;/td&gt;
&lt;td&gt;15.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Min&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25th&lt;/td&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50th&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;75th&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max&lt;/td&gt;
&lt;td&gt;99&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;For these types of problems, I use correlation (rank correlation) as an evaluation metric. Since I want to know which tracks are/will be popular, I am interested to know does my predicted score indicates higher actual popularity - and that's a correlation. I will use &lt;strong&gt;Spearman's Rho&lt;/strong&gt; and &lt;strong&gt;Kendall's Tau&lt;/strong&gt; with corresponding p-values.&lt;/p&gt;
&lt;h2&gt;Regression&lt;/h2&gt;
&lt;p&gt;As mentioned, regression is a reasonable approach to model popularity. Running it with sklearn's &lt;em&gt;GradientBoostingRegressor&lt;/em&gt;, which produces results as follows&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-regression" src="/images/predicted_vs_actual_regression_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.270&lt;/td&gt;
&lt;td&gt;5.25e-67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.184&lt;/td&gt;
&lt;td&gt;6.20e-66&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There is a statistically significant correlation between predicted and actual popularity. But let's see if we can do better.&lt;/p&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;p&gt;We can reframe our original regression problem to classification by thresholding the popularity index to create classes for our model to predict. This usually works better than regression, because we simplify the problem a bit. If the only thing we need is a relative ordering of the songs - scores from the classification model are perfectly good for it.&lt;/p&gt;
&lt;h3&gt;Split at the median&lt;/h3&gt;
&lt;p&gt;The simplest approach for binary class creation - split the scores at the median value. With this approach, we will have nicely distributed training data. This might not work if in your dataset values are very skewed towards one or the other end of the scale. In that case, you will have to experiment if having the imbalanced dataset works out OK or you should employ some re-balancing design patterns (the subject of my next post!).&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-classification-simple" src="/images/predicted_vs_actual_classification1_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.306&lt;/td&gt;
&lt;td&gt;7.69e-87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.210&lt;/td&gt;
&lt;td&gt;3.73e-85&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is an 11% and 14% improvement in Spearman's and Kendall's correlations respectively. Not bad for simple thresholding. Let's see if we can improve upon this result.&lt;/p&gt;
&lt;h3&gt;Middle values removed&lt;/h3&gt;
&lt;p&gt;Another trick that I always experiment with is trying to simplify a problem for the algorithm by removing middle values. Depending on you problem this might help for the model to better distinguish between &lt;code&gt;good&lt;/code&gt; and &lt;code&gt;bad&lt;/code&gt; examples. However, this means that we are discarding some of our data. And therefore this tradeoff will improve results when an amount of discarded data is less costly than the ambiguity that is removed.&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-classification-middle-removed" src="/images/predicted_vs_actual_classification2_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.303&lt;/td&gt;
&lt;td&gt;7.98e-87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.207&lt;/td&gt;
&lt;td&gt;1.39e-82&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In our example, I took top and bottom 40 percent of the dataset, and as we can see this ended up hurting performance. I also varied the amount of data I remove, and this approach always has a detrimental effect on model performance in this case.&lt;/p&gt;
&lt;h2&gt;Classification with Neutral Class&lt;/h2&gt;
&lt;p&gt;The above example tried to remove ambiguity introduced by splitting continuous variable at a threshold, but hurt model performance because part of the data was removed. The &lt;strong&gt;Neutral Class&lt;/strong&gt; design pattern takes care of that - instead of removing part of the data, we give it a class and use it for prediction. Then, at inference time, we only look at the &lt;code&gt;high&lt;/code&gt; class probability.&lt;/p&gt;
&lt;p&gt;The neutral class design pattern is also useful when we train models on human labelers output - medical imaging applications for example. When human labelers do not agree, we can represent that uncertainty to our model via neutral class.&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-classification-neutral-class" src="/images/predicted_vs_actual_classification3_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.308&lt;/td&gt;
&lt;td&gt;8.23e-88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.211&lt;/td&gt;
&lt;td&gt;7.16e-86&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, an additional 1% and 0.5% increase in Spearman's and Kendall's metrics respectively compared to our simple classification approach. This won't make-or-break your ML project, but the effect it has also will depend on the dataset. And with this simple change, any positive effect is welcome.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Looking at the scatterplots (or even looking at the label distribution) we see that more can be done to increase model performance. We could train a model to distinguish &lt;code&gt;0&lt;/code&gt; (or very low) scores from the higher ones and then train another model to predict higher scores on the output of the first. This is called the &lt;strong&gt;Cascade&lt;/strong&gt; design pattern and I will write about it in the future. &lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, we explored two ML design patterns - &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt;. I have shown that these can work in tandem by reframing a problem at hand from regression to classification and then adding a neutral class to help our model distinguish better high and low values. These two steps add a nice performance boost if we can live with having an estimate of a probability of the &lt;code&gt;high class&lt;/code&gt; rather than an estimate of the &lt;code&gt;value itself&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In my next post, I will take a look into &lt;strong&gt;Rebalancing&lt;/strong&gt; and &lt;strong&gt;Ensembles&lt;/strong&gt; design patterns. I think they are useful daily since most of the interesting problems are imbalanced by nature (predicting high-value customers, churners, etc.).&lt;/p&gt;</content><category term="Data Science"></category><category term="machine learning"></category><category term="ml"></category><category term="data science"></category><category term="design patterns"></category></entry><entry><title>Machine Learning Design Patterns: Data Representation</title><link href="https://va1da2.github.io/machine-learning-design-patterns-data-representation.html" rel="alternate"></link><published>2021-01-09T12:26:48+02:00</published><updated>2021-01-09T12:26:48+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2021-01-09:/machine-learning-design-patterns-data-representation.html</id><summary type="html">&lt;p&gt;Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can benefit immensely by following such patterns. In this and following posts I will discuss ML patterns outlined in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;. Let us start with &lt;strong&gt;Data Representation Patterns&lt;/strong&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can benefit immensely by following such patterns. In this and following posts I will discuss ML patterns outlined in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Data Representation Design Patterns&lt;/h1&gt;
&lt;p&gt;Let us start with &lt;strong&gt;Data Representation Patterns&lt;/strong&gt;. These patterns focus on the feature engineering part of the ML workflow. Simple techniques that could not reasonably called patterns include &lt;strong&gt;min-max scaling&lt;/strong&gt;, &lt;strong&gt;clipping&lt;/strong&gt;, and &lt;strong&gt;z-score normalization&lt;/strong&gt; for linear scaling, &lt;strong&gt;logarithms&lt;/strong&gt;, &lt;strong&gt;taking a root of a value&lt;/strong&gt;, &lt;strong&gt;histogram equalization&lt;/strong&gt;, and &lt;strong&gt;box-cox transform&lt;/strong&gt; for non-linear transformations, &lt;strong&gt;one-hot-encoding&lt;/strong&gt; for categorical feature handling (this might be a pattern, but it's just too common these days and there are quite a few better approaches) and &lt;strong&gt;array statistics&lt;/strong&gt; for the array of categorical (or even numeric) inputs feature.&lt;/p&gt;
&lt;p&gt;Patterns that are presented and we will discuss here are: &lt;strong&gt;Embeddings&lt;/strong&gt;, &lt;strong&gt;Feature Cross&lt;/strong&gt;, &lt;strong&gt;Multimodal Input&lt;/strong&gt;, and &lt;strong&gt;Hashed Feature&lt;/strong&gt;, &lt;/p&gt;
&lt;h2&gt;Embeddings&lt;/h2&gt;
&lt;p&gt;An embedding is a learnable representation of a high cardinality feature into a lower-dimensional space while preserving information. I would even say, that embeddings can enhance categorical features by encoding them in such a way, that makes ML task easier for a learning algorithm. These days (DL boom) it seems that everybody knows to use an image embedding in some ML task if an image should be included as a feature. However, this pattern is not only about this type of embedding. Let us see what problems this pattern address.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;The problem with categorical features is that simple conversion techniques do not capture relationships between classes and therefore rely on the algorithm to distill those relationships. Whatever categorical feature you have - a day of the week (especially if you start the week on Sunday, Saturday and Sunday will be far apart in terms of numerical value, while they are most likely close to each other in meaning), book or food category (we can one-hot encode them, but we cannot encode relationships among the categories this way) or any other categorical variable. The cardinality may also play a role here - if we have a huge vocabulary for categorical feature (fine-grained catalog in an e-shop can be prohibitively big to one-hot encode)&lt;/p&gt;
&lt;p&gt;There is also a problem with unstructured data incorporation to our algorithm. Text, image, and audio are rich sources of information but are not easy to incorporate in algorithms that are not specifically built for those types of input.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;Embeddings are a great way to encode categorical or unstructured data so that other algorithms can use a better representation of the feature.&lt;/p&gt;
&lt;p&gt;Embedding categorical features is probably the best and easiest way to improve model performance. By learning an embedding for any categorical feature, we extract information on how categories relate to each other for the task we are trying to learn. This boost performance of the algorithm and has a nice side effect - those learned embeddings can be used in other learning tasks.&lt;/p&gt;
&lt;p&gt;Embedding unstructured data is almost the only way to include that data in learning models. If we have a problem that could be better solved by including unstructured information, we could use one of the pre-trained models for images (one of many trained on ImageNet) or text (e.g. Glove/Word2Vec word vectors) and include the provided information to any other model together with structured data.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;The embedding pattern is quite unique - there is no real alternative to it. We can just encode categories by integers, or we can one-hot encode them, but it is not even close. If at all possible, we should always use &lt;strong&gt;Embedding pattern&lt;/strong&gt;, the only consideration is about what type of embedding to include.&lt;/p&gt;
&lt;h2&gt;Feature Cross&lt;/h2&gt;
&lt;p&gt;By combining feature values and making every combination a separate feature, we help our algorithm to learn relationships between features faster.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;When our features relate in non-linear ways, we can improve our model by providing those non-linear features by "crossing" them.&lt;/p&gt;
&lt;p&gt;One common example is a time of day and the day of the week for some event. If we want to predict some attribute of the given event (for example demand for bikes in a certain city bike-sharing spot) having just time of day and day of the week might not be enough and an explicit &lt;code&gt;AND&lt;/code&gt; relationship could improve our model.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;Feature cross is a simple multiplication of two categorical features (or bucketed numerical ones), such that if we have the day of a week (Monday, Tuesday, etc.) and the time of day (1 PM, 2 PM, etc.) features, we could get the time of the day of a week feature (Monday 1 PM, Monday 2 PM, Tuesday 1 PM, etc.). Crossing features increase cardinality considerably, therefore using this pattern with the embedding pattern could yield even better results.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;Feature crosses are a simple and very powerful way to introduce non-linearity in our models and help to learn relationships faster. This pattern is even better when using it with the embeddings pattern discussed above. However, we should consider features that we want to cross carefully. Since this pattern increases the complexity of the model quite a bit, we should not go and cross all our features.&lt;/p&gt;
&lt;h2&gt;Multimodal Input&lt;/h2&gt;
&lt;p&gt;When we have multiple representations of the phenomenon that we want to model, we should try to include all those representations in our algorithm.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;Many algorithms that are available online are designed for a specific type of input - image models (ResNets), text models (all the transformers out there), audio models (I haven't used any myself so far). However, there is a big class of problems where we would like to use several different types of inputs - numerical and categorical features combined with text or image or even all of the above (for example modeling social media campaign results include all the above input types).&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;By employing the embedding pattern, we can join different types of input into a single model. We can either use any of the pre-trained models for image/text input and extract the final layer features to concatenate them with numerical or categorical features. For example, if we are making predictions about the scene in the picture - having metadata on the scene (date and time information, weather information, or camera information) can significantly increase the accuracy of our model.&lt;/p&gt;
&lt;p&gt;Additionally, this pattern can be used with the same data, but different representations - bucketing is the simplest example. For example, if we have a distance feature in a dataset as a continuous feature, bucketing it could help to learn non-linear relationships, where very short and very long distances are correlated with our outcome.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;As data scientists/machine learning engineers, we should always seek new features to add to our models. By seeking out features from different modalities of the phenomenon we can build better models. Similarly, we can increase our model's performance by presenting the same information from a different angle.&lt;/p&gt;
&lt;h2&gt;Hashed Feature&lt;/h2&gt;
&lt;p&gt;The hashed feature design pattern is a very interesting approach meant for addressing cold start, incomplete vocabulary, and model size problems.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;Having high cardinality categorical features poses three main challenges when building an ML model:
* Not all categories might exist in the training dataset;
* The number of categories might be prohibitively big;
* Cold-start problem;&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;The proposed solution is a deterministic hash function (authors of the book proposes &lt;a href="https://github.com/google/farmhash"&gt;&lt;code&gt;FarmHash&lt;/code&gt;&lt;/a&gt;). We would hash a given categorical feature in a pre-defined number of buckets and would use that as a feature instead of the original value. With this approach, we tick all the above boxes:
* All categories would get a bucket, even those that were not present in the training dataset;
* We control how many buckets we hash our values into;
* Even new values of the feature, that weren't available during training time would get handled (our model would not error out.)&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;This pattern is the least appealing of all the data representation design patterns. There might be situations where this is necessary, but it should be a necessity. By randomly assigning buckets to our feature values we might group very different values together and therefore our model would suffer. So I would go with anything other than hashing if at all possible - describing high cardinality feature with values metadata or descriptive statistics. Maybe grouping those values based on those statistics and then using that as an input for future values to handle the cold start problem.&lt;/p&gt;
&lt;p&gt;On the other hand, this is the only new design pattern, so I am very glad that the authors decided to include this.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This is just the first post in the series about ML design patterns. The book contains several more chapters I intend to write about, including &lt;strong&gt;Problem Representation Design Pattern&lt;/strong&gt;, &lt;strong&gt;Model Training Patterns&lt;/strong&gt;, &lt;strong&gt;Model Serving Design Patterns&lt;/strong&gt;, &lt;strong&gt;Reproducibility Design Patterns&lt;/strong&gt;, and &lt;strong&gt;Responsible AI Design Patterns&lt;/strong&gt;.&lt;/p&gt;
&lt;h1&gt;Next posts:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Part 1 of &lt;strong&gt;Problem Representation Design Pattern&lt;/strong&gt;, where I discuss &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt; design patterns, can be found &lt;a href="/machine-learning-design-patterns-problem-representation-part-1.html"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;</content><category term="Data Science"></category><category term="machine learning"></category><category term="ml"></category><category term="data science"></category><category term="design patterns"></category></entry><entry><title>2020 in Books</title><link href="https://va1da2.github.io/2020-in-books.html" rel="alternate"></link><published>2020-12-23T13:11:46+02:00</published><updated>2020-12-23T13:11:46+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2020-12-23:/2020-in-books.html</id><summary type="html">&lt;p&gt;2020 was not easy by any measure. COVID-19, adapting to parenthood, rollercoaster of working at an early stage startup - it was a memorable year (though I might not remember a lot because of sleep deprivation). However, I still managed to read a few books and there were several that I would like to reflect upon. So here is an overview and I will share more details on a few books in later posts.&lt;/p&gt;</summary><content type="html">&lt;p&gt;2020 was not easy by any measure. COVID-19, adapting to parenthood, rollercoaster of working at an early stage startup - it was a memorable year (though I might not remember a lot because of sleep deprivation). However, I still managed to read a few books and there were several that I would like to reflect upon. So here is an overview and I will share more details on a few books in later posts.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I have read 27 books this year (a bit less than the past two years), but I will aim to read even less (maybe around 20 or so) going forward and spent a little bit more time on the ones that I do read. I can roughly categorize my books into five categories: Work-related reading (Software engineering, Machine Learning/Data Science, Product Management, and People Management), Fiction, Real-life events/people, Parenthood (Familly psychology), and Miscellaneous. One observation I can see right at the start - I do not read fiction enough. Just one this year. I will try to fix this next year. Another issue is that I haven't read enough Machine Learning / Data Science books. I have read a lot of blog posts or articles, but that is a separate matter. I will try to address this next year too.&lt;/p&gt;
&lt;h1&gt;Fiction (1)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;1984 by George Orwell&lt;/strong&gt; - This is one of the classics that I haven't read before and I started it with no expectations. I have also read &lt;code&gt;The Animal Farm&lt;/code&gt; previously and was not impressed (maybe because I did live a bit under the soviet regime and it still echos to this day in Lithuania). However, what I found was a very relevant novel in times of &lt;code&gt;fake news&lt;/code&gt; and &lt;code&gt;alternative facts&lt;/code&gt;. This novel didn't feel like a stretch of the imagination. It was a very chilling realization, especially considering that there are groups of people in the 21st century that live by very different facts than scientifically accepted ones. But the most appealing idea for me in this book was that of a &lt;code&gt;human condition&lt;/code&gt;. No matter how severe the punishment, no matter how negligible the odds, humans are drawn to act out their inner beliefs and disregard the dangers. It is tragic and maybe even stupid for any one single individual, but very very beneficial for humanity - no matter how small the odds, some will succeed, and that will move the entire species forward.&lt;/p&gt;
&lt;h1&gt;Parenthood (3)&lt;/h1&gt;
&lt;p&gt;I have started reading parenthood books last year as I was expecting my son's birth. What I didn't expect, was to find that it was mostly learning about myself. It provided me with insight into my childhood behavior. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hold On To Your Kids by Gordon Neufeld &amp;amp; Gabor Mate&lt;/strong&gt; - I have sought this book out after watching the author's &lt;a href="https://www.youtube.com/watch?v=gX4EFwv76Vg"&gt;interview&lt;/a&gt;. I highly recommend watching it and the book. Although the book is a bit repetitive at times and is a bit dated (it was first published in 2004) the message it carries is timeless and couldn't be more relevant today - &lt;code&gt;attachment&lt;/code&gt; with our children is crucial for them to develop and for us to be able to perform our parental duties, while &lt;code&gt;peer orientation&lt;/code&gt; robs our children of the secure connections they need to develop and parents of "power" to guide our children towards maturation.&lt;/p&gt;
&lt;p&gt;Another book about parenting that I have read this year was  &lt;strong&gt;Brain Rules For Baby by John Medina&lt;/strong&gt;. In this book, John Medina reviews research in child development and makes the connection between &lt;code&gt;smart&lt;/code&gt; and &lt;code&gt;happy&lt;/code&gt; kids - surprise surprise, the same behavior and methods that help children develop make them happy. Or to put it the other way around - kids need to be happy and safe for them to develop. Not an earth-shattering revelation, but just a reminder where to concentrate the effort.&lt;/p&gt;
&lt;p&gt;On a related topic, I have also read a very good book on the relationship in a family where both sides want to excel not only in the relationship, but at work too - &lt;strong&gt;Couples that Work by Jennifer Petriglieri&lt;/strong&gt;. Based on the authors' research, this book examines what transitions couples face in their life together, and give advice on how to go through these transitions as a couple. I've read this because having a child impacted my relationship with my wife in ways I didn't expect. And according to the author, having a child is the first of three transitions that couples go through life and to manage this successfully requires quite a big shift in mindset. It helped me understand the dynamics of our family a bit better and even shifted my perspective a little bit. It is a work in progress, but I am glad I've read this book and recommend it to anyone going through tough times together - especially if you are a new family.&lt;/p&gt;
&lt;h1&gt;Work-related (5)&lt;/h1&gt;
&lt;p&gt;This year I haven't read enough on ML/DS topics (book-wise) and will remedy that during the upcoming year. The one technical book I have read I enjoyed very much - &lt;strong&gt;Building Machine Learning Powered Applications by Emmanuel Ameisen&lt;/strong&gt;. This is a very good overview of what it takes to deliver ML products. Data management and machine learning matters, but looking from the product perspective, how we define a problem matters a lot more. It also went into the iteration process of improving models and suggested very interesting ideas on how to improve ML models with the help of other ML models - train a model that predicts which inputs confuses our main model the most and gather/label more of these is genius.&lt;/p&gt;
&lt;p&gt;Having read little on technical aspects of ML/DS I have read a bit more on the managerial aspects of my work - I am taking care of a few data scientists at work. I wanted to be a bit more prepared, so I invested time in this area. I cannot distinguish one book, since I have gained something from them all.
&lt;strong&gt;Radical Candor by Kim Scott&lt;/strong&gt; highlights the benefits and even the necessity of being candid with your peers. Giving feedback the right way can transform you, your colleagues, and the company. Kim Scott also plots the trajectory that could be followed to introduce radical candor in their own company or department. I struggled following her advice - I was not persistent enough to extract candid feedback from my peers and by the same token was not able to give candid feedback back (as I did not want to offend them). I am aware of my failure and will try to improve upon this next year.
Another book that I have read with similar ideas was &lt;strong&gt;No Rules Rules by Reed Hastings &amp;amp; Erin Meyer&lt;/strong&gt;. The ideas behind high performance and innovation culture @ Netflix is very counter-intuitive, but with the right people, it works as is continuously demonstrated by the company. Authors walk readers through the origins of the philosophy and what it takes to maintain it. I have to admit - I am a bit intimidated by this approach. Especially now, when I have to devote a significant part of my time to family, I probably would be constantly stressed about my performance. But I would also gladly take on the challenge if presented with the opportunity. I actually even went to their hiring site to see if they have any remote positions. 
Another great book, and sort of a classic, on management, is &lt;strong&gt;High Output Management by Andrew S. Grove&lt;/strong&gt;. The main concept I took from it is &lt;code&gt;task-relevant maturity&lt;/code&gt;. This is crucial to consider, especially in the early-stage startup environment. If you hire people that are not yet capable to perform without much supervision velocity will suffer. I do not have a good answer to this question, but it seems that the two-to-one ratio of senior to junior employees is a good start.
While ML/DS is not software engineering, there is still a lot to be learned from the discipline. Therefore, &lt;strong&gt;The Nature of Software Development by Ron Jeffries&lt;/strong&gt; was an easy overview of how software is delivered in an agile environment.&lt;/p&gt;
&lt;h1&gt;Real-life events/people (10)&lt;/h1&gt;
&lt;p&gt;This category can be further broken down to biographies about prominent people and descriptions of events. I have started reading more about prominent people and events last year and found it to be a great inspiration - a lot of the time events happen at random and we, humans, tend to overfit a lot to the patterns we perceive. It is very refreshing to see how much luck plays a role and how much perseverance helps to achieve great things.&lt;/p&gt;
&lt;h2&gt;Biographies&lt;/h2&gt;
&lt;p&gt;I have read or listened to a few books on prominent people and the theme that emerged was &lt;code&gt;perseverance&lt;/code&gt;. In the &lt;strong&gt;The Snowball: Warren Buffett and the Business of Life by Alice Schroeder&lt;/strong&gt; the author goes through the history of Warren Buffett in fine detail - detailing his entrepreneurial mindset from childhood to the present day. From paperboy to the prominent businessman he became. From collecting and selling lost golf balls from the golf club ponds to the board rooms of the most prominent companies in the world. What emerges from this book is that there are no shortcuts - if you work hard enough for long enough (with a bit of good luck) you will achieve great things. It may not be Warren Buffett's level of fame, you need to be gifted for such hights, I believe, but it will be great things.
After watching &lt;code&gt;The First Man&lt;/code&gt; movie a couple of times, I've read the book &lt;strong&gt;First Man by James R. Hansen&lt;/strong&gt; too. Apollo missions are the greatest achievement in my book, so I wanted to read about the man that became the name for it. NASA couldn't have picked a better man to be the first on another world. I also thoroughly enjoyed &lt;strong&gt;Skunk Works by Ben R. Rich &amp;amp; Leo Janos&lt;/strong&gt;. Reading about amazing engineering that made Lockheed A-12 possible (among other Skunk Works innovations) highlights what is possible when people put their minds to it. 
&lt;strong&gt;Facts and Fears by James R. Clapper&lt;/strong&gt; is from a very different field, that I have little interest in and therefore knowledge of - Intelligence. And because of that, it was very illuminating. It was very interesting to listen to Clappers' perspectives and insights on the processes and abilities and shortcomings of intelligence. What it can and cannot deliver and how much uncertainty there is always present in intelligence assessment.&lt;/p&gt;
&lt;p&gt;The one book that maybe should not entirely be in this category is &lt;strong&gt;A Carlin Home Companion by Kelly Carlin&lt;/strong&gt;. I wanted to read this book to get an insight into George and what kind of family man he was. I did get that, but I also got a lot of Kelly's teenage experiences, which wasn't my point - but it is her's book. &lt;/p&gt;
&lt;h2&gt;Events/History&lt;/h2&gt;
&lt;p&gt;Similar to biographies, I like reading about significant events and experiences or achievements. All these books are amazing and I would recommend reading them all as they highlight what we, humans, can achieve or endure to achieve our goals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Making of the Atomic Bomb by Richard Rhodes&lt;/strong&gt; is probably a bit different from others, as in this book Rhodes goes all the events that enable and later made the atomic bomb. Chemistry and physics discoveries are discussed and notable scientists are described. What I did get from this book that something truly significant can only be achieved via cooperation between people. It highlighted the importance of diversity (of opinion, point of view, you name it) in achieved making something so unbelievable at the time. It also showed that not all great men saw this the same way - there were quite a few that doubted the possibility of making it. However, the description of the impact of the bomb on two Japanese cities left the strongest impact on me. After reading around the Internet and watching &lt;code&gt;The Pacific&lt;/code&gt; I was of the impression that bombs were the only way to end that war. But there had to be a better way. The horrors of the atomic bomb impact described in the book was something I can scarcely imagine.&lt;/p&gt;
&lt;p&gt;Another very different development, which also had and still has a huge effect on us all is the Internet, and &lt;strong&gt;How the Internet Happened by Brian McCullough&lt;/strong&gt; describes beautifully how this phenomenon happened. From the birth of the World Wide Web in the CERN, through the dot come bonanza, to the present day. It was very interesting to hear about the early days, the first companies (e.g. Netscape) and how things rapidly evolved.&lt;/p&gt;
&lt;p&gt;Other books in this category described one particular event and super-human efforts that were required to come out the other side of them. &lt;strong&gt;Touching the Void by Joe Simpson&lt;/strong&gt; describes one trip to the Andes by the author, where he survived after he fell (was cut-of from the rope, to be precise) into the crevasse. I have seen the movie by the same name several times before, but wanted to read the book too. The journey Joe had to make is unbelievable and it only highlights the strength of the human spirit - we are not the ones who would wait still for our demise. &lt;strong&gt;Into the Thin Air by Jon Krakauer&lt;/strong&gt; described a bit different journey but also already widely known because of several adaptations to the big screen. The book describes events during a guided expedition to Mt. Everest in 1996 (also known as &lt;a href="https://en.wikipedia.org/wiki/1996_Mount_Everest_disaster"&gt;1996 Mount Everest disaster&lt;/a&gt;). It is a chilling reminder of what wait us when we venture a bit too far out of our comfort zone and do not pay attention. The fact that two of the most experienced mountaineers (Rob Hall and Scott Fisher) perished during this disaster highlights the fact of how fallible we all are. Mountaineers that should have known better fell to hubris, that they were hired to curb. 
I have entertained the thought of &lt;a href="https://www.adventureconsultants.com/expeditions/antarctica/south-pole-all-the-way/#:~:text=The%20route%20from%20Hercules%20Inlet,adventure%20under%20their%20own%20power."&gt;going to the South Pole&lt;/a&gt;. So to understand better what it takes and what hardships prominent explorers faced, I wanted to read something on the topic. &lt;strong&gt;Endurance by Alfred Lansing&lt;/strong&gt; came up in my search and even if this is not the kind of journey I would ultimately embark upon, I wanted to see how the early explorers faired and what challenges they faced. Ernest Shackleton's journey is fascinating. They embarked on this journey (departed South Georgia) on December 1914 and were locked in the ice quite quickly. However, the real journey for them started once &lt;code&gt;Endurance&lt;/code&gt;, their ship, sank to the bottom of the Weddell Sea - most of the crew managed to save themselves through cold and wet and hunger. Once again, while reading these books you cannot escape the feeling that everything is possible.&lt;/p&gt;
&lt;h1&gt;Miscelanious (8)&lt;/h1&gt;
&lt;p&gt;The last category is a "catch-all" for all the books that I do not know better categories yet. Topics are varied too. From &lt;strong&gt;Kabinetas 339 by Dovydas Pancerovas &amp;amp; Birute Dovydaite&lt;/strong&gt;, a Lithuanian book on the 17th government dealings and relationships with journalists. Maybe I am spoiled by the quality of the Western books, but it is poorly written. However, I am glad I bought it. We need to support journalists and authors to produce more, that's the only way they will improve. 
In the &lt;strong&gt;The Art of Statistics by David Spiegelhalter&lt;/strong&gt;, Spiegelhalter goes through data stories explaining how the correct statistical analysis can uncover hidden patterns. This is a book on statistics but aimed at the general audience and I enjoyed reading it.
Other books described entrepreneurial efforts, examined their impact on subjects and the world at large. &lt;strong&gt;Chaos Monkeys by Antonio G. Martinez&lt;/strong&gt; is the author's take on entrepreneurship and Silicon Valley culture. It vividly describes a journey from Goldman Sachs trading desk, where Martinez worked as a quant after flunking from Physics PhD program) to Silicon Valley startup scene to founding and selling his own company to working at Facebook and leaving this all to sailing and writing for a living. I didn't like the tone of the book. It was too "Goldman Sachs'ish" and the author tried to show how well-read he is by inserting needless metaphors everywhere he could fit them in, but I appreciated his take on the industry and the overall evaluation of the culture.
&lt;strong&gt;The Everything Store by Brad Stone&lt;/strong&gt; details the rise of Amazon. You have to appreciate the foresight of Jeff Bezos on this. I liked the description of his childhood and college years, as I didn't know much about him beforehand. It also detailed failed experiments along the way. It only further solidified my belief that making the right call is not the goal. Making the call and then working hard to implement it is. Being flexible to let it go, once it does not work out is part of success.
&lt;strong&gt;The Next Great Migration by Sonia Shah&lt;/strong&gt; investigated the cultural bias against migrants that is prevalent currently. Shah contrasts these views with conservationists, who believe that certain animals should be banned from areas where they have not been found recently because they are not "native". The emerging consensus among scientists is that there is no such thing as "native" animal species. Animals (humans included) and plants migrate all the time. However, the issue of refugees is not that simple. Animals who migrate to other areas need to fight with the ones that are already inhabiting them for space, as humans, we think we are better than that, but we have to have rules on migration then. It's not black and white - migration is always a force for good. However, what I think is clear (and there is a lot of evidence to support this), that diversity enriches our lives. Diversity of opinions, experiences, and of cultural backgrounds.
&lt;strong&gt;Human Errors by Nathan H. Lents&lt;/strong&gt; was interesting to listen, because it showcases through a series of "flaws", that evolution truly is random. And it does not get everything right. It's just random mutations that some help our cause of survival and reproduction and some don't. And that timescale at which this force works is too big for us humans to comprehend.
The &lt;strong&gt;Pale Blue Dot by Carl Sagan&lt;/strong&gt; failed to impress me. I don't know exactly why - maybe it is because this is a really old book, maybe because I am already a firm supporter of the scientific approach and exploration of the unknown. It was interesting to hear the information on the old experiments (all the drones sent to other planets of the Sun), but other than that, this book left nothing for me. However, I do have &lt;a href="https://upload.wikimedia.org/wikipedia/commons/7/73/Pale_Blue_Dot.png"&gt;the picture&lt;/a&gt; to remind me daily about the scale of our life.
One last book that I managed to squeeze in during the festive period was &lt;strong&gt;Man's Search For Meaning by  Viktor E. Frankl&lt;/strong&gt;. A psychiatrist's account of the horrors he had to endure in the Nazie concentration camps (Auschwitz and others). It's a short read and it's more of a collection of observations than a detailed account of the time author spent in these camps. It is written with the purpose to illustrate a authors' ideas and a school of psychotherapy he founded - &lt;a href="https://en.wikipedia.org/wiki/Logotherapy"&gt;Logotherapy&lt;/a&gt;. A few quotes from the book would illustrate it best:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Those who have a 'why' to live, can bear with almost any 'how'.&lt;/p&gt;
&lt;p&gt;Nietzsche&lt;/p&gt;
&lt;p&gt;In some ways suffering ceases to be suffering at the moment it finds a meaning, such as the meaning of a sacrifice.&lt;/p&gt;
&lt;p&gt;Viktor E. Frankl&lt;/p&gt;
&lt;p&gt;Everything can be taken from a man but one thing: the last of the human freedoms—to choose one’s attitude in any given set of circumstances, to choose one’s own way.&lt;/p&gt;
&lt;p&gt;Viktor E. Frankl&lt;/p&gt;
&lt;/blockquote&gt;</content><category term="Books"></category><category term="2020 Review"></category><category term="Year Review"></category></entry><entry><title>A Case For Agile Data Science</title><link href="https://va1da2.github.io/a-case-for-agile-data-science.html" rel="alternate"></link><published>2020-11-23T05:50:58+02:00</published><updated>2020-11-23T05:50:58+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2020-11-23:/a-case-for-agile-data-science.html</id><summary type="html">&lt;p&gt;I have encountered a lot of resistance in the data science community against agile methodology and specifically the scrum framework. I don’t see it this way and claim that most disciplines would improve by adopting an agile mindset. We will go through a typical scrum sprint to highlight the compatibility of the data science process and the agile development process. Finally, we discuss when a scrum is not an appropriate process to follow. If you are a consultant working on many projects at a time or your work requires deep concentration on a single and narrow issue (narrow, so that you alone can solve it).&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This article was first published in &lt;a href="https://towardsdatascience.com/agile-data-science-data-science-can-and-should-be-agile-c719a511b868"&gt;TowardsDataScience&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;tl;dr;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;I have encountered a lot of resistance in the data science community against agile methodology and specifically scrum framework. &lt;/li&gt;
&lt;li&gt;I don’t see it this way and claim that most disciplines would improve by adopting an agile mindset. &lt;/li&gt;
&lt;li&gt;We will go through a typical scrum sprint to highlight the compatibility of the data science process and the agile development process. &lt;/li&gt;
&lt;li&gt;Finally, we discuss when a scrum is not an appropriate process to follow. If you are a consultant working on many projects at a time or your work requires deep concentration on a single and narrow issue (narrow, so that you alone can solve it).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;I have found a medium post recently, which claims that &lt;a href="https://towardsdatascience.com/why-scrum-is-awful-for-data-science-db3e5c1bb3b4"&gt;Scrum is awful for data science&lt;/a&gt;. I’m afraid I have to disagree and would like to make a case for Agile Data Science.&lt;/p&gt;
&lt;p&gt;Ideas for this post are significantly influenced by the &lt;a href="https://www.amazon.co.uk/Agile-Data-Science-Russell-Jurney/dp/1491960116"&gt;Agile Data Science 2.0&lt;/a&gt; book (which I highly recommend) and personal experience. I am eager to know other experiences, so please share them in the comments.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;First, we need to agree on what data science is and how it solves business problems so we can investigate the process of data science and how agile (and specifically Scrum) can improve it.&lt;/p&gt;
&lt;h2&gt;What is Data Science?&lt;/h2&gt;
&lt;p&gt;There are countless definitions online. For example, &lt;a href="https://en.wikipedia.org/wiki/Data_science"&gt;Wikipedia&lt;/a&gt; gives such a description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In my opinion, it is quite an accurate definition of what data science tries to accomplish. But I would simplify this definition further.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Science solves business problems by combining business understanding, data and algorithms.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Compared to the definition in Wikipedia, I would like to stress that data scientists should aim to &lt;strong&gt;solve&lt;/strong&gt; business problems rather than &lt;strong&gt;“extract knowledge and insights.”&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;How Data Science Solves business problems?&lt;/h2&gt;
&lt;p&gt;So data science is here to solve business problems. We need to accomplish a few things along the way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Understand the business problem;&lt;/li&gt;
&lt;li&gt;Identify and acquire available data;&lt;/li&gt;
&lt;li&gt;Clean / transform / prepare data;&lt;/li&gt;
&lt;li&gt;Select and fit an appropriate “model” for a given data;&lt;/li&gt;
&lt;li&gt;Deploy model to “production” — this is our attempt to solving a given problem;&lt;/li&gt;
&lt;li&gt;Monitoring performance;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As with everything, there are countless ways to go about implementing those steps, but I will try to persuade you that the agile (incremental and iterative) approach brings the most value to the company and the most joy to data scientists.&lt;/p&gt;
&lt;h2&gt;Agile Data Science Manifesto&lt;/h2&gt;
&lt;p&gt;I took this from page 6 in the &lt;a href="https://www.amazon.co.uk/Agile-Data-Science-Russell-Jurney/dp/1491960116"&gt;Agile Data Science 2.0&lt;/a&gt; book, so you are encouraged to read the original, but here it is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Iterate, iterate, iterate — tables, charts, reports, predictions.&lt;/li&gt;
&lt;li&gt;Ship intermediate output. Even failed experiments have output.&lt;/li&gt;
&lt;li&gt;Prototype experiments over implementing tasks.&lt;/li&gt;
&lt;li&gt;Integrate the tyrannical opinion of data in product management.&lt;/li&gt;
&lt;li&gt;Climb up and down the data-value pyramid as you work.&lt;/li&gt;
&lt;li&gt;Discover and pursue the critical path to a killer product.&lt;/li&gt;
&lt;li&gt;Get meta. Describe the process, not just the end state.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not all the steps are self-explanatory, and I encourage you to go and read what Russel Jurney had to say, but I hope that the main idea is clear — we share and intermediate output, and we iterate to achieve value.&lt;/p&gt;
&lt;p&gt;Given the above preliminaries, let us go over a standard week for a scrum team. And we will assume a one week sprint.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Scrum Team Sprint&lt;/h2&gt;
&lt;h3&gt;Day 1&lt;/h3&gt;
&lt;p&gt;There are many sprint structure variations, but I will assume that planning is done on Monday morning. The team will decide which user stories from the product backlog will be transferred to the Sprint backlog. The most pressing issue for our business, as evident from the backlog ranking, is customer fraud — fraudulent transactions are causing our valuable customers out of our platform. During the previous backlog refinement session, the team already discussed this task, and the product owner got additional information from the Fraud Investigation team. So during the meeting, the team decides to start with a simple experiment (and already is thinking of interesting iterations further down the road) — an initial model based on simple features of the transaction and participating users. Work is split so that the data scientist can go and have a look at the data team identified for this problem. The data engineer will set up the pipeline for model output integration to DWH systems, and the full-stack engineer starts to set up a page for transaction review and alert system for the Fraud Investigation team.&lt;/p&gt;
&lt;h3&gt;Day 2&lt;/h3&gt;
&lt;p&gt;At the start of Tuesday, all team gathers and shares progress. Data scientist shows a few graphs which indicate that even with limited features, we will have a decent model. At the same time, the data engineer is already halfway through setting up the system to score incoming transactions with the new model. The full-stack engineer is also progressing nicely, and just after a few minutes, everyone is back at their desk working on the agreed tasks.&lt;/p&gt;
&lt;h3&gt;Day 3&lt;/h3&gt;
&lt;p&gt;As with Tuesday, the team starts Wednesday with a standup meeting to share their progress. There is already a simple model build and some accuracy and error rate numbers. The data engineer shows the infrastructure for the transaction scoring, and the team discusses how the features arrive at the system and what needs to be done for them to be ready for the algorithm. The full-stack engineer shows the admin panel with metadata on transactions is displayed and the triggering mechanism. Another discussion follows on the threshold value for the model output to trigger a message for a fraud analyst. The team agrees that we need to be able to adjust this value since different models might have different distributions, and also, depending on other variables, we might want to increase and decrease the number of approved transactions.&lt;/p&gt;
&lt;h3&gt;Day 4&lt;/h3&gt;
&lt;p&gt;On Thursday, the team already has all the pieces, and during the standup, discuss how to integrate those pieces. Team also outlines how to best monitor models in production, so that model performance could be evaluated and also degradation could be detected before it causes any real damage. They agree that a simple dashboard for monitoring accuracy and error rates will suffice for now.&lt;/p&gt;
&lt;h3&gt;Day 5&lt;/h3&gt;
&lt;p&gt;Friday is a demo day. During standup, the team discusses the last issues remaining with the first iteration of the transaction fraud detection. Team members prepare for the meeting with the fraud analysts that will be using this solution.&lt;/p&gt;
&lt;p&gt;During the demo, the team shows what they have built for the fraud analysts. The team presents performance metrics and their implications for the fraud analysts. All feedback is converted to tasks for future sprints.
Another vital part of the Sprint is a retrospective — meeting where the team discusses three things:
1. What went well in the Sprint;
2. What could be improved;
3. What will we commit to improving in the next Sprint;&lt;/p&gt;
&lt;h3&gt;Further down the road&lt;/h3&gt;
&lt;p&gt;During the next Sprint, the team is working on another most important item from the product backlog. It might be feedback from the fraud analysts, or it might be something else that the product owner thinks will improve the overall business the most. However, the team closely monitors the performance of the initial version of the solution. It will continue to do so because ML solutions are sensitive to changes in underlying assumptions that the model made about data distribution.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Above is a relatively “clean” exposition of the scrum process for data science solutions. Real-world rarely is that way, but I wanted to convey a few points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data Science cannot stand on its own. If we’re going to impact the real world we have to collaborate in a cross-functional team, it should be a part of a wider team;&lt;/li&gt;
&lt;li&gt;Iteration is critical in data science, and we should expose artifacts of those iterations to our stakeholders to receive feedback as fast as possible;&lt;/li&gt;
&lt;li&gt;Scrum is a framework that is designed for iterative progress. Therefore it is a perfect fit for data science work;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, it is &lt;em&gt;not&lt;/em&gt; a framework for any endeavor. If your job requires you to think deeply for days, then Scrum and agile would probably be very disruptive and counterproductive. Also, if your work requires you to handle a lot of different and small data science-related tasks, following Scrum would be inappropriate, and maybe Kanban should be considered. However, typical product data science work is not like that. Iteration is king, and getting feedback fast is key to providing the right solutions to business problems.&lt;/p&gt;
&lt;h3&gt;In summary&lt;/h3&gt;
&lt;p&gt;Data Science is a perfect fit for the Scrum with a single modification — we do not expect to ship finished models. Instead, we ship artifacts of our work and solicit feedback from our stakeholders so we can make progress faster. Project managers might not like data science for the unpredictability of the progress, but iteration is not at fault, it is the only way forward.&lt;/p&gt;</content><category term="Data Science"></category><category term="data science"></category><category term="agile"></category><category term="iterative development"></category></entry><entry><title>Vaidas</title><link href="https://va1da2.github.io/about.html" rel="alternate"></link><published>2020-11-23T05:50:57+02:00</published><updated>2020-11-23T05:50:57+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2020-11-23:/about.html</id><summary type="html">&lt;p&gt;About me.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am Vaidas Armonas - father, husband, data scientist/machine learning engineer (this part is in heavy development). I love solving business problems with data and (when necessary) ML models. Interested in extracting knowledge from data, software engineering, in particular how good practices contribute to productivity in data science, Data/MlOps, finding product (data products especially) market fit and few other things.&lt;/p&gt;
&lt;p&gt;All opinions are my own.&lt;/p&gt;</content><category term="About"></category></entry></feed>