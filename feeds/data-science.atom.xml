<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Thoughts on Data Science, ML and Startups - Data Science</title><link href="https://va1da2.github.io/" rel="alternate"></link><link href="https://va1da2.github.io/feeds/data-science.atom.xml" rel="self"></link><id>https://va1da2.github.io/</id><updated>2021-01-16T06:01:54+02:00</updated><entry><title>Machine Learning Design Patterns: Problem Representation Part 1</title><link href="https://va1da2.github.io/machine-learning-design-patterns-problem-representation-part-1.html" rel="alternate"></link><published>2021-01-16T06:01:54+02:00</published><updated>2021-01-16T06:01:54+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2021-01-16:/machine-learning-design-patterns-problem-representation-part-1.html</id><summary type="html">&lt;p&gt;In my previous &lt;a href="/machine-learning-design-patterns-data-representation.html"&gt;post&lt;/a&gt; I have discussed data representation patterns presented in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;. In this post I would like to talk about the next topic in the above-mentioned book - &lt;strong&gt;problem representation design patterns&lt;/strong&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my previous &lt;a href="/machine-learning-design-patterns-data-representation.html"&gt;post&lt;/a&gt; I have discussed data representation patterns presented in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;. In this post I would like to talk about the next topic in the above-mentioned book - problem representation. After taking care of our data representation, this is the next logical step (and therefore the next chapter in the book). This is also probably the most important decision to make for an ML problem - the decision how to model a given problem will define how well our solution will perform. The good bit is that we do not need to make this decision correct from the start - as with everything in ML, it is an iterative process, and when you find that your problem cannot be solved by regression, try classification (always try classification if you can).&lt;/p&gt;
&lt;p&gt;I will do it differently this time - instead of just discussing patterns, I will define a task which we will solve using different design patterns. This way we will be able to compare results and see the influence of problem representation.&lt;/p&gt;
&lt;p&gt;In this post, I will concentrate on &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt; design patterns. Next time I will cover &lt;strong&gt;Rebalancing&lt;/strong&gt; and &lt;strong&gt;Ensemble&lt;/strong&gt; design patterns. But first, let's define our task.&lt;/p&gt;
&lt;h1&gt;Task: Predict song popularity&lt;/h1&gt;
&lt;p&gt;To illustrate the above-mentioned design patterns I will try to predict track popularity on Spotify only using track (mostly audio) features such as &lt;em&gt;danceability&lt;/em&gt;, &lt;em&gt;liveness&lt;/em&gt; and &lt;em&gt;tempo&lt;/em&gt;. Data can be downloaded from &lt;a href="https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks"&gt;Kaggle&lt;/a&gt; and the full list of features used and I will provide the accompanying notebook shortly (needs a bit of editing).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Popularity&lt;/strong&gt; is a Spotify metric calculated for each track mostly based on the number of plays and the recency of those plays. Given the above definition, we expect that new songs will be more popular on average. So we will limit ourselves 
to the tracks that were released in the past decade. Again, for more details, see the notebook.&lt;/p&gt;
&lt;h1&gt;Reframing and Neutral Class Design Patterns&lt;/h1&gt;
&lt;p&gt;Like I have already mentioned, in this post I would like to discuss and illustrate two problem representation design patterns - &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt;. The first should be bread and butter for any data scientist, but the second I haven't seen anywhere else so far.&lt;/p&gt;
&lt;h1&gt;Solving the task&lt;/h1&gt;
&lt;p&gt;The task is simple - predict which song is popular. Popularity rating is an integer from 1 to 100. It's not a real-valued target, as for example price of a house would be, but regression is a reasonable approach here. However, if the only thing we want to predict is popularity (not the rating itself), we can make this task a classification problem by thresholding the popularity index.&lt;/p&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;More on data can be found in the notebook referenced above. &lt;/p&gt;
&lt;p&gt;We have 19,788 tracks collected for the years 2011-2020. We split this dataset randomly to train and test sets - 15,830 and 3,958. We have 11 audio features to predict popularity from.&lt;/p&gt;
&lt;p&gt;The popularity has the following distributions for train and test splits
&lt;img alt="popularity-distribution" src="/images/popularity_distribution_train_test_better.png"&gt;&lt;/p&gt;
&lt;p&gt;Not ideal, but close. Statistics are close too:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Statistic&lt;/th&gt;
&lt;th&gt;Popularity /train/&lt;/th&gt;
&lt;th&gt;Popularity /test/&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Count&lt;/td&gt;
&lt;td&gt;15,830&lt;/td&gt;
&lt;td&gt;3,958&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td&gt;58.89&lt;/td&gt;
&lt;td&gt;58.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Std&lt;/td&gt;
&lt;td&gt;15.30&lt;/td&gt;
&lt;td&gt;15.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Min&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25th&lt;/td&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50th&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;75th&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max&lt;/td&gt;
&lt;td&gt;99&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;For these types of problems, I use correlation (rank correlation) as an evaluation metric. Since I want to know which tracks are/will be popular, I am interested to know does my predicted score indicates higher actual popularity - and that's a correlation. I will use &lt;strong&gt;Spearman's Rho&lt;/strong&gt; and &lt;strong&gt;Kendall's Tau&lt;/strong&gt; with corresponding p-values.&lt;/p&gt;
&lt;h2&gt;Regression&lt;/h2&gt;
&lt;p&gt;As mentioned, regression is a reasonable approach to model popularity. Running it with sklearn's &lt;em&gt;GradientBoostingRegressor&lt;/em&gt;, which produces results as follows&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-regression" src="/images/predicted_vs_actual_regression_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.270&lt;/td&gt;
&lt;td&gt;5.25e-67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.184&lt;/td&gt;
&lt;td&gt;6.20e-66&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There is a statistically significant correlation between predicted and actual popularity. But let's see if we can do better.&lt;/p&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;p&gt;We can reframe our original regression problem to classification by thresholding the popularity index to create classes for our model to predict. This usually works better than regression, because we simplify the problem a bit. If the only thing we need is a relative ordering of the songs - scores from the classification model are perfectly good for it.&lt;/p&gt;
&lt;h3&gt;Split at the median&lt;/h3&gt;
&lt;p&gt;The simplest approach for binary class creation - split the scores at the median value. With this approach, we will have nicely distributed training data. This might not work if in your dataset values are very skewed towards one or the other end of the scale. In that case, you will have to experiment if having the imbalanced dataset works out OK or you should employ some re-balancing design patterns (the subject of my next post!).&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-classification-simple" src="/images/predicted_vs_actual_classification1_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.306&lt;/td&gt;
&lt;td&gt;7.69e-87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.210&lt;/td&gt;
&lt;td&gt;3.73e-85&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is an 11% and 14% improvement in Spearman's and Kendall's correlations respectively. Not bad for simple thresholding. Let's see if we can improve upon this result.&lt;/p&gt;
&lt;h3&gt;Middle values removed&lt;/h3&gt;
&lt;p&gt;Another trick that I always experiment with is trying to simplify a problem for the algorithm by removing middle values. Depending on you problem this might help for the model to better distinguish between &lt;code&gt;good&lt;/code&gt; and &lt;code&gt;bad&lt;/code&gt; examples. However, this means that we are discarding some of our data. And therefore this tradeoff will improve results when an amount of discarded data is less costly than the ambiguity that is removed.&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-classification-middle-removed" src="/images/predicted_vs_actual_classification2_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.303&lt;/td&gt;
&lt;td&gt;7.98e-87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.207&lt;/td&gt;
&lt;td&gt;1.39e-82&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In our example, I took top and bottom 40 percent of the dataset, and as we can see this ended up hurting performance. I also varied the amount of data I remove, and this approach always has a detrimental effect on model performance in this case.&lt;/p&gt;
&lt;h2&gt;Classification with Neutral Class&lt;/h2&gt;
&lt;p&gt;The above example tried to remove ambiguity introduced by splitting continuous variable at a threshold, but hurt model performance because part of the data was removed. The &lt;strong&gt;Neutral Class&lt;/strong&gt; design pattern takes care of that - instead of removing part of the data, we give it a class and use it for prediction. Then, at inference time, we only look at the &lt;code&gt;high&lt;/code&gt; class probability.&lt;/p&gt;
&lt;p&gt;The neutral class design pattern is also useful when we train models on human labelers output - medical imaging applications for example. When human labelers do not agree, we can represent that uncertainty to our model via neutral class.&lt;/p&gt;
&lt;h4&gt;Predicted vs. Actual Scatter Plot&lt;/h4&gt;
&lt;p&gt;&lt;img alt="predicted-vs-actual-popularity-classification-neutral-class" src="/images/predicted_vs_actual_classification3_better.png"&gt;&lt;/p&gt;
&lt;h4&gt;Correlation coefficients&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Coefficient&lt;/th&gt;
&lt;th&gt;p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speaerman's Rho&lt;/td&gt;
&lt;td&gt;0.308&lt;/td&gt;
&lt;td&gt;8.23e-88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kendall's Tau&lt;/td&gt;
&lt;td&gt;0.211&lt;/td&gt;
&lt;td&gt;7.16e-86&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, an additional 1% and 0.5% increase in Spearman's and Kendall's metrics respectively compared to our simple classification approach. This won't make-or-break your ML project, but the effect it has also will depend on the dataset. And with this simple change, any positive effect is welcome.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Looking at the scatterplots (or even looking at the label distribution) we see that more can be done to increase model performance. We could train a model to distinguish &lt;code&gt;0&lt;/code&gt; (or very low) scores from the higher ones and then train another model to predict higher scores on the output of the first. This is called the &lt;strong&gt;Cascade&lt;/strong&gt; design pattern and I will write about it in the future. &lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, we explored two ML design patterns - &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt;. I have shown that these can work in tandem by reframing a problem at hand from regression to classification and then adding a neutral class to help our model distinguish better high and low values. These two steps add a nice performance boost if we can live with having an estimate of a probability of the &lt;code&gt;high class&lt;/code&gt; rather than an estimate of the &lt;code&gt;value itself&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In my next post, I will take a look into &lt;strong&gt;Rebalancing&lt;/strong&gt; and &lt;strong&gt;Ensembles&lt;/strong&gt; design patterns. I think they are useful daily since most of the interesting problems are imbalanced by nature (predicting high-value customers, churners, etc.).&lt;/p&gt;</content><category term="Data Science"></category><category term="machine learning"></category><category term="ml"></category><category term="data science"></category><category term="design patterns"></category></entry><entry><title>Machine Learning Design Patterns: Data Representation</title><link href="https://va1da2.github.io/machine-learning-design-patterns-data-representation.html" rel="alternate"></link><published>2021-01-09T12:26:48+02:00</published><updated>2021-01-09T12:26:48+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2021-01-09:/machine-learning-design-patterns-data-representation.html</id><summary type="html">&lt;p&gt;Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can benefit immensely by following such patterns. In this and following posts I will discuss ML patterns outlined in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;. Let us start with &lt;strong&gt;Data Representation Patterns&lt;/strong&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can benefit immensely by following such patterns. In this and following posts I will discuss ML patterns outlined in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Data Representation Design Patterns&lt;/h1&gt;
&lt;p&gt;Let us start with &lt;strong&gt;Data Representation Patterns&lt;/strong&gt;. These patterns focus on the feature engineering part of the ML workflow. Simple techniques that could not reasonably called patterns include &lt;strong&gt;min-max scaling&lt;/strong&gt;, &lt;strong&gt;clipping&lt;/strong&gt;, and &lt;strong&gt;z-score normalization&lt;/strong&gt; for linear scaling, &lt;strong&gt;logarithms&lt;/strong&gt;, &lt;strong&gt;taking a root of a value&lt;/strong&gt;, &lt;strong&gt;histogram equalization&lt;/strong&gt;, and &lt;strong&gt;box-cox transform&lt;/strong&gt; for non-linear transformations, &lt;strong&gt;one-hot-encoding&lt;/strong&gt; for categorical feature handling (this might be a pattern, but it's just too common these days and there are quite a few better approaches) and &lt;strong&gt;array statistics&lt;/strong&gt; for the array of categorical (or even numeric) inputs feature.&lt;/p&gt;
&lt;p&gt;Patterns that are presented and we will discuss here are: &lt;strong&gt;Embeddings&lt;/strong&gt;, &lt;strong&gt;Feature Cross&lt;/strong&gt;, &lt;strong&gt;Multimodal Input&lt;/strong&gt;, and &lt;strong&gt;Hashed Feature&lt;/strong&gt;, &lt;/p&gt;
&lt;h2&gt;Embeddings&lt;/h2&gt;
&lt;p&gt;An embedding is a learnable representation of a high cardinality feature into a lower-dimensional space while preserving information. I would even say, that embeddings can enhance categorical features by encoding them in such a way, that makes ML task easier for a learning algorithm. These days (DL boom) it seems that everybody knows to use an image embedding in some ML task if an image should be included as a feature. However, this pattern is not only about this type of embedding. Let us see what problems this pattern address.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;The problem with categorical features is that simple conversion techniques do not capture relationships between classes and therefore rely on the algorithm to distill those relationships. Whatever categorical feature you have - a day of the week (especially if you start the week on Sunday, Saturday and Sunday will be far apart in terms of numerical value, while they are most likely close to each other in meaning), book or food category (we can one-hot encode them, but we cannot encode relationships among the categories this way) or any other categorical variable. The cardinality may also play a role here - if we have a huge vocabulary for categorical feature (fine-grained catalog in an e-shop can be prohibitively big to one-hot encode)&lt;/p&gt;
&lt;p&gt;There is also a problem with unstructured data incorporation to our algorithm. Text, image, and audio are rich sources of information but are not easy to incorporate in algorithms that are not specifically built for those types of input.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;Embeddings are a great way to encode categorical or unstructured data so that other algorithms can use a better representation of the feature.&lt;/p&gt;
&lt;p&gt;Embedding categorical features is probably the best and easiest way to improve model performance. By learning an embedding for any categorical feature, we extract information on how categories relate to each other for the task we are trying to learn. This boost performance of the algorithm and has a nice side effect - those learned embeddings can be used in other learning tasks.&lt;/p&gt;
&lt;p&gt;Embedding unstructured data is almost the only way to include that data in learning models. If we have a problem that could be better solved by including unstructured information, we could use one of the pre-trained models for images (one of many trained on ImageNet) or text (e.g. Glove/Word2Vec word vectors) and include the provided information to any other model together with structured data.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;The embedding pattern is quite unique - there is no real alternative to it. We can just encode categories by integers, or we can one-hot encode them, but it is not even close. If at all possible, we should always use &lt;strong&gt;Embedding pattern&lt;/strong&gt;, the only consideration is about what type of embedding to include.&lt;/p&gt;
&lt;h2&gt;Feature Cross&lt;/h2&gt;
&lt;p&gt;By combining feature values and making every combination a separate feature, we help our algorithm to learn relationships between features faster.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;When our features relate in non-linear ways, we can improve our model by providing those non-linear features by "crossing" them.&lt;/p&gt;
&lt;p&gt;One common example is a time of day and the day of the week for some event. If we want to predict some attribute of the given event (for example demand for bikes in a certain city bike-sharing spot) having just time of day and day of the week might not be enough and an explicit &lt;code&gt;AND&lt;/code&gt; relationship could improve our model.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;Feature cross is a simple multiplication of two categorical features (or bucketed numerical ones), such that if we have the day of a week (Monday, Tuesday, etc.) and the time of day (1 PM, 2 PM, etc.) features, we could get the time of the day of a week feature (Monday 1 PM, Monday 2 PM, Tuesday 1 PM, etc.). Crossing features increase cardinality considerably, therefore using this pattern with the embedding pattern could yield even better results.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;Feature crosses are a simple and very powerful way to introduce non-linearity in our models and help to learn relationships faster. This pattern is even better when using it with the embeddings pattern discussed above. However, we should consider features that we want to cross carefully. Since this pattern increases the complexity of the model quite a bit, we should not go and cross all our features.&lt;/p&gt;
&lt;h2&gt;Multimodal Input&lt;/h2&gt;
&lt;p&gt;When we have multiple representations of the phenomenon that we want to model, we should try to include all those representations in our algorithm.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;Many algorithms that are available online are designed for a specific type of input - image models (ResNets), text models (all the transformers out there), audio models (I haven't used any myself so far). However, there is a big class of problems where we would like to use several different types of inputs - numerical and categorical features combined with text or image or even all of the above (for example modeling social media campaign results include all the above input types).&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;By employing the embedding pattern, we can join different types of input into a single model. We can either use any of the pre-trained models for image/text input and extract the final layer features to concatenate them with numerical or categorical features. For example, if we are making predictions about the scene in the picture - having metadata on the scene (date and time information, weather information, or camera information) can significantly increase the accuracy of our model.&lt;/p&gt;
&lt;p&gt;Additionally, this pattern can be used with the same data, but different representations - bucketing is the simplest example. For example, if we have a distance feature in a dataset as a continuous feature, bucketing it could help to learn non-linear relationships, where very short and very long distances are correlated with our outcome.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;As data scientists/machine learning engineers, we should always seek new features to add to our models. By seeking out features from different modalities of the phenomenon we can build better models. Similarly, we can increase our model's performance by presenting the same information from a different angle.&lt;/p&gt;
&lt;h2&gt;Hashed Feature&lt;/h2&gt;
&lt;p&gt;The hashed feature design pattern is a very interesting approach meant for addressing cold start, incomplete vocabulary, and model size problems.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;Having high cardinality categorical features poses three main challenges when building an ML model:
* Not all categories might exist in the training dataset;
* The number of categories might be prohibitively big;
* Cold-start problem;&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;The proposed solution is a deterministic hash function (authors of the book proposes &lt;a href="https://github.com/google/farmhash"&gt;&lt;code&gt;FarmHash&lt;/code&gt;&lt;/a&gt;). We would hash a given categorical feature in a pre-defined number of buckets and would use that as a feature instead of the original value. With this approach, we tick all the above boxes:
* All categories would get a bucket, even those that were not present in the training dataset;
* We control how many buckets we hash our values into;
* Even new values of the feature, that weren't available during training time would get handled (our model would not error out.)&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;This pattern is the least appealing of all the data representation design patterns. There might be situations where this is necessary, but it should be a necessity. By randomly assigning buckets to our feature values we might group very different values together and therefore our model would suffer. So I would go with anything other than hashing if at all possible - describing high cardinality feature with values metadata or descriptive statistics. Maybe grouping those values based on those statistics and then using that as an input for future values to handle the cold start problem.&lt;/p&gt;
&lt;p&gt;On the other hand, this is the only new design pattern, so I am very glad that the authors decided to include this.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This is just the first post in the series about ML design patterns. The book contains several more chapters I intend to write about, including &lt;strong&gt;Problem Representation Design Pattern&lt;/strong&gt;, &lt;strong&gt;Model Training Patterns&lt;/strong&gt;, &lt;strong&gt;Model Serving Design Patterns&lt;/strong&gt;, &lt;strong&gt;Reproducibility Design Patterns&lt;/strong&gt;, and &lt;strong&gt;Responsible AI Design Patterns&lt;/strong&gt;.&lt;/p&gt;
&lt;h1&gt;Next posts:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Part 1 of &lt;strong&gt;Problem Representation Design Pattern&lt;/strong&gt;, where I discuss &lt;strong&gt;Reframing&lt;/strong&gt; and &lt;strong&gt;Neutral Class&lt;/strong&gt; design patterns, can be found &lt;a href="/machine-learning-design-patterns-problem-representation-part-1.html"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;</content><category term="Data Science"></category><category term="machine learning"></category><category term="ml"></category><category term="data science"></category><category term="design patterns"></category></entry><entry><title>A Case For Agile Data Science</title><link href="https://va1da2.github.io/a-case-for-agile-data-science.html" rel="alternate"></link><published>2020-11-23T05:50:58+02:00</published><updated>2020-11-23T05:50:58+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2020-11-23:/a-case-for-agile-data-science.html</id><summary type="html">&lt;p&gt;I have encountered a lot of resistance in the data science community against agile methodology and specifically the scrum framework. I don’t see it this way and claim that most disciplines would improve by adopting an agile mindset. We will go through a typical scrum sprint to highlight the compatibility of the data science process and the agile development process. Finally, we discuss when a scrum is not an appropriate process to follow. If you are a consultant working on many projects at a time or your work requires deep concentration on a single and narrow issue (narrow, so that you alone can solve it).&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This article was first published in &lt;a href="https://towardsdatascience.com/agile-data-science-data-science-can-and-should-be-agile-c719a511b868"&gt;TowardsDataScience&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;tl;dr;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;I have encountered a lot of resistance in the data science community against agile methodology and specifically scrum framework. &lt;/li&gt;
&lt;li&gt;I don’t see it this way and claim that most disciplines would improve by adopting an agile mindset. &lt;/li&gt;
&lt;li&gt;We will go through a typical scrum sprint to highlight the compatibility of the data science process and the agile development process. &lt;/li&gt;
&lt;li&gt;Finally, we discuss when a scrum is not an appropriate process to follow. If you are a consultant working on many projects at a time or your work requires deep concentration on a single and narrow issue (narrow, so that you alone can solve it).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;I have found a medium post recently, which claims that &lt;a href="https://towardsdatascience.com/why-scrum-is-awful-for-data-science-db3e5c1bb3b4"&gt;Scrum is awful for data science&lt;/a&gt;. I’m afraid I have to disagree and would like to make a case for Agile Data Science.&lt;/p&gt;
&lt;p&gt;Ideas for this post are significantly influenced by the &lt;a href="https://www.amazon.co.uk/Agile-Data-Science-Russell-Jurney/dp/1491960116"&gt;Agile Data Science 2.0&lt;/a&gt; book (which I highly recommend) and personal experience. I am eager to know other experiences, so please share them in the comments.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;First, we need to agree on what data science is and how it solves business problems so we can investigate the process of data science and how agile (and specifically Scrum) can improve it.&lt;/p&gt;
&lt;h2&gt;What is Data Science?&lt;/h2&gt;
&lt;p&gt;There are countless definitions online. For example, &lt;a href="https://en.wikipedia.org/wiki/Data_science"&gt;Wikipedia&lt;/a&gt; gives such a description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In my opinion, it is quite an accurate definition of what data science tries to accomplish. But I would simplify this definition further.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Science solves business problems by combining business understanding, data and algorithms.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Compared to the definition in Wikipedia, I would like to stress that data scientists should aim to &lt;strong&gt;solve&lt;/strong&gt; business problems rather than &lt;strong&gt;“extract knowledge and insights.”&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;How Data Science Solves business problems?&lt;/h2&gt;
&lt;p&gt;So data science is here to solve business problems. We need to accomplish a few things along the way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Understand the business problem;&lt;/li&gt;
&lt;li&gt;Identify and acquire available data;&lt;/li&gt;
&lt;li&gt;Clean / transform / prepare data;&lt;/li&gt;
&lt;li&gt;Select and fit an appropriate “model” for a given data;&lt;/li&gt;
&lt;li&gt;Deploy model to “production” — this is our attempt to solving a given problem;&lt;/li&gt;
&lt;li&gt;Monitoring performance;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As with everything, there are countless ways to go about implementing those steps, but I will try to persuade you that the agile (incremental and iterative) approach brings the most value to the company and the most joy to data scientists.&lt;/p&gt;
&lt;h2&gt;Agile Data Science Manifesto&lt;/h2&gt;
&lt;p&gt;I took this from page 6 in the &lt;a href="https://www.amazon.co.uk/Agile-Data-Science-Russell-Jurney/dp/1491960116"&gt;Agile Data Science 2.0&lt;/a&gt; book, so you are encouraged to read the original, but here it is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Iterate, iterate, iterate — tables, charts, reports, predictions.&lt;/li&gt;
&lt;li&gt;Ship intermediate output. Even failed experiments have output.&lt;/li&gt;
&lt;li&gt;Prototype experiments over implementing tasks.&lt;/li&gt;
&lt;li&gt;Integrate the tyrannical opinion of data in product management.&lt;/li&gt;
&lt;li&gt;Climb up and down the data-value pyramid as you work.&lt;/li&gt;
&lt;li&gt;Discover and pursue the critical path to a killer product.&lt;/li&gt;
&lt;li&gt;Get meta. Describe the process, not just the end state.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not all the steps are self-explanatory, and I encourage you to go and read what Russel Jurney had to say, but I hope that the main idea is clear — we share and intermediate output, and we iterate to achieve value.&lt;/p&gt;
&lt;p&gt;Given the above preliminaries, let us go over a standard week for a scrum team. And we will assume a one week sprint.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Scrum Team Sprint&lt;/h2&gt;
&lt;h3&gt;Day 1&lt;/h3&gt;
&lt;p&gt;There are many sprint structure variations, but I will assume that planning is done on Monday morning. The team will decide which user stories from the product backlog will be transferred to the Sprint backlog. The most pressing issue for our business, as evident from the backlog ranking, is customer fraud — fraudulent transactions are causing our valuable customers out of our platform. During the previous backlog refinement session, the team already discussed this task, and the product owner got additional information from the Fraud Investigation team. So during the meeting, the team decides to start with a simple experiment (and already is thinking of interesting iterations further down the road) — an initial model based on simple features of the transaction and participating users. Work is split so that the data scientist can go and have a look at the data team identified for this problem. The data engineer will set up the pipeline for model output integration to DWH systems, and the full-stack engineer starts to set up a page for transaction review and alert system for the Fraud Investigation team.&lt;/p&gt;
&lt;h3&gt;Day 2&lt;/h3&gt;
&lt;p&gt;At the start of Tuesday, all team gathers and shares progress. Data scientist shows a few graphs which indicate that even with limited features, we will have a decent model. At the same time, the data engineer is already halfway through setting up the system to score incoming transactions with the new model. The full-stack engineer is also progressing nicely, and just after a few minutes, everyone is back at their desk working on the agreed tasks.&lt;/p&gt;
&lt;h3&gt;Day 3&lt;/h3&gt;
&lt;p&gt;As with Tuesday, the team starts Wednesday with a standup meeting to share their progress. There is already a simple model build and some accuracy and error rate numbers. The data engineer shows the infrastructure for the transaction scoring, and the team discusses how the features arrive at the system and what needs to be done for them to be ready for the algorithm. The full-stack engineer shows the admin panel with metadata on transactions is displayed and the triggering mechanism. Another discussion follows on the threshold value for the model output to trigger a message for a fraud analyst. The team agrees that we need to be able to adjust this value since different models might have different distributions, and also, depending on other variables, we might want to increase and decrease the number of approved transactions.&lt;/p&gt;
&lt;h3&gt;Day 4&lt;/h3&gt;
&lt;p&gt;On Thursday, the team already has all the pieces, and during the standup, discuss how to integrate those pieces. Team also outlines how to best monitor models in production, so that model performance could be evaluated and also degradation could be detected before it causes any real damage. They agree that a simple dashboard for monitoring accuracy and error rates will suffice for now.&lt;/p&gt;
&lt;h3&gt;Day 5&lt;/h3&gt;
&lt;p&gt;Friday is a demo day. During standup, the team discusses the last issues remaining with the first iteration of the transaction fraud detection. Team members prepare for the meeting with the fraud analysts that will be using this solution.&lt;/p&gt;
&lt;p&gt;During the demo, the team shows what they have built for the fraud analysts. The team presents performance metrics and their implications for the fraud analysts. All feedback is converted to tasks for future sprints.
Another vital part of the Sprint is a retrospective — meeting where the team discusses three things:
1. What went well in the Sprint;
2. What could be improved;
3. What will we commit to improving in the next Sprint;&lt;/p&gt;
&lt;h3&gt;Further down the road&lt;/h3&gt;
&lt;p&gt;During the next Sprint, the team is working on another most important item from the product backlog. It might be feedback from the fraud analysts, or it might be something else that the product owner thinks will improve the overall business the most. However, the team closely monitors the performance of the initial version of the solution. It will continue to do so because ML solutions are sensitive to changes in underlying assumptions that the model made about data distribution.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Above is a relatively “clean” exposition of the scrum process for data science solutions. Real-world rarely is that way, but I wanted to convey a few points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data Science cannot stand on its own. If we’re going to impact the real world we have to collaborate in a cross-functional team, it should be a part of a wider team;&lt;/li&gt;
&lt;li&gt;Iteration is critical in data science, and we should expose artifacts of those iterations to our stakeholders to receive feedback as fast as possible;&lt;/li&gt;
&lt;li&gt;Scrum is a framework that is designed for iterative progress. Therefore it is a perfect fit for data science work;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, it is &lt;em&gt;not&lt;/em&gt; a framework for any endeavor. If your job requires you to think deeply for days, then Scrum and agile would probably be very disruptive and counterproductive. Also, if your work requires you to handle a lot of different and small data science-related tasks, following Scrum would be inappropriate, and maybe Kanban should be considered. However, typical product data science work is not like that. Iteration is king, and getting feedback fast is key to providing the right solutions to business problems.&lt;/p&gt;
&lt;h3&gt;In summary&lt;/h3&gt;
&lt;p&gt;Data Science is a perfect fit for the Scrum with a single modification — we do not expect to ship finished models. Instead, we ship artifacts of our work and solicit feedback from our stakeholders so we can make progress faster. Project managers might not like data science for the unpredictability of the progress, but iteration is not at fault, it is the only way forward.&lt;/p&gt;</content><category term="Data Science"></category><category term="data science"></category><category term="agile"></category><category term="iterative development"></category></entry></feed>