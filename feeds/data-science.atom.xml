<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Thoughts on Data Science, ML and Startups - Data Science</title><link href="https://va1da2.github.io/" rel="alternate"></link><link href="https://va1da2.github.io/feeds/data-science.atom.xml" rel="self"></link><id>https://va1da2.github.io/</id><updated>2021-01-09T12:26:48+02:00</updated><entry><title>Machine Learning Design Patterns: Data Representation</title><link href="https://va1da2.github.io/machine-learning-design-patterns-data-representation.html" rel="alternate"></link><published>2021-01-09T12:26:48+02:00</published><updated>2021-01-09T12:26:48+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2021-01-09:/machine-learning-design-patterns-data-representation.html</id><summary type="html">&lt;p&gt;Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can benefit immensely by following such patterns. In this and following posts I will discuss ML patterns outlined in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can benefit immensely by following such patterns. In this and following posts I will discuss ML patterns outlined in &lt;a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;amp;dchild=1"&gt;&lt;strong&gt;Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp;amp; M. Munn&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Data Representation Design Patterns&lt;/h1&gt;
&lt;p&gt;Let us start with &lt;strong&gt;Data Representation Patterns&lt;/strong&gt;. These patterns focus on the feature engineering part of the ML workflow. Simple techniques that could not reasonably called patterns include &lt;strong&gt;min-max scaling&lt;/strong&gt;, &lt;strong&gt;clipping&lt;/strong&gt;, and &lt;strong&gt;z-score normalization&lt;/strong&gt; for linear scaling, &lt;strong&gt;logarithms&lt;/strong&gt;, &lt;strong&gt;taking a root of a value&lt;/strong&gt;, &lt;strong&gt;histogram equalization&lt;/strong&gt;, and &lt;strong&gt;box-cox transform&lt;/strong&gt; for non-linear transformations, &lt;strong&gt;one-hot-encoding&lt;/strong&gt; for categorical feature handling (this might be a pattern, but it's just too common these days and there are quite a few better approaches) and &lt;strong&gt;array statistics&lt;/strong&gt; for the array of categorical (or even numeric) inputs feature.&lt;/p&gt;
&lt;p&gt;Patterns that are presented and we will discuss here are: &lt;strong&gt;Embeddings&lt;/strong&gt;, &lt;strong&gt;Feature Cross&lt;/strong&gt;, &lt;strong&gt;Multimodal Input&lt;/strong&gt;, and &lt;strong&gt;Hashed Feature&lt;/strong&gt;, &lt;/p&gt;
&lt;h2&gt;Embeddings&lt;/h2&gt;
&lt;p&gt;An embedding is a learnable representation of a high cardinality feature into a lower-dimensional space while preserving information. I would even say, that embeddings can enhance categorical features by encoding them in such a way, that makes ML task easier for a learning algorithm. These days (DL boom) it seems that everybody knows to use an image embedding in some ML task if an image should be included as a feature. However, this pattern is not only about this type of embedding. Let us see what problems this pattern address.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;The problem with categorical features is that simple conversion techniques do not capture relationships between classes and therefore rely on the algorithm to distill those relationships. Whatever categorical feature you have - a day of the week (especially if you start the week on Sunday, Saturday and Sunday will be far apart in terms of numerical value, while they are most likely close to each other in meaning), book or food category (we can one-hot encode them, but we cannot encode relationships among the categories this way) or any other categorical variable. The cardinality may also play a role here - if we have a huge vocabulary for categorical feature (fine-grained catalog in an e-shop can be prohibitively big to one-hot encode)&lt;/p&gt;
&lt;p&gt;There is also a problem with unstructured data incorporation to our algorithm. Text, image, and audio are rich sources of information but are not easy to incorporate in algorithms that are not specifically built for those types of input.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;Embeddings are a great way to encode categorical or unstructured data so that other algorithms can use a better representation of the feature.&lt;/p&gt;
&lt;p&gt;Embedding categorical features is probably the best and easiest way to improve model performance. By learning an embedding for any categorical feature, we extract information on how categories relate to each other for the task we are trying to learn. This boost performance of the algorithm and has a nice side effect - those learned embeddings can be used in other learning tasks.&lt;/p&gt;
&lt;p&gt;Embedding unstructured data is almost the only way to include that data in learning models. If we have a problem that could be better solved by including unstructured information, we could use one of the pre-trained models for images (one of many trained on ImageNet) or text (e.g. Glove/Word2Vec word vectors) and include the provided information to any other model together with structured data.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;The embedding pattern is quite unique - there is no real alternative to it. We can just encode categories by integers, or we can one-hot encode them, but it is not even close. If at all possible, we should always use &lt;strong&gt;Embedding pattern&lt;/strong&gt;, the only consideration is about what type of embedding to include.&lt;/p&gt;
&lt;h2&gt;Feature Cross&lt;/h2&gt;
&lt;p&gt;By combining feature values and making every combination a separate feature, we help our algorithm to learn relationships between features faster.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;When our features relate in non-linear ways, we can improve our model by providing those non-linear features by "crossing" them.&lt;/p&gt;
&lt;p&gt;One common example is a time of day and the day of the week for some event. If we want to predict some attribute of the given event (for example demand for bikes in a certain city bike-sharing spot) having just time of day and day of the week might not be enough and an explicit &lt;code&gt;AND&lt;/code&gt; relationship could improve our model.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;Feature cross is a simple multiplication of two categorical features (or bucketed numerical ones), such that if we have the day of a week (Monday, Tuesday, etc.) and the time of day (1 PM, 2 PM, etc.) features, we could get the time of the day of a week feature (Monday 1 PM, Monday 2 PM, Tuesday 1 PM, etc.). Crossing features increase cardinality considerably, therefore using this pattern with the embedding pattern could yield even better results.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;Feature crosses are a simple and very powerful way to introduce non-linearity in our models and help to learn relationships faster. This pattern is even better when using it with the embeddings pattern discussed above. However, we should consider features that we want to cross carefully. Since this pattern increases the complexity of the model quite a bit, we should not go and cross all our features.&lt;/p&gt;
&lt;h2&gt;Multimodal Input&lt;/h2&gt;
&lt;p&gt;When we have multiple representations of the phenomenon that we want to model, we should try to include all those representations in our algorithm.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;Many algorithms that are available online are designed for a specific type of input - image models (ResNets), text models (all the transformers out there), audio models (I haven't used any myself so far). However, there is a big class of problems where we would like to use several different types of inputs - numerical and categorical features combined with text or image or even all of the above (for example modeling social media campaign results include all the above input types).&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;By employing the embedding pattern, we can join different types of input into a single model. We can either use any of the pre-trained models for image/text input and extract the final layer features to concatenate them with numerical or categorical features. For example, if we are making predictions about the scene in the picture - having metadata on the scene (date and time information, weather information, or camera information) can significantly increase the accuracy of our model.&lt;/p&gt;
&lt;p&gt;Additionally, this pattern can be used with the same data, but different representations - bucketing is the simplest example. For example, if we have a distance feature in a dataset as a continuous feature, bucketing it could help to learn non-linear relationships, where very short and very long distances are correlated with our outcome.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;As data scientists/machine learning engineers, we should always seek new features to add to our models. By seeking out features from different modalities of the phenomenon we can build better models. Similarly, we can increase our model's performance by presenting the same information from a different angle.&lt;/p&gt;
&lt;h2&gt;Hashed Feature&lt;/h2&gt;
&lt;p&gt;The hashed feature design pattern is a very interesting approach meant for addressing cold start, incomplete vocabulary, and model size problems.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;p&gt;Having high cardinality categorical features poses three main challenges when building an ML model:
* Not all categories might exist in the training dataset;
* The number of categories might be prohibitively big;
* Cold-start problem;&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;The proposed solution is a deterministic hash function (authors of the book proposes &lt;a href="https://github.com/google/farmhash"&gt;&lt;code&gt;FarmHash&lt;/code&gt;&lt;/a&gt;). We would hash a given categorical feature in a pre-defined number of buckets and would use that as a feature instead of the original value. With this approach, we tick all the above boxes:
* All categories would get a bucket, even those that were not present in the training dataset;
* We control how many buckets we hash our values into;
* Even new values of the feature, that weren't available during training time would get handled (our model would not error out.)&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;This pattern is the least appealing of all the data representation design patterns. There might be situations where this is necessary, but it should be a necessity. By randomly assigning buckets to our feature values we might group very different values together and therefore our model would suffer. So I would go with anything other than hashing if at all possible - describing high cardinality feature with values metadata or descriptive statistics. Maybe grouping those values based on those statistics and then using that as an input for future values to handle the cold start problem.&lt;/p&gt;
&lt;p&gt;On the other hand, this is the only new design pattern, so I am very glad that the authors decided to include this.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This is just the first post in the series about ML design patterns. The book contains several more chapters I intend to write about, including &lt;strong&gt;Problem Representation Design Pattern&lt;/strong&gt;, &lt;strong&gt;Model Training Patterns&lt;/strong&gt;, &lt;strong&gt;Model Serving Design Patterns&lt;/strong&gt;, &lt;strong&gt;Reproducibility Design Patterns&lt;/strong&gt;, and &lt;strong&gt;Responsible AI Design Patterns&lt;/strong&gt;.&lt;/p&gt;</content><category term="Data Science"></category><category term="machine learning"></category><category term="ml"></category><category term="data science"></category><category term="design patterns"></category></entry><entry><title>A Case For Agile Data Science</title><link href="https://va1da2.github.io/a-case-for-agile-data-science.html" rel="alternate"></link><published>2020-11-23T05:50:58+02:00</published><updated>2020-11-23T05:50:58+02:00</updated><author><name>Vaidas Armonas</name></author><id>tag:va1da2.github.io,2020-11-23:/a-case-for-agile-data-science.html</id><summary type="html">&lt;p&gt;I have encountered a lot of resistance in the data science community against agile methodology and specifically the scrum framework. I don’t see it this way and claim that most disciplines would improve by adopting an agile mindset. We will go through a typical scrum sprint to highlight the compatibility of the data science process and the agile development process. Finally, we discuss when a scrum is not an appropriate process to follow. If you are a consultant working on many projects at a time or your work requires deep concentration on a single and narrow issue (narrow, so that you alone can solve it).&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This article was first published in &lt;a href="https://towardsdatascience.com/agile-data-science-data-science-can-and-should-be-agile-c719a511b868"&gt;TowardsDataScience&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;tl;dr;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;I have encountered a lot of resistance in the data science community against agile methodology and specifically scrum framework. &lt;/li&gt;
&lt;li&gt;I don’t see it this way and claim that most disciplines would improve by adopting an agile mindset. &lt;/li&gt;
&lt;li&gt;We will go through a typical scrum sprint to highlight the compatibility of the data science process and the agile development process. &lt;/li&gt;
&lt;li&gt;Finally, we discuss when a scrum is not an appropriate process to follow. If you are a consultant working on many projects at a time or your work requires deep concentration on a single and narrow issue (narrow, so that you alone can solve it).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;I have found a medium post recently, which claims that &lt;a href="https://towardsdatascience.com/why-scrum-is-awful-for-data-science-db3e5c1bb3b4"&gt;Scrum is awful for data science&lt;/a&gt;. I’m afraid I have to disagree and would like to make a case for Agile Data Science.&lt;/p&gt;
&lt;p&gt;Ideas for this post are significantly influenced by the &lt;a href="https://www.amazon.co.uk/Agile-Data-Science-Russell-Jurney/dp/1491960116"&gt;Agile Data Science 2.0&lt;/a&gt; book (which I highly recommend) and personal experience. I am eager to know other experiences, so please share them in the comments.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;First, we need to agree on what data science is and how it solves business problems so we can investigate the process of data science and how agile (and specifically Scrum) can improve it.&lt;/p&gt;
&lt;h2&gt;What is Data Science?&lt;/h2&gt;
&lt;p&gt;There are countless definitions online. For example, &lt;a href="https://en.wikipedia.org/wiki/Data_science"&gt;Wikipedia&lt;/a&gt; gives such a description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In my opinion, it is quite an accurate definition of what data science tries to accomplish. But I would simplify this definition further.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Science solves business problems by combining business understanding, data and algorithms.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Compared to the definition in Wikipedia, I would like to stress that data scientists should aim to &lt;strong&gt;solve&lt;/strong&gt; business problems rather than &lt;strong&gt;“extract knowledge and insights.”&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;How Data Science Solves business problems?&lt;/h2&gt;
&lt;p&gt;So data science is here to solve business problems. We need to accomplish a few things along the way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Understand the business problem;&lt;/li&gt;
&lt;li&gt;Identify and acquire available data;&lt;/li&gt;
&lt;li&gt;Clean / transform / prepare data;&lt;/li&gt;
&lt;li&gt;Select and fit an appropriate “model” for a given data;&lt;/li&gt;
&lt;li&gt;Deploy model to “production” — this is our attempt to solving a given problem;&lt;/li&gt;
&lt;li&gt;Monitoring performance;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As with everything, there are countless ways to go about implementing those steps, but I will try to persuade you that the agile (incremental and iterative) approach brings the most value to the company and the most joy to data scientists.&lt;/p&gt;
&lt;h2&gt;Agile Data Science Manifesto&lt;/h2&gt;
&lt;p&gt;I took this from page 6 in the &lt;a href="https://www.amazon.co.uk/Agile-Data-Science-Russell-Jurney/dp/1491960116"&gt;Agile Data Science 2.0&lt;/a&gt; book, so you are encouraged to read the original, but here it is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Iterate, iterate, iterate — tables, charts, reports, predictions.&lt;/li&gt;
&lt;li&gt;Ship intermediate output. Even failed experiments have output.&lt;/li&gt;
&lt;li&gt;Prototype experiments over implementing tasks.&lt;/li&gt;
&lt;li&gt;Integrate the tyrannical opinion of data in product management.&lt;/li&gt;
&lt;li&gt;Climb up and down the data-value pyramid as you work.&lt;/li&gt;
&lt;li&gt;Discover and pursue the critical path to a killer product.&lt;/li&gt;
&lt;li&gt;Get meta. Describe the process, not just the end state.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not all the steps are self-explanatory, and I encourage you to go and read what Russel Jurney had to say, but I hope that the main idea is clear — we share and intermediate output, and we iterate to achieve value.&lt;/p&gt;
&lt;p&gt;Given the above preliminaries, let us go over a standard week for a scrum team. And we will assume a one week sprint.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Scrum Team Sprint&lt;/h2&gt;
&lt;h3&gt;Day 1&lt;/h3&gt;
&lt;p&gt;There are many sprint structure variations, but I will assume that planning is done on Monday morning. The team will decide which user stories from the product backlog will be transferred to the Sprint backlog. The most pressing issue for our business, as evident from the backlog ranking, is customer fraud — fraudulent transactions are causing our valuable customers out of our platform. During the previous backlog refinement session, the team already discussed this task, and the product owner got additional information from the Fraud Investigation team. So during the meeting, the team decides to start with a simple experiment (and already is thinking of interesting iterations further down the road) — an initial model based on simple features of the transaction and participating users. Work is split so that the data scientist can go and have a look at the data team identified for this problem. The data engineer will set up the pipeline for model output integration to DWH systems, and the full-stack engineer starts to set up a page for transaction review and alert system for the Fraud Investigation team.&lt;/p&gt;
&lt;h3&gt;Day 2&lt;/h3&gt;
&lt;p&gt;At the start of Tuesday, all team gathers and shares progress. Data scientist shows a few graphs which indicate that even with limited features, we will have a decent model. At the same time, the data engineer is already halfway through setting up the system to score incoming transactions with the new model. The full-stack engineer is also progressing nicely, and just after a few minutes, everyone is back at their desk working on the agreed tasks.&lt;/p&gt;
&lt;h3&gt;Day 3&lt;/h3&gt;
&lt;p&gt;As with Tuesday, the team starts Wednesday with a standup meeting to share their progress. There is already a simple model build and some accuracy and error rate numbers. The data engineer shows the infrastructure for the transaction scoring, and the team discusses how the features arrive at the system and what needs to be done for them to be ready for the algorithm. The full-stack engineer shows the admin panel with metadata on transactions is displayed and the triggering mechanism. Another discussion follows on the threshold value for the model output to trigger a message for a fraud analyst. The team agrees that we need to be able to adjust this value since different models might have different distributions, and also, depending on other variables, we might want to increase and decrease the number of approved transactions.&lt;/p&gt;
&lt;h3&gt;Day 4&lt;/h3&gt;
&lt;p&gt;On Thursday, the team already has all the pieces, and during the standup, discuss how to integrate those pieces. Team also outlines how to best monitor models in production, so that model performance could be evaluated and also degradation could be detected before it causes any real damage. They agree that a simple dashboard for monitoring accuracy and error rates will suffice for now.&lt;/p&gt;
&lt;h3&gt;Day 5&lt;/h3&gt;
&lt;p&gt;Friday is a demo day. During standup, the team discusses the last issues remaining with the first iteration of the transaction fraud detection. Team members prepare for the meeting with the fraud analysts that will be using this solution.&lt;/p&gt;
&lt;p&gt;During the demo, the team shows what they have built for the fraud analysts. The team presents performance metrics and their implications for the fraud analysts. All feedback is converted to tasks for future sprints.
Another vital part of the Sprint is a retrospective — meeting where the team discusses three things:
1. What went well in the Sprint;
2. What could be improved;
3. What will we commit to improving in the next Sprint;&lt;/p&gt;
&lt;h3&gt;Further down the road&lt;/h3&gt;
&lt;p&gt;During the next Sprint, the team is working on another most important item from the product backlog. It might be feedback from the fraud analysts, or it might be something else that the product owner thinks will improve the overall business the most. However, the team closely monitors the performance of the initial version of the solution. It will continue to do so because ML solutions are sensitive to changes in underlying assumptions that the model made about data distribution.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Above is a relatively “clean” exposition of the scrum process for data science solutions. Real-world rarely is that way, but I wanted to convey a few points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data Science cannot stand on its own. If we’re going to impact the real world we have to collaborate in a cross-functional team, it should be a part of a wider team;&lt;/li&gt;
&lt;li&gt;Iteration is critical in data science, and we should expose artifacts of those iterations to our stakeholders to receive feedback as fast as possible;&lt;/li&gt;
&lt;li&gt;Scrum is a framework that is designed for iterative progress. Therefore it is a perfect fit for data science work;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, it is &lt;em&gt;not&lt;/em&gt; a framework for any endeavor. If your job requires you to think deeply for days, then Scrum and agile would probably be very disruptive and counterproductive. Also, if your work requires you to handle a lot of different and small data science-related tasks, following Scrum would be inappropriate, and maybe Kanban should be considered. However, typical product data science work is not like that. Iteration is king, and getting feedback fast is key to providing the right solutions to business problems.&lt;/p&gt;
&lt;h3&gt;In summary&lt;/h3&gt;
&lt;p&gt;Data Science is a perfect fit for the Scrum with a single modification — we do not expect to ship finished models. Instead, we ship artifacts of our work and solicit feedback from our stakeholders so we can make progress faster. Project managers might not like data science for the unpredictability of the progress, but iteration is not at fault, it is the only way forward.&lt;/p&gt;</content><category term="Data Science"></category><category term="data science"></category><category term="agile"></category><category term="iterative development"></category></entry></feed>