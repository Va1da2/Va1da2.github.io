<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>Thoughts on Data Science, ML and Startups - ml problem representation</title>
        <link rel="stylesheet" href="https://va1da2.github.io/theme/css/main.css" />
        <link href="https://va1da2.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Thoughts on Data Science, ML and Startups Atom Feed" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://va1da2.github.io/">Thoughts on Data Science, ML and Startups</a></h1>
                <nav><ul>
                    <li><a href="https://va1da2.github.io/category/about.html">About</a></li>
                    <li><a href="https://va1da2.github.io/category/books.html">Books</a></li>
                    <li><a href="https://va1da2.github.io/category/data-science.html">Data Science</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="https://va1da2.github.io/machine-learning-design-patterns-problem-representation-part-2.html">Machine Learning Design Patterns: Problem Representation Part 2</a></h1>
<footer class="post-info">
        <abbr class="published" title="2021-01-24T05:44:06+02:00">
                Published: Sun 24 January 2021
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://va1da2.github.io/author/vaidas-armonas.html">Vaidas Armonas</a>
        </address>
<p>In <a href="https://va1da2.github.io/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="https://va1da2.github.io/tag/machine-learning.html">machine learning</a> <a href="https://va1da2.github.io/tag/ml.html">ml</a> <a href="https://va1da2.github.io/tag/data-science.html">data science</a> <a href="https://va1da2.github.io/tag/design-patterns.html">design patterns</a> <a href="https://va1da2.github.io/tag/ml-problem-representation.html">ml problem representation</a> </p>
</footer><!-- /.post-info --><p>In the <a href="/machine-learning-design-patterns-problem-representation-part-1.html">first part</a> of this <strong>Problem Representation</strong> series, we saw that representing seemingly regression problem as a classification problem can bring increased performance. We also saw that constructing a label in a specific way can additionality increase the performance, but results weren't great - we achieved about 30% correlation only. To increase performance, we will split our problem  - separate low popularity and medium-high popularity songs and treat them separately. Therefore this post is about separating unpopular tracks as best as we can. For this will take a look at two ML representation design patterns from the <a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;dchild=1">book</a> - <strong>Rebalancing</strong> and <strong>Ensembles</strong>. As before, the code to reproduce these results is on <a href="https://github.com/Va1da2/blog-notebooks/blob/main/sampling-ensemble-ml-design-patterns/Rebalancing%20and%20Ensembles.ipynb">github</a>.</p>
<h1>Task: Classifying songs <code>unpopular</code> vs <code>popular</code></h1>
<p>Looking at the distribution of popularity for our dataset (tracks produced from 2011), we see that a small portion of the songs is very unpopular (have a popularity score below 20):
<img alt="popularity-distribution-with-low-scores-highlighted" src="/images/popularity_distribution_train_test_highlight_low.png"></p>
<p>Low scores are outliers - only around <code>4%</code> of all popularity scores in our dataset fall below the popularity score of 20. However, our track popularity models developed in <a href="/machine-learning-design-patterns-problem-representation-part-1.html">part 1</a> of this series still tried to accommodate these low scores. Our models might perform better if we remove these "outlier" tracks before predicting the song's popularity. To do this, we need to classify songs - <code>unpopular</code>, the ones having Popularity of 20 or less, and <code>popular</code>, all the others. Since only 4% of our dataset tracks have such a low popularity score, this is a highly imbalanced problem. To solve such a task, we will try out a couple of ML design patterns - <strong>Rebalancing</strong> and <strong>Ensembles</strong>.</p>
<h1>Rebalancing and Ensemble Design Patterns</h1>
<p><strong>Rebalancing Design Pattern</strong> addresses the problem of the imbalanced dataset in three ways:
* <strong>undersampling</strong> the majority class - randomly or selectively discarding data from majority class;
* <strong>oversampling</strong> the minority class - producing additional data points from the minority class based on some heuristic;
* <strong>weighted classes</strong> - giving weight to errors from minority class to stair model in producing better predictions. </p>
<p><strong>Ensemble Design Pattern</strong> is a combination of multiple machine learning algorithms trained on subsamples of data to reduce bias, variance, or both. There are three main approaches for ensembles:
* <strong>Bagging</strong> (bootstrap aggregating) - great for reducing variance in ML models. We train a few models on random samples of our dataset and average (or take the majority vote) of their output. 
* <strong>Boosting</strong> - used for bias reduction, as it constructs a more powerful ensemble model than any of its models. 
* <strong>Stacking</strong> - yet another way to combine models. Training a model on top of other model outputs to find the input model outputs' best weighting. </p>
<p>We will use <em>bagging</em> in solving our problem since we will train those models on small data, and our goal is to reduce the variance of the combined score.</p>
<h1>Solving the task</h1>
<p>We will go through the experiments to see which produces the best result for our use-case. First, let's see a bit more about the data we are working with and define evaluation metrics.</p>
<h3>Data</h3>
<p><em>You will find a bit more EDA in the notebook referenced above.</em></p>
<p>We have 19,788 tracks collected for the years 2011-2020. We split this dataset randomly to train and test sets - 15,830 and 3,958. We have 11 audio features to predict popularity. Based on the plot provided above, let us mark tracks with the popularity of 20 and less as <code>unpopular</code> and other songs as <code>popular</code>. Having this definition, summary statistics for the label are:</p>
<table>
<thead>
<tr>
<th>Statistic</th>
<th>Popularity /train/</th>
<th>Popularity /test/</th>
</tr>
</thead>
<tbody>
<tr>
<td>Count</td>
<td>15,830</td>
<td>3,958</td>
</tr>
<tr>
<td>Mean</td>
<td>0.042</td>
<td>0.041</td>
</tr>
<tr>
<td>Std</td>
<td>0.201</td>
<td>0.200</td>
</tr>
</tbody>
</table>
<p>The distributions of response are close in training and test sets. </p>
<h3>Evaluation</h3>
<p>Our problem is a standard classification - the only thing we care about is to classify as accurately as possible examples in the test set. Since our dataset is highly imbalanced, <code>accuracy</code> is not an appropriate metric. We will use <code>precision</code>, <code>recall</code>, and <code>f1-score</code> for evaluation instead as these metrics better capture the performance of the model in our setting.</p>
<p>Let us start our experiments with the naive approach.</p>
<h2>Naive classification</h2>
<p>The naive approach is not to modify our dataset or classifier's hyper-parameters (weights for the classes). Since we evaluate our models based on precision, recall, and f1-score, we need to select a score threshold. We will do this by looking at the recall-precision-f1 graph like the one below.</p>
<p><img alt="naive-model-recall-precision-f1" src="/images/naive_model_precision_recall_f1_tradeoff.png"></p>
<p>The best point we can find from the above graph is <code>0.402</code> recall, <code>0.564</code> precision, and <code>0.470</code> f1-score. The metrics mentioned above correspond to the threshold of <code>0.205</code>.</p>
<p>We have our baseline. Next, let us explore various ways to account for imbalance in our dataset and see the impact on model performance.</p>
<h2>Weighting classes</h2>
<p>One way to achieve rebalancing is through the algorithm parameter rather than through changes to the training data. We can penalize errors made on the minority class examples more, and in this way, bias our algorithm to learn a better representation of the minority class. </p>
<p>After running a few experiments, we achieved the best f1-score with the minority-to-majority weight ratio of 7-to-1. The performance of the model trained with this weighing is as follows.</p>
<p><img alt="weighing-scheme-model-threshold-selection" src="/images/examples_weights_precision_recall_f1_tradeoff.png"></p>
<p>The above graph's optimal point is an f1-score of <code>0.489</code> (a 4% improvement on the naive approach), the precision of <code>0.524</code>, recall of <code>0.457</code>. We achieve these metrics with the threshold of <code>0.545</code>.</p>
<p>Now let us see what impact does the rebalancing of the data brings.</p>
<h2>Undersampling</h2>
<p>Dealing with the imbalanced dataset can be a series of blog posts in itself. Here we will look into a couple of techniques for <code>undersampling</code> and later for <code>oversampling</code> to get a sense of how these techniques perform. Overall - throwing out data is not wise, so we expect oversampling to come out on top. Let's see what our experiments show.</p>
<p>We will use a great package to make this process easy for us - <a href="https://imbalanced-learn.org/stable/index.html">imbalanced-learn</a>.</p>
<h3>Random</h3>
<p>Random undersampling is the most straightforward approach we can take to alter our dataset - randomly discarding some of the majority class records to achieve a specified "balanced" level. Running a few experiments for different values of this balance, we have found that the best performing ratio is <code>0.2</code> - undersampling until the balance is 20% minority examples and 80% majority examples (the original dataset was 4% minority and 96% majority).</p>
<p><img alt="random-undersampling-precision-recall-tradeoff" src="/images/random_undersampling_precision_recall_f1_tradeoff.png"></p>
<p>The above graph's optimal point is an f1-score of <code>0.484</code>, the precision of <code>0.469</code>, recall of <code>0.500</code>. We achieve these metrics with the threshold of <code>0.423</code>. We see that this approach gives us the model with a slightly different precision-recall tradeoff - we have better recall with this model and a somewhat lower precision score. With the f1-score being almost the same, we can pick a model that fits our use-case better in terms of this tradeoff later. Next, let us investigate a bit cleverer undersampling technique.</p>
<h3>NearMiss</h3>
<p>NearMiss method is based on a <a href="https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf">paper</a> by Zhang J. and Mani I. (2003). There are three variations of the algorithm:
1.  <strong>NearMiss1</strong> - selects majority class examples with the smallest average distance to three closets minority class examples.
2. <strong>NearMiss2</strong> - selects majority class examples according to their distance to three farthest minority class examples.
3. <strong>NearMiss3</strong> - selects a given number of the closest majority class examples to the minority class example. </p>
<p>After running nine experiments for each of the above variations, we achieve the best result for <code>version 3</code> with the equal sampling strategy - we downsampled the majority class to a 50/50 ratio. </p>
<p><img alt="near-miss-undersampling-precision-recall-f1-tradeoff" src="/images/near_miss_precision_recall_f1_tradeoff.png"></p>
<p>Given how much data we discard, it is no surprise that results are worse than those of other methods. We achieve the best performance with the threshold of <code>0.692</code> - the precision of <code>0.483</code>, recall of <code>0.341</code>, and f1-score of <code>0.400</code>. Let us see if oversampling can improve results.</p>
<h2>Oversampling</h2>
<p>It is no surprise that we lose model performance as we discard records from our dataset. Our dataset is not very big, to begin with, so next, we will try a couple of oversampling methods - <strong>SMOTE</strong> and <strong>ADASYN</strong>.</p>
<h3>SMOTE</h3>
<p>SMOTE was proposed in the paper by <a href="https://arxiv.org/pdf/1106.1813.pdf">Chawla et al. (2002)</a> as a means of improving model performance on imbalanced datasets and as an alternative to random over-sampling with replacement. Instead of oversampling actual records, this approach creates synthetic examples from the minority class. Let's see how it performs on our dataset.</p>
<p>Running several experiments with different oversampling ratios, we find that oversampling the dataset to the 20/80 level gives us the best performing model.</p>
<p><img alt="smote-oversampling-precision-recall-f1-tradeoff" src="/images/smote_oversampling_precision_recall_f1_tradeoff.png"></p>
<p>The best performance is with the threshold of <code>0.567</code> - the precision of <code>0.500</code>, recall of <code>0.396</code>, and f1-score of <code>0.442</code>. 
Oversampling with SMOTE produced a better model than NearMiss undersampling. However, it still does not outperform any of the naive, weighted, or models produced by randomly undersampling records. Let us investigate another oversampling technique.</p>
<h3>ADASYN</h3>
<p>ADASYN is the algorithm proposed in <a href="https://ieeexplore.ieee.org/abstract/document/4633969">Haibo et al. (2008)</a>. The main difference from SMOTE is that ADASYN generates more synthetic examples that are harder to learn instead of treating every minority group example equally.</p>
<p>As with SMOTE, ADASYN oversampling produces the best model with the dataset balance of 20/80.</p>
<p><img alt="adasyn-oversampling-precision-recall-f1-tradeoff" src="/images/adasyn_oversampling_precision_recall_f1_tradeoff.png"></p>
<p>As is evident from the graph, oversampling with ADASYN gives us a model that outperforms just one other - NearMiss model - the precision of <code>0.453</code>, recall of <code>0.378</code>, and overall f1-score of <code>0.412</code>.</p>
<h2>Ensemble</h2>
<p>The <strong>Ensemble</strong> design pattern is useful for many problems, not just for imbalanced datasets. Model performance will be more stable by training and aggregating multiple models that have seen different parts of the training dataset. There are different ensemble techniques to target bias or variance issues, and there are algorithms that already incorporate this technique - Random forest (bagging) and Boosted trees (boosting). However, when having an imbalanced dataset, there is a twist.</p>
<p>We do not want to train the model on the random sample of our data; what we want to do, is to randomly downsample the majority class and trained our model on balanced data. Then we repeat this process as many times as needed for a stable performance. In our case, with song popularity prediction, aggregating 40 models produced the best result.</p>
<p>By running several experiments, we found that the best ensemble has 45 models. That's a lot, but models are tree-based algorithms, so the inference is not snail-slow.</p>
<p><img alt="ensemble-precision-recall-f1-tradeoff" src="/images/ensemble_precision_recall_f1_tradeoff.png"></p>
<p>Ensemble approach gives us the third-best model with the precision of <code>0.481</code>, recall of <code>0.470</code>, and f1-score of <code>0.475</code> at a threshold of <code>0.733</code>. </p>
<h1>Discussion</h1>
<p>Putting all our results together we get the following table:</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
<th align="left">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Weighting training examples</td>
<td>0.524</td>
<td>0.457</td>
<td>0.489</td>
<td align="left">Best performing weighting at 7-to-1; threshold - 0.545</td>
</tr>
<tr>
<td>Random undersampling</td>
<td>0.469</td>
<td>0.500</td>
<td>0.484</td>
<td align="left">Best performing undersampling to 20/80; threshold - 0.423</td>
</tr>
<tr>
<td>Ensemble</td>
<td>0.481</td>
<td>0.470</td>
<td>0.475</td>
<td align="left">Best performing ensemble was with 45 models; threshold - 0.733</td>
</tr>
<tr>
<td>Naive</td>
<td>0.564</td>
<td>0.402</td>
<td>0.470</td>
<td align="left">Threshold - 0.205</td>
</tr>
<tr>
<td>SMOTE oversampling</td>
<td>0.500</td>
<td>0.396</td>
<td>0.442</td>
<td align="left">Best performing oversampling at 20/80; threshold - 0.567</td>
</tr>
<tr>
<td>ADASYN oversampling</td>
<td>0.453</td>
<td>0.378</td>
<td>0.412</td>
<td align="left">Best performing oversampling at 20/80; threshold - 0.578</td>
</tr>
<tr>
<td>NearMiss undersampling</td>
<td>0.483</td>
<td>0.341</td>
<td>0.400</td>
<td align="left">Best performing undersampling was done with version 3 to 50/50; threshold - 0.692</td>
</tr>
</tbody>
</table>
<p>Evaluating models on the highest f1-score achieved, we have two close contenders with slightly different performance profiles. Model build by weighing training examples has higher precision but lower recall, while model build by randomly undersampling majority class in training data has a higher recall and lower precision. The next two models (the 3rd and 4th by f1 score) are close in terms of f1 score but are slightly different in the performance profile. The ensemble model is relatively balanced, with recall and precision being near, while the Naive model has the highest precision among selected models, with recall being barely above 0.4.</p>
<p>We set out on this imbalanced classification journey to "clean" our dataset from unpopular tracks before predicting that popularity for "normal" tracks. By applying our top-performing model, we get the following popularity distribution:</p>
<p><img alt="popularity-distribution-for-original-and-cleaned-dataset" src="/images/comparison_of_popularity_distributions.png"></p>
<p>We remove around 60% of the unpopular songs with our model. We want to remove more, but we don't know if this will be enough to impact the overall popularity prediction performance. We will investigate this in <strong>Part 3</strong> of this <strong>Problem Representation Design Patterns</strong> series. We will bring together the last two posts and combine them with the <strong>Cascade</strong> design pattern.</p>
<h3>Other Posts in Machine Learning Design Patterns Series</h3>
<ul>
<li><a href="/machine-learning-design-patterns-data-representation.html">Data Representation Design Patters</a></li>
<li><a href="/machine-learning-design-patterns-problem-representation-part-1.html">Problem Representation Design Patterns Part 1</a></li>
</ul>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="https://va1da2.github.io/machine-learning-design-patterns-problem-representation-part-1.html" rel="bookmark"
                           title="Permalink to Machine Learning Design Patterns: Problem Representation Part 1">Machine Learning Design Patterns: Problem Representation Part 1</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2021-01-16T06:01:54+02:00">
                Published: Sat 16 January 2021
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://va1da2.github.io/author/vaidas-armonas.html">Vaidas Armonas</a>
        </address>
<p>In <a href="https://va1da2.github.io/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="https://va1da2.github.io/tag/machine-learning.html">machine learning</a> <a href="https://va1da2.github.io/tag/ml.html">ml</a> <a href="https://va1da2.github.io/tag/data-science.html">data science</a> <a href="https://va1da2.github.io/tag/design-patterns.html">design patterns</a> <a href="https://va1da2.github.io/tag/ml-problem-representation.html">ml problem representation</a> </p>
</footer><!-- /.post-info -->                <p>In my previous <a href="/machine-learning-design-patterns-data-representation.html">post</a>, I have discussed data representation patterns presented in <a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;dchild=1"><strong>Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp; M. Munn</strong></a>. In this post, I would like to talk about the next topic in the book mentioned above - <strong>problem representation design patterns</strong>.</p>
                <a class="readmore" href="https://va1da2.github.io/machine-learning-design-patterns-problem-representation-part-1.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="https://va1da2.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="https://www.linkedin.com/in/vaidasarmonas/">linkedin</a></li>
                            <li><a href="https://github.com/Va1da2">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

    <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-187862762-1', 'auto');
    ga('send', 'pageview');
    </script>
</body>
</html>