<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>Thoughts on Data Science, ML and Startups</title>
        <link rel="stylesheet" href="https://va1da2.github.io/theme/css/main.css" />
        <link href="https://va1da2.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Thoughts on Data Science, ML and Startups Atom Feed" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://va1da2.github.io/">Thoughts on Data Science, ML and Startups</a></h1>
                <nav><ul>
                    <li><a href="https://va1da2.github.io/category/about.html">About</a></li>
                    <li><a href="https://va1da2.github.io/category/books.html">Books</a></li>
                    <li><a href="https://va1da2.github.io/category/data-science.html">Data Science</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="https://va1da2.github.io/machine-learning-design-patterns-problem-representation-part-1.html">Machine Learning Design Patterns: Problem Representation Part 1</a></h1>
<footer class="post-info">
        <abbr class="published" title="2021-01-16T06:01:54+02:00">
                Published: Sat 16 January 2021
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://va1da2.github.io/author/vaidas-armonas.html">Vaidas Armonas</a>
        </address>
<p>In <a href="https://va1da2.github.io/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="https://va1da2.github.io/tag/machine-learning.html">machine learning</a> <a href="https://va1da2.github.io/tag/ml.html">ml</a> <a href="https://va1da2.github.io/tag/data-science.html">data science</a> <a href="https://va1da2.github.io/tag/design-patterns.html">design patterns</a> </p>
</footer><!-- /.post-info --><p>In my previous <a href="/machine-learning-design-patterns-data-representation.html">post</a> I have discussed data representation patterns presented in <a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;dchild=1"><strong>Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp; M. Munn</strong></a>. In this post I would like to talk about the next topic in the above-mentioned book - problem representation. After taking care of our data representation, this is the next logical step (and therefore the next chapter in the book). This is also probably the most important decision to make for an ML problem - the decision how to model a given problem will define how well our solution will perform. The good bit is that we do not need to make this decision correct from the start - as with everything in ML, it is an iterative process, and when you find that your problem cannot be solved by regression, try classification (always try classification if you can).</p>
<p>I will do it differently this time - instead of just discussing patterns, I will define a task which we will solve using different design patterns. This way we will be able to compare results and see the influence of problem representation.</p>
<p>In this post, I will concentrate on <strong>Reframing</strong> and <strong>Neutral Class</strong> design patterns. Next time I will cover <strong>Rebalancing</strong> and <strong>Ensemble</strong> design patterns. But first, let's define our task.</p>
<h1>Task: Predict song popularity</h1>
<p>To illustrate above-mentioned design patterns I will try to predict track popularity on Spotify only using track (mostly audio) features such as <em>danceability</em>, <em>liveness</em> and <em>tempo</em>. Data can be downloaded from <a href="https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks">Kaggle</a> and the full list of features used and the whole analysis can be found <a href="PUT LINK TO GITHUB WITH NOBTEOOK!">here</a>.</p>
<p><strong>Popularity</strong> is a Spotify metric calculated for each track mostly based on the number of plays and the recency of those plays. Given the above definition, we expect that new songs will be more popular on average. So we will limit ourselves 
to the tracks that were released in the past decade. Again, for more details, see the notebook.</p>
<h1>Reframing and Neutral Class Design Patterns</h1>
<p>Like I have already mentioned, in this post I would like to discuss and illustrate two problem representation design patterns - <strong>Reframing</strong> and <strong>Neutral Class</strong>. The first should be bread and butter for any data scientist, but the second I haven't seen anywhere else so far.</p>
<h1>Solving the task</h1>
<p>The task is simple - predict which song is popular. Popularity rating is an integer from 1 to 100. It's not a real-valued target, as for example price of a house would be, but regression is a reasonable approach here. However, if the only thing we want to predict is popularity (not the rating itself), we can make this task a classification problem by thresholding the popularity index.</p>
<h3>Data</h3>
<p>More on data can be found in the notebook referenced above. </p>
<p>We have 19,788 tracks collected for the years 2011-2020. We split this dataset randomly to train and test sets - 15,830 and 3,958. We have 11 audio features to predict popularity from.</p>
<p>The popularity has the following distributions for train and test splits
<img alt="popularity-distribution" src="/images/popularity_distribution_train_test_better.png"></p>
<p>Not ideal, but close. Statistics are close too:</p>
<table>
<thead>
<tr>
<th>Statistic</th>
<th>Popularity /train/</th>
<th>Popularity /test/</th>
</tr>
</thead>
<tbody>
<tr>
<td>Count</td>
<td>15,830</td>
<td>3,958</td>
</tr>
<tr>
<td>Mean</td>
<td>58.89</td>
<td>58.58</td>
</tr>
<tr>
<td>Std</td>
<td>15.30</td>
<td>15.14</td>
</tr>
<tr>
<td>Min</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>25th</td>
<td>54</td>
<td>53</td>
</tr>
<tr>
<td>50th</td>
<td>61</td>
<td>60</td>
</tr>
<tr>
<td>75th</td>
<td>67</td>
<td>67</td>
</tr>
<tr>
<td>Max</td>
<td>99</td>
<td>100</td>
</tr>
</tbody>
</table>
<h3>Evaluation</h3>
<p>For these types of problems, I use correlation (rank correlation) as an evaluation metric. Since I want to know which tracks are/will be popular, I am interested to know does my predicted score indicates higher actual popularity - and that's a correlation. I will use <strong>Spearman's Rho</strong> and <strong>Kendall's Tau</strong> with corresponding p-values.</p>
<h2>Regression</h2>
<p>As mentioned, regression is a reasonable approach to model popularity. Running it with sklearn's <em>GradientBoostingRegressor</em>, which produces results as follows</p>
<h4>Predicted vs. Actual Scatter Plot</h4>
<p><img alt="predicted-vs-actual-popularity-regression" src="/images/predicted_vs_actual_regression_better.png"></p>
<h4>Correlation coefficients</h4>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Coefficient</th>
<th>p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Speaerman's Rho</td>
<td>0.270</td>
<td>5.25e-67</td>
</tr>
<tr>
<td>Kendall's Tau</td>
<td>0.184</td>
<td>6.20e-66</td>
</tr>
</tbody>
</table>
<p>There is a statistically significant correlation between predicted and actual popularity. But let's see if we can do better.</p>
<h2>Classification</h2>
<p>We can reframe our original regression problem to classification by thresholding the popularity index to create classes for our model to predict. This usually works better than regression, because we simplify the problem a bit. If the only thing we need is a relative ordering of the songs - scores from the classification model are perfectly good for it.</p>
<h3>Split at the median</h3>
<p>The simplest approach for binary class creation - split the scores at the median value. With this approach, we will have nicely distributed training data. This might not work if in your dataset values are very skewed towards one or the other end of the scale. In that case, you will have to experiment if having the imbalanced dataset works out OK or you should employ some re-balancing design patterns (the subject of my next post!).</p>
<h4>Predicted vs. Actual Scatter Plot</h4>
<p><img alt="predicted-vs-actual-popularity-classification-simple" src="/images/predicted_vs_actual_classification1_better.png"></p>
<h4>Correlation coefficients</h4>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Coefficient</th>
<th>p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Speaerman's Rho</td>
<td>0.306</td>
<td>7.69e-87</td>
</tr>
<tr>
<td>Kendall's Tau</td>
<td>0.210</td>
<td>3.73e-85</td>
</tr>
</tbody>
</table>
<p>It is an 11% and 14% improvement in Spearman's and Kendall's correlations respectively. Not bad for simple thresholding. Let's see if we can improve upon this result.</p>
<h3>Middle values removed</h3>
<p>Another trick that I always experiment with is trying to simplify a problem for the algorithm by removing middle values. Depending on you problem this might help for the model to better distinguish between <code>good</code> and <code>bad</code> examples. However, this means that we are discarding some of our data. And therefore this tradeoff will improve results when an amount of discarded data is less costly than the ambiguity that is removed.</p>
<h4>Predicted vs. Actual Scatter Plot</h4>
<p><img alt="predicted-vs-actual-popularity-classification-middle-removed" src="/images/predicted_vs_actual_classification2_better.png"></p>
<h4>Correlation coefficients</h4>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Coefficient</th>
<th>p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Speaerman's Rho</td>
<td>0.303</td>
<td>7.98e-87</td>
</tr>
<tr>
<td>Kendall's Tau</td>
<td>0.207</td>
<td>1.39e-82</td>
</tr>
</tbody>
</table>
<p>In our example, I took top and bottom 40 percent of the dataset, and as we can see this ended up hurting performance. I also varied the amount of data I remove, and this approach always has a detrimental effect on model performance in this case.</p>
<h2>Classification with Neutral Class</h2>
<p>The above example tried to remove ambiguity introduced by splitting continuous variable at a threshold, but hurt model performance because part of the data was removed. The <strong>Neutral Class</strong> design pattern takes care of that - instead of removing part of the data, we give it a class and use it for prediction. Then, at inference time, we only look at the <code>high</code> class probability.</p>
<p>The neutral class design pattern is also useful when we train models on human labelers output - medical imaging applications for example. When human labelers do not agree, we can represent that uncertainty to our model via neutral class.</p>
<h4>Predicted vs. Actual Scatter Plot</h4>
<p><img alt="predicted-vs-actual-popularity-classification-neutral-class" src="/images/predicted_vs_actual_classification3_better.png"></p>
<h4>Correlation coefficients</h4>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Coefficient</th>
<th>p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Speaerman's Rho</td>
<td>0.308</td>
<td>8.23e-88</td>
</tr>
<tr>
<td>Kendall's Tau</td>
<td>0.211</td>
<td>7.16e-86</td>
</tr>
</tbody>
</table>
<p>As you can see, an additional 1% and 0.5% increase in Spearman's and Kendall's metrics respectively compared to our simple classification approach. This won't make-or-break your ML project, but the effect it has also will depend on the dataset. And with this simple change, any positive effect is welcome.</p>
<h2>Discussion</h2>
<p>Looking at the scatterplots (or even looking at the label distribution) we see that more can be done to increase model performance. We could train a model to distinguish <code>0</code> (or very low) scores from the higher ones and then train another model to predict higher scores on the output of the first. This is called <strong>Cascade</strong> design pattern and I will write about it in the future. </p>
<h1>Summary</h1>
<p>In this post, we explored two ML design patterns - <strong>Reframing</strong> and <strong>Neutral Class</strong>. I have shown that these can work in tandem by reframing a problem at hand from regression to classification and then adding a neutral class to help our model distinguish better high and low values. These two steps add a nice performance boost if we can live with having an estimate of a probability of the <code>high class</code> rather than an estimate of the <code>value itself</code>.</p>
<p>In my next post, I will take a look into <strong>Rebalancing</strong> and <strong>Ensembles</strong> design patterns. I think they are useful daily since most of the interesting problems are imbalanced by nature (predicting high-value customers, churners, etc.).</p>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="https://va1da2.github.io/machine-learning-design-patterns-data-representation.html" rel="bookmark"
                           title="Permalink to Machine Learning Design Patterns: Data Representation">Machine Learning Design Patterns: Data Representation</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2021-01-09T12:26:48+02:00">
                Published: Sat 09 January 2021
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://va1da2.github.io/author/vaidas-armonas.html">Vaidas Armonas</a>
        </address>
<p>In <a href="https://va1da2.github.io/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="https://va1da2.github.io/tag/machine-learning.html">machine learning</a> <a href="https://va1da2.github.io/tag/ml.html">ml</a> <a href="https://va1da2.github.io/tag/data-science.html">data science</a> <a href="https://va1da2.github.io/tag/design-patterns.html">design patterns</a> </p>
</footer><!-- /.post-info -->                <p>Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can benefit immensely by following such patterns. In this and following posts I will discuss ML patterns outlined in <a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;dchild=1"><strong>Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp; M. Munn</strong></a>. Let us start with <strong>Data Representation Patterns</strong>.</p>
                <a class="readmore" href="https://va1da2.github.io/machine-learning-design-patterns-data-representation.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="https://va1da2.github.io/2020-in-books.html" rel="bookmark"
                           title="Permalink to 2020 in Books">2020 in Books</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2020-12-23T13:11:46+02:00">
                Published: Wed 23 December 2020
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://va1da2.github.io/author/vaidas-armonas.html">Vaidas Armonas</a>
        </address>
<p>In <a href="https://va1da2.github.io/category/books.html">Books</a>.</p>
<p>tags: <a href="https://va1da2.github.io/tag/2020-review.html">2020 Review</a> <a href="https://va1da2.github.io/tag/year-review.html">Year Review</a> </p>
</footer><!-- /.post-info -->                <p>2020 was not easy by any measure. COVID-19, adapting to parenthood, rollercoaster of working at an early stage startup - it was a memorable year (though I might not remember a lot because of sleep deprivation). However, I still managed to read a few books and there were several that I would like to reflect upon. So here is an overview and I will share more details on a few books in later posts.</p>
                <a class="readmore" href="https://va1da2.github.io/2020-in-books.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="https://va1da2.github.io/a-case-for-agile-data-science.html" rel="bookmark"
                           title="Permalink to A Case For Agile Data Science">A Case For Agile Data Science</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2020-11-23T05:50:58+02:00">
                Published: Mon 23 November 2020
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://va1da2.github.io/author/vaidas-armonas.html">Vaidas Armonas</a>
        </address>
<p>In <a href="https://va1da2.github.io/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="https://va1da2.github.io/tag/data-science.html">data science</a> <a href="https://va1da2.github.io/tag/agile.html">agile</a> <a href="https://va1da2.github.io/tag/iterative-development.html">iterative development</a> </p>
</footer><!-- /.post-info -->                <p>I have encountered a lot of resistance in the data science community against agile methodology and specifically the scrum framework. I don’t see it this way and claim that most disciplines would improve by adopting an agile mindset. We will go through a typical scrum sprint to highlight the compatibility of the data science process and the agile development process. Finally, we discuss when a scrum is not an appropriate process to follow. If you are a consultant working on many projects at a time or your work requires deep concentration on a single and narrow issue (narrow, so that you alone can solve it).</p>
                <a class="readmore" href="https://va1da2.github.io/a-case-for-agile-data-science.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="https://va1da2.github.io/about.html" rel="bookmark"
                           title="Permalink to Vaidas">Vaidas</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2020-11-23T05:50:57+02:00">
                Published: Mon 23 November 2020
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://va1da2.github.io/author/vaidas-armonas.html">Vaidas Armonas</a>
        </address>
<p>In <a href="https://va1da2.github.io/category/about.html">About</a>.</p>

</footer><!-- /.post-info -->                <p>About me.</p>
                <a class="readmore" href="https://va1da2.github.io/about.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="https://va1da2.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="https://www.linkedin.com/in/vaidasarmonas/">linkedin</a></li>
                            <li><a href="https://github.com/Va1da2">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>