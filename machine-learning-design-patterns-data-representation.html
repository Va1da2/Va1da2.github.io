<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>Machine Learning Design Patterns: Data Representation</title>
        <link rel="stylesheet" href="https://va1da2.github.io/theme/css/main.css" />
        <link href="https://va1da2.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Thoughts on Data Science, ML and Startups Atom Feed" />
        <meta name="description" content="Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can..." />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://va1da2.github.io/">Thoughts on Data Science, ML and Startups</a></h1>
                <nav><ul>
                    <li><a href="https://va1da2.github.io/category/about.html">About</a></li>
                    <li><a href="https://va1da2.github.io/category/books.html">Books</a></li>
                    <li class="active"><a href="https://va1da2.github.io/category/data-science.html">Data Science</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="https://va1da2.github.io/machine-learning-design-patterns-data-representation.html" rel="bookmark"
           title="Permalink to Machine Learning Design Patterns: Data Representation">Machine Learning Design Patterns: Data Representation</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2021-01-09T12:26:48+02:00">
                Published: Sat 09 January 2021
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://va1da2.github.io/author/vaidas-armonas.html">Vaidas Armonas</a>
        </address>
<p>In <a href="https://va1da2.github.io/category/data-science.html">Data Science</a>.</p>
<p>tags: <a href="https://va1da2.github.io/tag/machine-learning.html">machine learning</a> <a href="https://va1da2.github.io/tag/ml.html">ml</a> <a href="https://va1da2.github.io/tag/data-science.html">data science</a> <a href="https://va1da2.github.io/tag/design-patterns.html">design patterns</a> </p>
</footer><!-- /.post-info -->      <p>Design patterns are a set of best practices and solutions to common problems. Machine learning engineers as engineers in other disciplines can benefit immensely by following such idioms. In this and following posts, I will discuss ML patterns outlined in <a href="https://www.amazon.co.uk/Machine-Learning-Design-Patterns-Preparation/dp/1098115783/ref=sr_1_1?crid=NI2IJ980L4YN&amp;dchild=1"><strong>Machine Learning Design Patterns by V. Lakshmanan, S. Robinson &amp; M. Munn</strong></a></p>
<h1>Data Representation Design Patterns</h1>
<p>Let us start with <strong>Data Representation Patterns</strong>. These patterns focus on the feature engineering part of the ML workflow. It would be a stretch to call some simple and commonly used techniques a <code>design pattern</code>:
1. Linear transformations: <strong>min-max scaling</strong>, <strong>clipping</strong>, and <strong>z-score normalization</strong>; 
2. Non-linear transformations: <strong>logarithms</strong>, <strong>taking a root of a value</strong>, <strong>histogram equalization</strong>, and <strong>box-cox transform</strong>; 
3. Categorical feature handling: <strong>one-hot-encoding</strong> (this might be a pattern, but it's just too common these days and there are quite a few better approaches);
4. Handling array of categorical features: <strong>array statistics</strong>;</p>
<p>Patterns that are presented and we will discuss here are: <strong>Embeddings</strong>, <strong>Feature Cross</strong>, <strong>Multimodal Input</strong>, and <strong>Hashed Feature</strong>, </p>
<h2>Embeddings</h2>
<p>An embedding is a learnable representation of a high cardinality feature into a lower-dimensional space while preserving information. I would even say that embeddings can enhance categorical features by encoding them in such a way that makes ML tasks easier for a learning algorithm. These days (DL boom), it seems that everybody knows to use an image embedding in some ML task if an image is a contributing factor. However, this pattern is not only about this type of embedding. Let us see what problems this pattern address.</p>
<h3>Problem</h3>
<p>The problem with categorical features is that simple conversion techniques do not capture relationships between classes and therefore rely on the algorithm to distill those relationships. Whatever categorical feature you have - a day of the week (especially if you start the week on Sunday, Saturday and Sunday will be far apart in terms of numerical value, while they are most likely close to each other in meaning), book or food category (we can one-hot encode them, but we cannot encode relationships among the categories this way) or any other categorical variable. The cardinality may also play a role here - if we have a huge vocabulary for categorical feature (fine-grained catalog in an e-shop can be prohibitively big to one-hot encode)</p>
<p>There is also a problem with unstructured data incorporation into our algorithm. Text, image, and audio are rich sources of information but are not easy to incorporate in algorithms that are not specifically built for those types of input.</p>
<h3>Solution</h3>
<p>Embeddings are a great way to encode categorical or unstructured data so that other algorithms can use a better representation of the feature.</p>
<p>Embedding categorical features is probably the best and easiest way to improve model performance. By learning an embedding for any categorical feature, we extract information on how categories relate to each other for the task we are trying to learn. This boosts the performance of the algorithm and has a nice side effect - those learned embeddings can be used in other learning tasks.</p>
<p>Embedding unstructured data is almost the only way to include that data in learning models. If we have a problem that could be better solved by including unstructured information, we could use one of the pre-trained models for images (one of many trained on ImageNet) or text (e.g. Glove/Word2Vec word vectors) and include the provided information to any other model together with structured data.</p>
<h3>Discussion</h3>
<p>The embedding pattern is unique - there is no real alternative to it. We can just encode categories by integers, or we can one-hot encode them, but it is not even close. If at all possible, we should always use <strong>Embedding pattern</strong>, the only consideration is about what type of embedding to include.</p>
<h2>Feature Cross</h2>
<p>By combining feature values and making every combination a separate feature, we help our algorithm to learn relationships between features faster.</p>
<h3>Problem</h3>
<p>When our features relate in non-linear ways, we can improve our model by providing those non-linear features by "crossing" them.</p>
<p>One common example is a time of day and the day of the week for some event. If we want to predict some attribute of the given event (for example demand for bikes in a certain city bike-sharing spot) having just time of day and day of the week might not be enough and an explicit <code>AND</code> relationship could improve our model.</p>
<h3>Solution</h3>
<p>Feature cross is a simple multiplication of two categorical features (or bucketed numerical ones), such that if we have the day of a week (Monday, Tuesday, etc.) and the time of day (1 PM, 2 PM, etc.) features, we could get the time of the day of a week feature (Monday 1 PM, Monday 2 PM, Tuesday 1 PM, etc.). Crossing features increase cardinality considerably, therefore using this pattern with the embedding pattern could yield even better results.</p>
<h3>Discussion</h3>
<p>Feature crosses are a simple and very powerful way to introduce non-linearity in our models and help to learn relationships faster. This pattern is even better when using it with the embeddings pattern discussed above. However, we should consider features that we want to cross carefully. Since this pattern increases the complexity of the model quite a bit, we should not go and cross all our features.</p>
<h2>Multimodal Input</h2>
<p>When we have multiple representations of the phenomenon that we want to model, we should try to include all those representations in our algorithm.</p>
<h3>Problem</h3>
<p>Many algorithms that are available online are designed for a specific type of input - image models (ResNets), text models (all the transformers out there), audio models (I haven't used any myself so far). However, there is a big class of problems where we would like to use several different types of inputs - numerical and categorical features combined with text or image or even all of the above (for example modeling social media campaign results include all the above input types).</p>
<h3>Solution</h3>
<p>By employing the embedding pattern, we can join different types of input into a single model. We can either use any of the pre-trained models for image/text input and extract the final layer features to concatenate them with numerical or categorical features. For example, if we are making predictions about the scene in the picture - having metadata on the scene (date and time information, weather information, or camera information) can significantly increase the accuracy of our model.</p>
<p>Additionally, this pattern can be used with the same data, but different representations - bucketing is the simplest example. For example, if we have a distance feature in a dataset as a continuous feature, bucketing could help to learn non-linear relationships, where very short and very long distances are correlated with our outcome.</p>
<h3>Discussion</h3>
<p>As data scientists/machine learning engineers, we should always seek new features to add to our models. By seeking out features from different modalities of the phenomenon we can build better models. Similarly, we can increase our model's performance by presenting the same information from a different angle.</p>
<h2>Hashed Feature</h2>
<p>The hashed feature design pattern is a very interesting approach meant for addressing cold start, incomplete vocabulary, and model size problems.</p>
<h3>Problem</h3>
<p>Having high cardinality categorical features poses three main challenges when building an ML model:
* Not all categories might exist in the training dataset;
* The number of categories might be prohibitively big;
* Cold-start problem;</p>
<h3>Solution</h3>
<p>The proposed solution is a deterministic hash function (authors of the book proposes <a href="https://github.com/google/farmhash"><code>FarmHash</code></a>). We would hash a given categorical feature in a pre-defined number of buckets and would use that as a feature instead of the original value. With this approach, we tick all the above boxes:
* All categories would get a bucket, even those that were not present in the training dataset;
* We control how many buckets we hash our values into;
* Even new values of the feature, that wasn't available during training time would get handled (our model would not error out.)</p>
<h3>Discussion</h3>
<p>This pattern is the least appealing of all the data representation design patterns. There might be situations where this is necessary, but it should be a necessity. By randomly assigning buckets to our feature values we might group very different values together and therefore our model would suffer. So I would go with anything other than hashing if at all possible - describing high cardinality feature with values metadata or descriptive statistics. Maybe grouping those values based on those statistics and then using that as an input for future values to handle the cold start problem.</p>
<p>On the other hand, this is the only new design pattern, so I am very glad that the authors decided to include this.</p>
<h2>Conclusion</h2>
<p>This is just the first post in the series about ML design patterns. The book contains several more chapters I intend to write about, including <strong>Problem Representation Design Pattern</strong>, <strong>Model Training Patterns</strong>, <strong>Model Serving Design Patterns</strong>, <strong>Reproducibility Design Patterns</strong>, and <strong>Responsible AI Design Patterns</strong>.</p>
<h3>Other Posts in Machine Learning Design Patterns Series</h3>
<ul>
<li><a href="/machine-learning-design-patterns-problem-representation-part-1.html">Problem Representation Design Patterns Part 1</a></li>
<li><a href="/machine-learning-design-patterns-problem-representation-part-2.html">Problem Representation Design Patterns Part 2</a></li>
</ul>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="https://va1da2.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="https://www.linkedin.com/in/vaidasarmonas/">linkedin</a></li>
                            <li><a href="https://github.com/Va1da2">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

    <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-187862762-1', 'auto');
    ga('send', 'pageview');
    </script>
</body>
</html>